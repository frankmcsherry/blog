# Datalog in Rust

Over the Memorial Day weekend [Kris Micinski](https://kmicinski.com) hosted a logic programming workshop in the delightful [Minnowbrook Conference Center](https://minnowbrook.org) in upstate New York.
I, a notorious villain, was invited for what I was half sure was my long-due comeuppance.
As it turned out it was delightful, with lots of friendly and supportive people with shared goals.

We had one requirement, which was that we write a blog post reflecting on the event, which Kris would collect and share out with the wider world.
You are going to get a bunch of thoughtful, considerate takes on the workshop, so I thought I'd go in a different direction.
As great as the time was, it struggled with a standard academic problem: smart people working on challenging problems that don't always directly connect to the challenges at hand.
Outputs without outcomes, as they say in Product-speak.

After a few days of Datalog (and other logic programming languages) talks, the one that stood out was the last talk by [Denis Bueno]() on [ctdal](https://github.com/sandialabs/ctadl).
It's program analysis in Datalog!
But .. boy was it a struggle, to hear it.
Many tools didn't work; [Soufflé](http://souffle-lang.github.io) did, but you have to hold it right.
Many challenges, and .. to be honest the bad news in the best news if you want to make progress.

I figured I should put my money-time where my mouth-brain is, and see what it is like to build a thing that is meant to be usable as well as "performant".
Plus, I figure everyone benefits from seeing what it's like to build such a thing too, and you'll even get to learn about logic programming!

## Let's build an interactive Datalog, in Rust!

I've built a few things like Datalog in my day, but one thing I haven't done is try and make a thing that is all three of simple, useable, and performant.
I thought we might try and do that here, with Datalog!
You can follow along in the [datatoad repository](https://github.com/frankmcsherry/datatoad).

I have previously built much of [datafrog](https://crates.io/crates/datafrog) (with help!), which provides the guts of a Datalog engine that you are then invited to wire together your own damned self.
Understandably, this was not the Datalog renaissance you might imagine.
We aren't going to start from datafrog, but we'll use a lot of the same algorithmic ideas.
If you are familiar with it you should be able to follow along, and if you are not familiar with it you're doing just fine.

I had an outline and post that started by building a bad Datalog, a slow and awkward Datalog, that we would then improve together as we learned things.
It was sufficiently bad that I think we should just start with the good version, and we can talk through the load-bearing moments in more detail as we go.

By the end of this post, we'll have an interactive Datalog that we can bulk load facts into, add rules on the go, and have it keep up with pretty good performance.
Here's an example that does a nullability analysis on an `httpd` dataflow (in the PL sense) graph;
```
> .list
    e:	9905624
    n:	138331
30.708µs

> m(?b, ?a) :- n(?a, ?b) .
52.027834ms

> .list
    e:	9905624
    m:	138331
    n:	138331
45.916µs

> m(?c, ?a) :- m(?b, ?a), e(?b, ?c) .
8.360353834s

> .list
    e:	9905624
    m:	9393283
    n:	138331
49.042µs

> 
```
This takes about 2s in datafrog, vs 8.3s here, though the datafrog example uses `(u32, u32)` data, but here with `Vec<String>` data and no compiled queries it takes only 4x longer.
I hope to get that down in the future.

I haven't confirmed that this does a great job on all problems yet.
I haven't even confirmed that it is generally correct, though on these reachability problems it produces the same number of output tuples as the datafrog implementation.
So for the moment, no too worried about performance, and more worried about ergonomics, explainability, and future extensibility.


Here's an overview of my plan for what we'll cover.
I'm aspirationally describing each of the sections having not yet done all of the work, but I have high hopes!
Much of this derives from prior work on the [datafrog](https://crates.io/crates/datafrog) project, but we'll start a bit slower and in more depth, and end up going further as well.

My plan is to go top-down, starting with "what is Datalog", and building the framework for an interactive evaluator.

0. [Introducing Datalog](#introducing-datalog)

Once we have overview in place, we'll start to build up what are currently the three loci of functionality.

1. [Parsing Datalog](#how-to-parse-rules-from-text)
2. [Representing and maintaining sets of facts](#how-to-represent-sets-of-facts)
3. [Planning and evaluating Datalog rules](#how-to-apply-rules-to-facts)

There are some very natural next steps.
I haven't done these yet, but I'm certain they can be done without a great deal of effort.
I'll put them here, and if you are especially excited about them and for some reason I haven't already done them, use this paragraph to compel me to do so.

4. Spilling everything to disk.
5. Scaling out to multiple workers and processes.
6. Streaming and worst-case optimal joins.

I have a few more tricks planned, as the project is also partly about unlocking an ability to experiment with new data-parallel computation patterns.
It turns out most of what we'll be doing is mechanically very similar to [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), though substantially simpler (and less capable).

## Introducing Datalog

[Datalog](https://en.wikipedia.org/wiki/Datalog) is a language in which you describe simple logical rules, and it derives all of the consequences that can be reached from them.
The rules are each [Horn clauses](https://en.wikipedia.org/wiki/Horn_clause), which are a grim way of describing rules that takes a set of input facts to an output fact.
These logical statements have placeholder variables, and part of the fun of Datalog is that it will fill all of the possible settings of the values.

Datalog rules are shaped as a "head" which is the thing (or things) that could be inferred, followed by `:-` and then a "body" which is a list of sufficient conditions.
For example, we might write a rule that says that a triangle (a, b, c) exists if each of three edges exist:
```
tri(a, b, c) :- edge(a, b), edge(b, c), edge(a, c).
```
The names `tri` and `edge` are relations, and the variables `a`, `b`, and `c` are free variables.
The rule is implicitly universally quantified over the variables, meaning that for any setting of the variables, if the body is true for those settings then the head is true for those settings as well.
We are going to exclude rules that have variables in a head atom that are not present in the body.

We often forget when writing Datalog, but there are also "facts" that are like rules with an empty body, meaning they are true unconditionally.
For example, we might state that the following three edges exist through the facts
```
edge(1, 2) :- .
edge(1, 3) :- .
edge(2, 3) :- .
```

We could also write this using "multiple heads" as:
```
edge(1, 2), edge(1, 3), edge(2, 3) :- .
```


When rules and facts combine, more facts are inferred.
With these facts, we would expect to infer that `tri(1, 2, 3)` is true.
This now becomes a fact itself, and might lead to more facts if we had rules that relied on `tri`.

Datalog has an appealing monotonicity property: as you add more rules (including facts), the set of things that are true only increases.
Moreover, Datalog is confluent, in that no matter what order you put these rules in you'll end at the same set of facts for any set of input rules.

At this point, I hope you can imagine typing rule after rule into a Datalog shell, and watching the consequences of these rules spill back out at you.
And, I hope just after imagining this you are imagining them spilling out so quickly because we have done such a good job making this work well, that actually you want to read from and write to files, and use this for all sort of real (or at least plausible) applications.

### How to represent facts and rules

Let's start with rules, as facts end up as a special but different case of rules.

A rule is two lists of *atoms*, the list before the turnstile called the "head" and the list after the turnstile the "body".
Whenever all atoms in the body evaluate to true, the atoms of the head must also be true.
```rust
struct Rule {
    head: Vec<Atom>,
    body: Vec<Atom>,
}
```
An atom is a named relation, as well as a number of *terms* that matches the arity of the relation.

```rust
struct Atom {
    name: String,
    terms: Vec<Term>,
}
```
Each term is either a named variable or, a simplifying choice we'll make, a string literal.
```rust
enum Term {
    Var(String),
    Lit(String),
}
```
There's no strong reason to make it a string literal.
Any types can work, and often folks use integers or other compact types.
In fact, it's an outright lie and we'll really use `Vec<u8>` as the type, and if you'd like those bytes to mean `String` or `(u32,u32)` or something else, you'll be welcome to do so.
The only properties we'll use are literal equality and an arbitrary order, and using `Vec<u8>` intentionally doesn't close any doors.

Facts are rules that have an empty body, and heads with no variables.
We can treat facts as data, recording the list of literal strings in each position.
Although not where we will end up, you can think of them for now as 
```rust
type Fact = Vec<String>;
```
We'll also collect lists of facts and associate them with names, as our representation of relations.
Again, not exactly where we'll end up, but a great starting intuition.
```rust
type Facts = BTreeMap<String, Vec<Fact>>;
```
Finally, we'll bundle our facts and rules into our Datalog interpreter state.
```rust
struct State {
    /// Rules that are not facts.
    rules: Vec<Rule>,
    /// Rules with an empty body and only literals in the head.
    facts: facts::Facts, // as yet undisclosed
}
```

Up next, what do you even do with all these things?

### An overview of operation

We're going to write a fairly simple skeleton of our interactive Datalog shell.
It will set up some `State`, repeatedly read rules from the input, and apply them to our facts until no new facts emerge.
If you type a name rather than a rule, we'll print out the relation with that name.

The code uses a few methods on `State` we haven't defined yet.
We'll unpack each of these in coming sections.
```rust
fn main() {

    let mut state = State::default();

    use std::io::Write;
    println!("");
    print!("> ");
    let _ = std::io::stdout().flush();

    let mut text = String::new();
    while let Ok(_size) = std::io::stdin().read_line(&mut text) {

        if let Some(parsed) = parse::datalog(&text) {
            state.extend(parsed);
            state.update();
        }

        else {
            match text.trim() {
                ".list" => {
                    for (name, facts) in state.facts.iter() {
                        println!("\t{}:\t{:?}", name, facts.len());
                    }    
                }
                _ => {
                    println!("Parse failure: {:?}", text);
                }
            }
        }

        println!("");
        print!("> ");
        let _ = std::io::stdout().flush();
        text.clear();
    }

}
```

There are three methods in there we haven't seen yet: `parse::datalog`, `State::extend`, and `State::update`.
The Datalog parsing is its own self-contained bit, going from lines of text to lists of rules; we'll cover it in the next section.
Before heading to that, let's see the `extend` method and discuss the `update` method which is the beating heart of Datalog.

```rust
impl State {
```
The `extend` method mostly just looks for facts, which are rules whose heads have only literals in them, and with an empty body.
We store them explicitly as lists of data, in the `facts` member variable by the name of the head atom (the name of the relation).
```rust
    /// Adds new rules to the state.
    pub fn extend(&mut self, rules: impl IntoIterator<Item=Rule>) {
        for rule in rules.into_iter() { self.push(rule); }
    }

    /// Adds a new rule to the state.
    pub fn push(&mut self, rule: Rule) {
        if rule.body.is_empty() {
            for atom in rule.head.iter() {
                let mut lits = Vec::with_capacity(atom.terms.len());
                for term in atom.terms.iter() {
                    if let Term::Lit(text) = term {
                        lits.push(text.to_string().into_bytes());
                    }
                    else { continue; }
                }
                let mut builder = facts::FactBuilder::default();
                builder.push(lits);
                self.facts
                    .entry(atom.name.to_owned())
                    .or_default()
                    .add_set(builder);
            }
        }
        else {
            let rule_plan = crate::join::JoinPlan::from(&rule);
            join::implement_plan(&rule, &rule_plan, self.rules.len(), true, &mut self.facts);
            self.rules.push(rule);
        }
    }
}
```
There are some `join::` things going on here.
Hold your breath for a bit; they are a necessary step to kick off some work when a new rule shows up.
We'll get there, but for the moment just read them as "start the derivations for that new rule".

The `update` method is the beating heart of Datalog, where we repeatedly derive facts continuing as long as any new fact has been derived.
Once we fail to derive any new facts, well we won't derive any more by just trying again, so we are done for the moment.
```rust
/// Applies all rules to all facts, and indicates if new facts were
pub fn update(&mut self) {
    self.advance();
    while self.active() {
        for (index, rule) in self.rules.iter().enumerate() {
            let rule_plan = crate::join::JoinPlan::from(rule);
            join::implement_plan(rule, &rule_plan, index, false, &mut self.facts);
        }
        self.advance();
    }
}
```
There are some helper methods in there that we use to move our understanding of what we've learned forward.
Informally, `active` tells us whether there are new facts still to process, and `advance` checks all new facts against the old facts for novelty and gets us ready to go again.
```rust
/// True iff any relation has new facts
fn active(&self) -> bool {
    self.facts
        .values()
        .any(|x| x.active())
}

/// Checks new facts against old facts.
pub fn advance(&mut self) {
    for facts in self.facts.values_mut() {
        facts.advance();
    }
}
```
It turns out there is a lot of secret stuff going on in `facts.advance()`, and we'll get there once we unpack more about how we plan to represent sets of facts.
Before that, a segue into parsing Datalog.

## How to parse rules from text

We are going to write a Datalog parser.
If I took an undergraduate class on lexing and parsing, I can't remember it, so this is going to be a not-great way to get Datalog rules from lines of text.

The code that follows is currently `parse.rs` in the project.

This is also the moment where I reveal the actual grammar, which is lifted from Soufflé.
In particular, variables start with `?` to distinguish them from literals.
I could imagine prefering the other way around, using `"` to indicate string literals, but I figured tracking Soufflé might make everyone else's lives easier.

### Tokenization

The first step in many a parser is "tokenization": turning the sequence of characters into a sequence of more meaningful symbols.

For Datalog, we have just a few significant tokens: `.`, `,`, `(`, `)`, `:-`, and `?` (used to distinguish variables from literals).
Everything that is not one of these will be a string token, describing an atom or term name.
```rust
/// Text translated to a sequence of tokens.
#[derive(Debug, PartialEq)]
enum Token {
    Period,
    Comma,
    LParen,
    RParen,
    Turnstile,
    Question,
    Text(String),
}
```
We will need to add `Exclamation` whenever we get to adding negation to our rules.
Not today!

The tokenizer I wrote removes all whitespace, and converts `:-` into `←` to have a single symbol to scan for.
More ambitious individuals could find a better way to write this.
```rust
fn tokenize(text: &str) -> Option<Vec<Token>> {

    let mut text = text.replace(":-", "←");
    text.retain(|c| !c.is_whitespace());

    let mut result = Vec::new();

    let pattern = ['.', ',', '(', ')', '←', '?'];
    for token in text.split_inclusive(&pattern) {
        let mut split = token.split(&pattern);
        let prev = split.next().unwrap();
        if !prev.is_empty() {
            result.push(Token::Text(prev.to_owned()));
        }
        let next = token.chars().rev().next().unwrap();
        result.push (
            match next {
                '.' => Token::Period,
                ',' => Token::Comma,
                '(' => Token::LParen,
                ')' => Token::RParen,
                '←' => Token::Turnstile,
                '?' => Token::Question,
                _ => { None? } 
            }
        );
    }
    
    Some(result)
}
```
As you can see, some horror around finding the key moments for these symbols and breaking things apart into tokens.
The challenge I had with Rust was convincing it to show me these locations, without also consuming the symbols.
Again, I can imagine a better implementation.

### Parsing

We'll parse a token sequence by repeatedly peeking at the next token, and then attempting to extract one of our types: rules, made of atoms, made of terms.
```rust
fn parse(tokens: Vec<Token>) -> Option<Vec<Rule>> {
    let mut tokens = tokens.into_iter().peekable();
    let mut rules = Vec::new();
    while tokens.len() > 0 {
        rules.push(parse_rule(&mut tokens)?);
    }

    Some(rules)
}
```
Rule parsing extracts atoms until a turnstile, then atoms until a period.
```rust
fn parse_rule<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Rule> {
    let mut head = Vec::new();
    while &Token::Turnstile != tokens.peek()? {
        if &Token::Comma == tokens.peek()? { tokens.next(); }
        head.push(parse_atom(tokens)?);
    }
    let Token::Turnstile = tokens.next()? else { None? };
    let mut body = Vec::new();
    while &Token::Period != tokens.peek()? {
        if &Token::Comma == tokens.peek()? { tokens.next(); }
        body.push(parse_atom(tokens)?);
    }
    let Token::Period = tokens.next()? else { None? };

    Some(Rule { head, body })
}
```
Atom parsing extracts a name and left parenthesis, terms as long as there are commas, then expects a right parenthesis.
```rust
fn parse_atom<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Atom> {
    let Token::Text(name) = tokens.next()? else { None? };
    let Token::LParen     = tokens.next()? else { None? };

    let mut cols = Vec::new();
    cols.push(parse_term(tokens)?);
    while let Token::Comma = tokens.peek()? {
        tokens.next();
        cols.push(parse_term(tokens)?);
    }
    let Token::RParen     = tokens.next()? else { None? };
    Some(Atom { name: name.to_owned(), cols })
}
```
Term parsing checks for an optional question mark, and then expects a text name.
```rust
fn parse_term<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Term> {
    if let Token::Question = tokens.peek()? {
        tokens.next()?;
        let Token::Text(term) = tokens.next()? else { None? };
        Some(Term::Var(term.clone()))
    }
    else { 
        let Token::Text(term) = tokens.next()? else { None? };
        Some(Term::Lit(term.clone()))
    }
}
```
Malformed rules result in a `None` result, with no great effort made to clearly communicate what went wrong.
As you might expect by this point, we could have tried harder and made this better.

## How to represent sets of facts

We said above that facts will `Vec<String>`, and sets of facts are `Vec<Fact>`.
That wasn't entirely accurate.

The problem is that `Vec<Vec<String>>` is a memory management nightmare.
Allocations of allocations of allocations.
Memory everywhere .. else; not where you were just looking.
We are going to use a different physical representation of the same information.

This section corresponds to `facts.rs` in the project.

### Data oriented design / columns

Our project has (at this moment) one external dependence: [`columnar`](https://crates.io/crates/columnar): a crate that converts Rust types (like `Vec<Fact>`) into a flat layout of only a few linear allocations.
Informally, it packs all the strings together in one massive `Vec<u8>`, and then it records the offsets that delineate `String` boundaries, and then again offsets in that list that delineate `Fact` boundaries.
But it does this all for you with the magical incantation
```rust
<Fact as Columnar>::Container
```
This is the type we will use to represent sets of facts.
It's such a good idea, we'll even give it a name.
```rust
/// A sorted list of distinct facts.
struct FactContainer {
    ordered: <Fact as Columnar>::Container,
}

impl std::ops::Deref for FactContainer {
    type Target = <Fact as Columnar>::Container;
    fn deref(&self) -> &Self::Target {
        &self.ordered
    }
}
```
We are following a Rust idiom of "wrapper types".
The `FactContainer` type wraps a list of facts, but also indicates that the sequence is sorted and deduplicated.
Any time we have one of these, we'll be assured of these properties (as long as we build them carefully; implementation coming up in a moment).

A key downside of columnar containers is that they are basically append only.
In principle with a `Vec` you could swap a fact around halfway through a list, or change a string literal it contains.
We aren't going to want to do that though, so we are mostly good.

One thing we'll want to be able to do is add facts to a list, and if we need to keep it sorted and deduplicated we now have a bit of a problem.
We can't change the list, other than by appending to it, and appending may not preserve the sorted order.
We'll need a different technique to accumulate sets of facts than just the `FactContainer`.

Were going to use a [log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) to represent our sets of facts.
This may sound complicated, but it really just means "a list of `FactContainer`s".
```rust
/// A list of sets of fact that double in length.
#[derive(Clone, Default)]
pub struct FactLSM {
    pub layers: Vec<FactContainer>,
}
```
The key property with an LSM is that the "layers" grow in size geometrically.
To add some facts, you just put your `FactContainer` into the list, and then we tidy the list by merging layers whose sizes are within 2x of each other.
Here's one way you can do that.
```rust
impl FactLSM {
    /// Adds a layer and restores the invariant.
    fn push(&mut self, layer: FactContainer) {
        self.layers.push(layer);
        self.tidy();
    }

    /// Ensures layers double in size.
    fn tidy(&mut self) {
        self.layers.sort_by_key(|x| x.len());
        self.layers.reverse();
        while let Some(pos) = (1..self.layers.len()).position(|i| self.layers[i-1].len() < 2 * self.layers[i].len()) {
            while self.layers.len() > pos + 1 {
                let x = self.layers.pop().unwrap();
                let y = self.layers.pop().unwrap();
                self.layers.push(x.merge(y));
                self.layers.sort_by_key(|x| x.len());
                self.layers.reverse();
            }
        }
    }
}
```
There is a `FactContainer::merge` function we use here, which does the expected thing on two sorted lists of distinct facts: it merges the two into one sorted list of distinct facts.

The LSM is a handy way to maintain a set of facts as you work with it, especially if you do not need it in its final form right yet.
We'll eventually want to convert all of those layers into one `FactContainer`, but we can put that off until we are ready.
It's such a useful idiom that we'll also take a moment to introduce a helper type we'll use to do the building for us:
```rust
/// Staging area for collecting fact sets.
pub struct FactBuilder {
    /// Un-sorted, possibly repeated facts.
    active: <Fact as Columnar>::Container,
    /// Sorted, deduplicated facts.
    layers: FactLSM,
}
```
This type has space in `active` for disorderly facts, but once they get numerous enough it will convert them into a `FactContainer` and add them to its `layers`.

As a final bit of fact-organization, we'll want a way to reflect the fact "lifecycle", from a new but potentially repeated fact, to a recent novel fact, to facts that we have fully processed.
We'll do that with three distinct collections, .. ah .. two of which are LSMs for some reason, and not the third.
```rust
pub struct FactSet {
    /// Newly arrived facts that may not be novel.
    pub to_add: FactLSM,
    /// Distinct facts that are due to be processed.
    pub recent: FactContainer,
    /// Distinct facts that have been fully processed.
    pub stable: FactLSM,
}
```
In fact, `FactSet::advance` is one of the missing functions from the initial section.
It promised to do something about deduplicating facts, and "getting us ready to go again".
Let's see what that actually means.
```rust
/// Moves recent into stable, then to_add into recent.
pub fn advance(&mut self) {
    // Move recent into stable
    if !self.recent.is_empty() {
        self.stable.push(std::mem::take(&mut self.recent));
    }

    // Convert the LSM to one list of facts.
    if let Some(to_add) = self.to_add.flatten() {

        // Tidy stable for the work we are about to do.
        self.stable.tidy_through(2 * to_add.len());

        // Remove from to_add any facts already in stable.
        let mut starts = vec![0; self.stable.layers.len()];
        let stable = &self.stable;
        self.recent = to_add.filter(move |x| {
            starts.iter_mut().zip(&stable.layers).all(|(start, layer)| {
                crate::join::gallop::<Fact>(layer.borrow(), start, |y| y < x);
                *start >= layer.borrow().len() || layer.borrow().get(*start) != x
            })
        });
    }
}
```
That last closure is a mess and you should complain.
It's checking for each element of `to_add` whether it can be found anywhere in the stable LSM.
It's just .. kinda hard to read.

The lifecycle of facts, going from `to_add` to `recent` to `stable`, is what will guide our hand as we work to derive new facts, without redoing work we've already done for existing facts.
That's what's coming up next.

## How to apply rules to facts

Our last (finally!) step is to put down the logic that takes us from rules and facts, to even more facts.
It's here that several seemingly arbitrary choices come together.
But it's also pretty head stuff, as we try and understand what Datalog rules even mean.

This is the subject of the `join.rs` file in the project.

### Datalog rules are Joins

When we have a rule like our triangle rule, updated with the new `?` notation,
```
tri(?a, ?b, ?c) :- edge(?a, ?b), edge(?b, ?c), edge(?a, ?c).
```
we'll want to first try to apply it with the facts we have, from `edge`.
This might seem overwhelming at first, but we can approach it a few ways.

First, observe that there are only so many settings of `?a`, `?b`, and `?c` to actual literal values.
Each of the literal values need to come from a fact involving `edge`.
The `?a` values need to come from the first coordinate, and the `?c` values need to come from the second coordinate.
We could plausibly just consider all settings of these values.
It's a lot, but not infinite or anything.

Second, observe that all possible settings of these values is not infinite but it is really too many so we'll have to be smarter.
Let's try taking a smaller bite at a time, and start by finding the values a, b, and c that satisfy the last two `edge` constraints.
Let's give that the name `vee`, and rewrite our rule in terms of it:
```
vee(?a, ?b, ?c) :- edge(?b, ?c), edge(?a, ?c).
tri(?a, ?b, ?c) :- edge(?a, ?b), vee(?a, ?b, ?c).
```
I chose last two rather than first two, and it's totally arbitrary but the code works out better if we collapse towards the head.
If you want it the other way around, you can call `body.reverse()` just after you parse the rule.

We now have two rules, each of which has a body with a pair of relations.
I assure you that this produces the same results, and now we can think about just solving the problem for bodies containing only two atoms.
Specifically, let's try and find literal triples that work for `vee`.

We are looking at a thing called an ["equijoin"](https://en.wikipedia.org/wiki/Join_(relational_algebra)) in relational databases.
We have two relations, some coordinates of which must match, and we'd like to find all assignments of values where that can happen.
The unlock with equijoins is noticing that `?c` is present in both atoms, and we can start from it and explore outwards to find `?a` and `?b` that work.

For any two relations, how do we find the facts where certain key columns match?
By sorting the relations by those key columns, and merging them!
(That's why we sorted these things, in addition to helping to deduplicate).

### Generalizing

We need to figure out how to do this in general, rather than just for triangles based on edges.

Our plan of attack will be to go through the body right-to-left, turning the last pair of relations into a join, and placing their result at the end of the list.
Once we have only two relations remaining, we'll do a special-cased join that puts the results in the form the head requires.
If we only have one relation to start with, we skip all this and just transform that relation into the shape the heads require.

The concrete parts we'll need to bring together are:
1.  For each atom in the body, a transformation of the facts so that they start with the columns we'll need to equate.
    Also, if the atom requires filtering (e.g. literal columns or repeated variable names), we would do this here.
2.  For each join we will perform, the arity (number of columns) of the key, to know which prefix of each fact to equate.
    Also for each join other than the last, the order of columns it should produce, so that the relation it deposits is ready to be joined.
    We won't have to worry about applying filtering here.
3.  For each head atom, the "layout" facts need to be in to be inserted into the relation.
    Each coordinate may be either a bound variable, or a literal from the atom itself.

Here's the shape of the join plan we'll use.
I should stress, this is .. a very naive plan, and there are cooler ones I hope to get to in the future.
This is a "right linear" join plan, and they don't get much simpler (maybe left linear is).
Simpler doesn't mean easy in this case, though.
```rust
/// A description of a linear join plan for a Datalog rule.
///
/// The plan goes right to left, joining pairs of atoms until there are at most two left.
/// With one or two body atoms, we either:
/// 1. map the single body atom onto the head atoms, or
/// 2. join the two body atoms and map the result onto the head atoms.
#[derive(Debug)]
pub struct JoinPlan {
    /// For each body atom, the pre-work to put it in the right shape.
    bodys: Vec<Plan>,
    /// For each join other than the leftmost, the key arity and projection to apply to its output.
    ///
    /// This should land the results keyed appropriately, and pass along all demanded bindings.
    joins: Vec<(usize, Vec<usize>)>,
    /// For each head atom, the action to perform in the final join to correctly shape the result.
    ///
    /// An `Ok(index)` indicates a coordinate to copy, and an `Err(lit)` indicates a literal to copy.
    /// If `joins` is empty there is no final join, and each are applied to the rows of the body atom.
    heads: Vec<Vec<Result<usize, String>>>,

    /// The join arity for the final head-producing join, if such a join exists.
    arity: Option<usize>,
}
```

The two hard steps are now 1. create this join plan from a `Rule`, and 2. implement this join plan using joins.

### Create a `JoinPlan` from a `Rule`

This is unfortunately just a wall of text.

We can start off on the right foot by noticing for each variable the leftmost atom and rightmost atom the variable appears in.
The leftmost occurrence is the last moment (right-to-left) the bound variable needs to be retained; after this moment we can discard the values.
The rightmost occurrence tells us when a variable first enters play, which is important for other body atoms that mention the variable to know if they must plan to include it in their key columns.

With this information, each body atom can look at its own variables and the leftmost and rightmost numbers to partition its columns into 
1. dead columns that no one wants to hear about, 
2. key columns that will need to be equated with the incoming relation,
3. value columns that must be presented and carried on, but are not equated.

Similarly, for each join we can partition the columns of the two participating relations similarly:
1. dead columns that no one wants to hear about (perhaps they were just joined),
2. key columns that will need to be equated with the incoming relation,
3. value columns that must be presented and carried on, but are not equated.

This thinking gives us the `bodys` and `joins` members, as well as the awkward `arity` member (a special case because of the leftmost join).

The final member, `heads`, is just about reading through the head atoms, recording either a literal or the location of the named variable.
There is also some chaos in dealing with the possibility that there is no join at all, and what we need is to map the contents of the single body atom.

The code that does all of this, likely incorrectly, is in `joins.rs` nicely packaged up in a `From<&str>` implementation for `JoinPlan`:
```rust
impl<'a> From<&'a Rule> for JoinPlan {
    fn from(rule: &'a Rule) -> Self {
        ...
    }
}
```
And that's all I'm going to show you of it here!

### Implement a `JoinPlan` using Joins

We are getting down to business.
Up until this point everything was either pretty abstract, or pretty concrete but just involving some text or rules or things.
We are now heading in to the performance sensitive part of things.

Join plan implementation starts through a method that looks like this:
```rust
/// Implements a provided rule by way of a provided plan.
///
/// Additionally, a rule-unique identifier allows the logic to create new relation names.
/// The `stable` argument indicates whether we need to perform a full evaluation or only the changes.
pub fn implement_plan(
    rule: &Rule, 
    plan: &JoinPlan, 
    pos: usize, 
    stable: bool, 
    facts: &mut Facts) 
{
    ...
}
```

There are a few secrets being revealed here.
At least, you may have some questions about the `stable` argument and I'll now have to explain.

When `stable` is set we are going to join together the relations contents including `stable + recent`.
Both of those are valid distinct facts, and we'll want pretty much everything.
This is the mode we use when we introduce a new rule.
Even if facts are stable, the rule is new and we have to do everything just to get started.

When `stable` is unset, we want the join results that don't derive exclusively from `stable` facts.
We have already seen anything that can be redived from stable facts, so don't waste our time!
Not yet clear how we are going to accomplish this though.

Joins, and binary joins in particular, which is what we are implementing, are "bi-linear", meaning in this case that they distribute over addition.
If we have fact sets `A` and `B`, each of which have had some facts `a` and `b` "recently" added, we can write their join as:
```
  (A + a) ⋈ (B + b)
=
   A ⋈ B + A ⋈ b + a ⋈ B + a ⋈ b
```
The first term in the second line is `A ⋈ B` which is exactly the join results from stable facts.
Yuck!
We don't want that!

What we really want is the join results including recent facts, minus the join results on stable facts.
We can rewrite the above into that:
```
  (A + a) ⋈ (B + b) - A ⋈ B
=
   A ⋈ b + a ⋈ B + a ⋈ b
```
This is great news!
To get what we want, only the new derivations, we can perform three joins that each always involve a small letter, a "recent" addition.
With that `stable` flag in mind, we'll implement our join with the ability to toggle between "full" and "incremental" join.

#### Apply the body plans

The first step in our join implementation is to apply the plans in `JoinPlan::bodys`.
These are the things that tell us for each input how to re-lay out the columns, and how to filter them if necessary.
```rust
// New name for handy use.
let name = format!(".temp-{:?}", pos);
// Stash names of relations to draw facts out of.
let mut names = Vec::with_capacity(rule.body.len());

// For each body plan apply the plan to a newly named relation.
for (index, (plan, atom)) in plan.bodys.iter().zip(rule.body.iter()).enumerate() {

    if plan.identity() {
        facts.entry(atom.name.clone()).or_default();
        names.push(atom.name.clone());
    }
    else {
        let new_name = format!("{}-{:?}-in", name, index);
        let mut builder = FactBuilder::default();
        let found = facts.entry(new_name.clone()).or_default();
        if stable {
            for layer in found.stable.layers.iter() {
                plan.apply(layer, &mut builder);
            }
        }
        plan.apply(&found.recent, &mut builder);

        facts.get_mut(&new_name).unwrap().add_set(builder);
        names.push(new_name);
    }
}
```
There is a cute moment here where if the plan is a no-op, and we try to make them a no-op if at all possible, we can just skip the step.
In that case, we'll just use the source fact list as reported!

Otherwise, we use that trusty `FactBuilder` we described earlier, and apply the plan to `recent` and if asked to each layer of `stable`.
The builder is handed off to the `FactSet`, which knows how to integrate the `FactLSM` into its `to_add` member.

#### Apply the joins: right to left

We now have prepared body inputs, each in the sorted order they need to be in to perform a merge join.
The next step is to apply these merge joins until there are at most two relations left.
Each time we apply a join we need to indicate an `arity`, and what to do with the matching results.
We will just put them into a `FactBuilder`, and stash that associated with a new temporary name.

```rust
let mut r_name = names.pop().unwrap();
if names.len() > 1 {
    // Burn down the joins until there are at most two collections left.
    for (index, (arity, projection)) in plan.joins.iter().enumerate().rev() {

        let l_name = names.pop().unwrap();
        let mut builder = FactBuilder::default();
        join_with(facts.get(&l_name).unwrap(), facts.get(&r_name).unwrap(), stable, *arity, |v1, v2| {
            builder.push(projection.iter().copied().map(|i| {
                if i < v1.len() { v1.get(i) } else { v2.get(i - v1.len()) }
            }));
        });

        // Stash the results under the name we'll use next.
        r_name = format!("{}-{:?}-mid", name, index + 1);
        facts.entry(r_name.clone())
             .or_default()
             .add_set(builder);
    }
}
```
Clearly the join magic is in another castle.
The castle named `join_with`.
We'll finally get there, but it's basically just ripped off of `datafrog`, so don't get too excited.
Before then, one last step in this method.

#### Apply the last join: populate the heads

This last part is just horrible, at least in proportion to how little is going on.

If there are two body atoms to join, we are going to do the same thing as above but we'll use different instructions to lay out the facts.
We'll also use multiple builders, because there could be multiple heads.
If there is only one body atom we'll do the same thing but without a join, just by reading the relation.

Somehow I couldn't write this code very cleanly, so it's the same thing repeated three times.
You'll have to check out the repo to read it, I'm sorry.

#### Getting down to joining

Let's knock out that `join_with` function and be done!

```rust
/// Joins `body1` and `body2` using the first `arity` columns.
///
/// Matching elements are subjected to `action`.
/// When `stable` is set, we join the stable plus recent elements of each input;
/// when it is unset we exclude pairs of terms that are both stable.
pub fn join_with(
    body1: &FactSet,
    body2: &FactSet,
    stable: bool,
    arity: usize,
    mut action: impl FnMut(<Fact as Columnar>::Ref<'_>, <Fact as Columnar>::Ref<'_>),
)
{
    // Compare elements by their first `arity` columns only.
    // This is a moment where compile-time information about types would help us; perhaps column introspection can recover.
    let order = |x: <Fact as Columnar>::Ref<'_>, y: <Fact as Columnar>::Ref<'_>| { (0 .. arity).map(|i| x.get(i)).cmp((0 .. arity).map(|i| y.get(i))) };

    if stable {
        for layer1 in body1.stable.layers.iter() {
            for layer2 in body2.stable.layers.iter() {
                join::<Fact>(layer1, layer2, order, &mut action);
            }
        }
    }

    for stable2 in body2.stable.layers.iter() {
        join::<Fact>(&body1.recent, stable2, order, &mut action);
    }

    for stable1 in body1.stable.layers.iter() {
        join::<Fact>(stable1, &body2.recent, order, &mut action);
    }

    join::<Fact>(&body1.recent, &body2.recent, order, &mut action);
}
```

Ok this was apparently all a lie and now there is some other `join` function?

So all we are doing here is handling that `stable` bit.
We are performing the three joins that involve recent facts, and if asked the one join that involves only stable facts.
We don't pass `stable` along to the `join` function, which is join really going to join things I swear.
```rust
/// Match keys in `input1` and `input2` and act on matches.
fn join<'a, T: Columnar<Ref<'a> : Ord+std::fmt::Debug>> (
    input1: &'a T::Container,
    input2: &'a T::Container,
    mut order: impl FnMut(T::Ref<'a>, T::Ref<'a>) -> std::cmp::Ordering,
    mut action: impl FnMut(T::Ref<'a>, T::Ref<'a>),
) {
    use std::cmp::Ordering;

    let input1 = input1.borrow();
    let input2 = input2.borrow();

    let mut index1 = 0;
    let mut index2 = 0;

    // continue until either input is exhausted.
    while index1 < input1.len() && index2 < input2.len() {
        // compare the keys at this location.
        let pos1 = input1.get(index1);
        let pos2 = input2.get(index2);
        match order(pos1, pos2) {
            Ordering::Less => {
                // advance `index1` while strictly less than `pos2`.
                gallop::<T>(input1, &mut index1, |x| order(x, pos2) == Ordering::Less);
            },
            Ordering::Equal => {
                // Find *all* matches and increment indexes.
                let count1 = (index1..input1.len()).take_while(|i| order(input1.get(*i), pos1) == Ordering::Equal).count();
                let count2 = (index2..input2.len()).take_while(|i| order(input2.get(*i), pos2) == Ordering::Equal).count();

                for i1 in 0 .. count1 {
                    let row1 = input1.get(index1 + i1);
                    for i2 in 0 .. count2 {
                        let row2 = input2.get(index2 + i2);
                        action(row1, row2);
                    }
                }

                index1 += count1;
                index2 += count2;
            },
            std::cmp::Ordering::Greater => {
                // advance `index2` while strictly less than `pos1`.
                gallop::<T>(input2, &mut index2, |x| order(x, pos1) == Ordering::Less);
            },
        }
    }
}
```
Straight out of datafrog, which took it straight out of somewhere else.
I didn't invent joins.

This is a merge join, in that it is moving sequentially through sorted inputs and looking for keys that match.
When it finds matching keys, it calls the `action` function, which as we know just packs facts into a `FactBuilder`.
When it doesn't find matching keys, it calls the `gallop` function, which I definitely stole from the [EmptyHeaded](https://ppl.stanford.edu/papers/emptyheaded.pdf) folks, and which is a bit like a binary search to move forward quickly to the next possible match.

You want to see `gallop`, you say?
```rust
/// Increments `index` until just after the last element of `input` to satisfy `cmp`.
///
/// The method assumes that `cmp` is monotonic, never becoming true once it is false.
/// If an `upper` is supplied, it acts as a constraint on the interval of `input` explored.
#[inline(always)]
pub(crate) fn gallop<'a, T: Columnar>(input: <T::Container as Container<T>>::Borrowed<'a>, index: &mut usize, mut cmp: impl FnMut(<T as Columnar>::Ref<'a>) -> bool) {
    let upper = input.len();
    // if empty input, or already >= element, return
    if *index < upper && cmp(input.get(*index)) {
        let mut step = 1;
        while *index + step < upper && cmp(input.get(*index + step)) {
            *index += step;
            step <<= 1;
        }

        step >>= 1;
        while step > 0 {
            if *index + step < upper && cmp(input.get(*index + step)) {
                *index += step;
            }
            step >>= 1;
        }

        *index += 1;
    }
}
```
That is literally the very end of `join.rs`.
We are now done with the code tour.

### Experimentation

I have some data from the [Graspan](http://web.cs.ucla.edu/~harryxu/papers/wang-asplos17.pdf) project, acquired at the time but [still available on Google Drive](https://drive.google.com/drive/folders/1DRj-cfISV9v34vU7DH1PKEu13PaMIf71).
If you want to follow along, put this fragment at the top of the `main.rs` file.
```rust
for arg in std::env::args().skip(1) {

    // Read input data from a handy file.
    use std::fs::File;
    use std::io::{BufRead, BufReader};

    let mut dict: BTreeMap<String, facts::FactBuilder> = BTreeMap::default();
    let filename = arg;
    let file = BufReader::new(File::open(filename).unwrap());
    for readline in file.lines() {
        let line = readline.expect("read error");
        if !line.is_empty() && !line.starts_with('#') {
            let mut elts = line.split_whitespace().rev();
            if let Some(name) = elts.next() {
                dict.entry(name.to_string())
                    .or_default()
                    .push(elts.rev().map(|x| x.as_bytes()));
            }
        }
    }
    for (name, facts) in dict { 
        state.facts
             .entry(name)
             .or_default()
             .add_set(facts); 
    }
}
state.update();
```

There are a few different inputs and different program analyses to look at.
We'll break them down by analyses (dataflow and points-to) and go from there.

#### Nullability analysis

I'm starting things up this way, with the `httpd` dataflow input and the `.list` command once we are in the shell to see what we are looking at.
```
mcsherry@gallustrate datatoad % cargo run --release -- ~/Projects/datasets/graspan-dataflow/httpd_df
    Finished `release` profile [optimized] target(s) in 0.00s
     Running `target/release/datatoad /Users/mcsherry/Projects/datasets/graspan-dataflow/httpd_df`

> .list
        e:      9905624
        n:      138331
39.458µs

> 
```
Two relations, `e` and `n`, corresponding to "edges" and "nodes" respectively.
The same will be true for the `psql` and `linux` inputs as well, for the dataflow analysis.

What is this dataflow analysis?
*   The `n(?a, ?b)` relation describes the potential write of some value `?a` to a location `?b`.
*   The `e(?a, ?b)` relation describes the movement of the value at one location `?a` to another location `?b`, as through assignment in a program.

We're doing a "reachability" query to see which values written to some location can end up at any other through a path of edges.
We can write this in Datalog as:
```
n(?a, ?c) :- n(?a, ?b), e(?b, ?c) .
```
In words, "if value `?a` may be written to `?b`, and `?b` may be copied to `?c`, then the value `?a` may be written to `?c`".

Let's key this rule in to the shell and see what happens!

---

**Narrator**: What happened was an error.
Always test your code, kids.

---

With the error fixed, and pushed to the repo, the result is
```
> n(?a, ?c) :- n(?a, ?b), e(?b, ?c) .
15.494418333s

> .list
        .temp-0-0-in:   9393283
        e:      9905624
        n:      9393283
45.375µs

> 
```

We took about 15s here, which .. is a number.
It's quite a lot faster than what Graspan reports for the same problem (684s).
But it's not the end of the story.

There's also this `.temp-0-0-in` up there, which isn't clearly anything we care about.
And it has .. roughly as many recorsd as `n`, and that number isn't small.
What's going on?

When we look at the join, we want to order the columns of `n` first by `?b` and then by `?a`, so that they can be joined with `e(?b, ?c)`, which already has `?b` up front.
As such, we need to make an intermediate collection that just re-orders `n`, re-sorts it, it won't need to re-dedup it but we'll do that as well.
So `.temp-0-0-in` is there only because we need to reshape `n` to make it work for the join.

We could try and load the data in differently, or sort `n` by its second coordinate, or something else complicated, but let's not.
A simpler thing we can do, by virtual of `n` being small initially, is to reshape it once and develop the reshaped variable instead.
Here's what that looks like:
```
m(?b, ?a) :- n(?a, ?b) .
m(?c, ?a) :- m(?b, ?a), e(?b, ?c) .
```
Pretty easy, with a bit of care taken to wiggle the variables around correctly.
In fact, screw this let's use proper names to make this clearer.
```
m(?loc, ?val) :- n(?val, ?loc) .
m(?loc, ?val) :- m(?mid, ?val), e(?mid, ?loc) .
```
I'd love to give `e`, `n`, and `m` better names, but for the moment that would involve copying the whole of the data around.
Could do, but it's not the performance no-op that using better variable bindings is.
For now.

Let's see what happens now.
```
> m(?loc, ?val) :- n(?val, ?loc) .
58.964458ms

> m(?loc, ?val) :- m(?mid, ?val), e(?mid, ?loc) .
8.434536333s

> .list
        .temp-0-0-in:   138331
        e:      9905624
        m:      9393283
        n:      138331
23.959µs

> 
```
This is now substantially faster, but it does still have a `.temp-0-0-in` in there.
This is my bad.
It corresponds to the first rule, remapping `n` to `m`.
There is no reason to have a non-identity map there, and I'll just need to tighten the logic for that (relatively untested) special case.

The good news is that we don't have a `.temp` relation for the join.
Both `e` and `m` are laid out the way the join wants them laid out, and we don't have to make or maintain copies of the data.
Other than that glitch in the copying.
But we can ignore the glitch for now.

---

**Narrator**: He could not ignore the glitch.

---

The glitch went unnoticed because even for single body atoms, we create a plan that orders the columns alphabetically, because it uses a `BTreeSet` to hold the names.
When we switched from `?a` and `?b` to instead use `?val` and `?loc`, for narrative reasons, that order changed.
It reveals a larger glitch, spanning joins as well, that body atoms have the *set* of their key columns dictated at them, but they can choose the *order* of these columns, and prefer their natural order if there is one that is compatible.

We'll fix this, but not right now.

What we've learned here is that there is a bit of query planning and optimization that is up to the user.
The initial shape of the data determines how efficient the plan can be, in a way that is tricky to fix without deferring computation until we see the ways the data will be used.
That's not the intent of our current Datalog interpreter, which eagerly performs work, but it's a reasonable thing to think about supporting.

At this point we can load up other datasets and hit them with the same analysis.
There is a `psql` dataset and several `linux` datasets.
My understanding of the `linux` datasets is that they are meant to be analyzed in isolation: they all use identifiers starting at `0` and going up, but they don't correspond to the same code locations across files.
The largest by far is `lnx_kernel_df`, and I'll just use that one.

I put the results together, reporting the time to perform the rule evaluation.
The time to load the data is not insubstantial, though it could/should be instantaneous (more on this when we do "spill to disk").

| dataflow | httpd | psql | lnx_kernel |
| ---       | ---: | ---: | ---: |
| graspan  | 684s | 8640s | 42840s* |
| datatoad | 8.43s | 24.33s | 55.01s |
| datafrog | 1.30s | 4.06s | 8.03s |

I had previously forgotten to subtract the loading time out for datafrog numbers, which led me to feeling more smug than I currently do about the datatoad numbers.
We will unpack where the time goes in a future update, but the answer is "dealing with the uncertainty of variable lengthed facts and literals".
Also datafrog hasn't had as many eyes on; perhaps there are some low hanging fruits others can see!

I put a `*` next to the Graspan `lnx_kernel` number because they report the times for everything together, but clearly not together like "run at the same time" because of the colliding identifiers.
The `lnx_kernel` input is twice the size of all the other inputs combined, so it's not directionally misleading, although absolutely factually inaccurate.

We have some overhead over datafrog we need to explain, but are already comfortably non-trivial with respect to Graspan.
The right point of comparison is probably Soufflé , which is the tool it seems like practicing professionals reach for, and which is certainly better than Graspan.

#### Aliasing analysis

A second analysis the Graspan folks consider, taken from [Zheng and Rugina](https://www.cs.cornell.edu/~xinz/papers/alias-popl08.pdf), is about analyzing the potential "aliasing" of values and memory locations.
Their formulation works with expressions of two flavors: values `e` and locations in memory `*e`.
They start from two input relations:
```
Assignment:     A(?val, ?loc)   // for each `?loc <- ?val`
Dereference:    D(?val, ?loc)   // for each `?loc` as `*?val`.
```
From these input data, they would like to determine two classes of aliasing (quoted from Zheng and Rugina):
* Memory (or location) aliases: two lvalue expressions are memory aliases if they might denote the same memory location;
* Value aliases: two expressions are value aliases if they might evaluate to the same pointer value.

If these are in any way confusing (they are to me) we can also just move forward with the data and the expressed analyses.
They move forward, describing memory and value aliasing thusly, using terms we'll need to unpack.
```
Memory aliases:  M ::= D^T V D
Value aliases:   V ::= F^T M^? F
Value flows:     F ::= (A M^?)^*
```
Ok.
What do these weird things mean?

The right hand sides are sequences of binary relations, where the intent is that `Q ::= X Y Z`  the existence of intermediate variables that link up the the relations into a path.
In Datalog we might write instead:
```
Q(?a, ?d) :- X(?a, ?b), Y(?b, ?c), Z(?c, ?d).
```
There are some additional weird symbols that we don't see in Datalog:
* The `^T` means "transpose", and it turns the relation around.
* The `^?` means "optionally", and the term can either be present or absent.
* The `^*` means "repeatedly", and allows any number of repetitions (including zero) of the term.

For example, we could write the `M` rule in Datalog as
```
M(?l1, ?l2) :- D(?v1, ?l1), V(?v1, ?v2), D(?v2, ?l2).
```
Notice how we handled the `^T` just by changing the order of the variables in first `D` atom.
Informally, the rule says that if values `?v1` and `?v2` are potentially aliased, then the memory locations that results from dereferencing them may be "memory aliased".
That seems like it could make sense.

If we try and do the other two rules we run in to the `^?` and `^*` terms, and .. they are annoying for Datalog.
Datalog doesn't really have a "yeah, that .. or could just skip it".

The `^?` operator is relatively easy to accommodate, just by repeating the rule with and without the term.
For value aliases, which may or may not involve a memory alias, we can write the two rules
```
V(?v1, ?v2) :- F(?v3, ?v1), F(?v3, ?v2).
V(?v1, ?v2) :- F(?l1, ?v1), M(?l1, ?l2), F(?l2, ?v2).
```

The `^*` operator is a bit harder, because one "alternative" rule would have an empty body.
Informally, this kinda means that it is unioned with the identity operator, which we may have to represent explicitly.
We'll do that for now, but there is another fix that involves dramatically expanding out the set of rules.
We'll discuss that if this approach ends up terrible.

The value flow term is any number (including zero) of steps along a `A M^?` path, which we can write as:
```
F(?val, ?unk) :- A(?val, ?loc), F(?loc, ?unk).
F(?val, ?unk) :- A(?val, ?loc), M(?loc, ?loc2), F(?loc2, ?unk).
```
I put `?unk` as the right hand variable of `F` because .. we don't really know what it is yet.
The rules say that you get whatever `?unk` you found from the existing `?unk`, but where does it all start from?

Informally, you start from the identity operator on the combination of all values and locations.
We'll need values because `F`'s first argument results from `A`'s first argument, a value.
We'll need locations because `F`'s first argument joins with locations.
We don't have permission to do anything less, so let's put all values and locations we have ever heard about into `F`.
```
F(?v, ?v), F(?l, ?l) :- A(?v, ?l).
F(?v, ?v), F(?l, ?l) :- D(?v, ?l).
```
These last rules are the unlock that both complete the definition of `F` and start the iteration.
Let's do that now.

While all of this was running, I went and read the Zheng and Rugina paper.
It's maybe a good moment to point out that while the Graspan paper took the analysis from Zheng and Rugina, the whole point of their paper was to avoid doing the sort of bottom-up analysis we are doing.
The Zheng and Rugina paper is literally "Demand-Driven Alias Analysis for C", where "demand-driven" means it tries to prove certain specific target facts, rather than all facts.
We'll come back to that in a bit.

```
> F(?v, ?v), F(?l, ?l) :- a(?v, ?l).
686.574892166s

> F(?v, ?v), F(?l, ?l) :- d(?v, ?l).
736.344523208s

> .list
        -a:     362799
        -d:     1147612
        .temp-0-1-in:   361947256
        .temp-0-1-mid:  172922754
        .temp-2-1-in:   92806768
        .temp-2-1-mid:  173310916
        .temp-3-0-in:   362799
        .temp-4-0-in:   362799
        .temp-4-1-in:   92806768
        .temp-4-1-mid:  173310916
        .temp-5-0-in:   362799
        .temp-6-0-in:   1147612
        F:      2669647
        M:      92806768
        V:      361947256
        a:      362799
        d:      1147612
145.333µs

> 
```

Oh boy that took a while.
My the process is also sitting at 50.13GB, on my laptop, so .. that spilling to disk thing seems to already be happening.
Let's look a bit closer and see if we can't dial in the numbers a bit more by writing the query more carefully.
Then we'll discuss the whole "demand-driven" thing.

Let's start by looking at these temporary relations.
Why do they each exist?
* The `.temp-0-1-in` term is `V` "backwards" in the production of `M`.
* The `.temp-2-1-in` term is `M` "backwards" in the production of `V`.
* The `.temp-3-0-in` term is `a` "backwards" in the production of `F`.
* The `.temp-4-0-in` term is `a` "backwards" in the production of `F`.
* The `.temp-4-1-in` term is `M` "backwards" in the production of `F`.
* The `.temp-5-0-in` term is `a` "backwards" in the production of `F`.
* The `.temp-6-0-in` term is `d` "backwards" in the production of `F`.

The two big relations here are `V` and `M`, which are each "backwards" in *all* of their uses.
The `a` and `d` relations are pretty small, but we can replace their uses with `-a` and `-d`, which are the Graspan names for the transposes (loaded as part of the input data).
Let's instead produce and use `-V` and `-M`, following the Graspan nomenclature, flipping the order of their columns and making our rules now:
```
-M(?l2, ?l1) :- d(?v1, ?l1), -V(?v2, ?v1), d(?v2, ?l2) .

-V(?v2, ?v1) :- F(?v3, ?v1), F(?v3, ?v2).
-V(?v2, ?v1) :- F(?l1, ?v1), -M(?l2, ?l1), F(?l2, ?v2).

F(?val, ?unk) :- -a(?loc, ?val), F(?loc, ?unk).
F(?val, ?unk) :- -a(?loc, ?val), -M(?loc2, ?loc), F(?loc2, ?unk).
F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
```

As we key in the final two rules, the system kicks into action again
```
> F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
418.523364167s

> F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
397.395456458s

> .list
        -M:     92806768
        -V:     361947256
        -a:     362799
        -d:     1147612
        .temp-0-1-mid:  172922754
        .temp-2-1-mid:  173310916
        .temp-4-1-mid:  173310916
        F:      2669647
        a:      362799
        d:      1147612
531.042µs

> 
```
We've removed all of the `-in` temporary relations, and the process is now at 31.96GB.
The numbers of tuples in the `-M`, `-V`, and `F` relations match their counterparts in the previous execution, so we've almost certainly produced the same thing.
It might even be correct, but let's not get too unrealistic.
We do still have the temporary intermediate results for the three-way joins, but we'll have to wait to learn about streaming joins before we can remove them.

Our total ends up at 815.92s, or 13.6 minutes, which compares favorably with Graspan's 8.3 hours.
Still not sure if we are computing the right thing, so let's only be lightly pleased with ourselves.
But pleased nonetheless.

---

Let's do a next unlock, and read the Zheng and Rugina more carefully.
When they get to explaining their solution to the problem, relatively abstract up until this point, we learn a very important detail

> We now present the demand-driven algorithm for answering memory may-alias queries. 
> Given two lvalue expressions e1 and e2, the algorithm determines whether M (e1, e2) holds.

They only need to produce `M` as an output!
And really they only need to be able to *probe* `M`, which is what that "demand-driven" term means, but let's talk about `M` first.
If you only need to produce `M`, you *don't* need to produce `V`, which is great news because `V` is enormous.
But don't we need `V` to produce `M`? No, not really.

The `V` relation contains *all* value aliasing, but `M` is only interested in value aliasing for addresses: values that appear as the first column in `d`.
Why the hell are we computing all of `V` then?
Let's inline the definition of `V` into `M` (make sure to choose fresh variable names; I sure didn't the first time!).
```
-M(?l2, ?l1) :- d(?v1, ?l1), F(?v3, ?v1), F(?v3, ?v2), d(?v2, ?l2) .
-M(?l2, ?l1) :- d(?v1, ?l1), F(?l3, ?v1), -M(?l4, ?l3), F(?l4, ?v2), d(?v2, ?l2) .
F(?val, ?unk) :- -a(?loc, ?val), F(?loc, ?unk).
F(?val, ?unk) :- -a(?loc, ?val), -M(?loc2, ?loc), F(?loc2, ?unk).
F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
```
Keyed in, this results in
```
> F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
234.626393458s

> F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
98.101901291s

> .list
        -M:     92806768
        -a:     362799
        -d:     1147612
        .temp-0-1-mid:  17768284
        .temp-0-2-in:   2669647
        .temp-0-2-mid:  1859886
        .temp-1-1-mid:  172280896
        .temp-1-2-mid:  73474947
        .temp-1-3-in:   2669647
        .temp-1-3-mid:  1859886
        .temp-3-1-mid:  173310916
        F:      2669647
        a:      362799
        d:      1147612
86.75µs

> 
```
The `-M` and `F` counts have stayed the same, so probably still the right answer.
The `-V` count is gone, which was the whole point.
The process is now sitting at 18.96GB, so that was clearly a bit of a savings.

We do have some new join plan temporaries now.
Notice in particular that the sizes of `.temp-0-2-mid` and `.temp-1-3-mid` are identical.
They both correspond to the join of the two atoms at the end of the first two rules `F(?v3, ?v2), d(?v2, ?l2)`.

Also, if you turn your head upside down and inside out, you can see that these are the same as the *first* two atoms in each of these joins.
Why don't we just give them a name and use that.
We'll call it `Fd` to evoke the sequencing of the two.
```
Fd(?v3, ?l2) :- F(?v3, ?v2), d(?v2, ?l2).
```
Rewritten, the `-M` productions are now a fair bit shorter.
```
-M(?l2, ?l1) :- Fd(?v3, ?l1), Fd(?v3, ?l2) .
-M(?l2, ?l1) :- Fd(?l3, ?l1), -M(?l4, ?l3), Fd(?l4, ?l2) .
F(?val, ?unk) :- -a(?loc, ?val), F(?loc, ?unk).
F(?val, ?unk) :- -a(?loc, ?val), -M(?loc2, ?loc), F(?loc2, ?unk).
F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
```
Again, keying in the final two rules kick things off
```
> F(?v, ?v), F(?l, ?l) :- -a(?l, ?v).
163.471007292s

> F(?v, ?v), F(?l, ?l) :- -d(?l, ?v).
61.675481125s

> .list
        -M:     92806768
        -a:     362799
        -d:     1147612
        .temp-0-0-in:   2669647
        .temp-2-1-mid:  73474947
        .temp-4-1-mid:  173310916
        F:      2669647
        Fd:     1859886
        a:      362799
        d:      1147612
103.333µs

> 
```
Again, a pretty nice improvement.

One last thing to do for now.
The only thing we do with `F` is put a `d` on the end.
Also `Fd` is a bit smaller than `F`, so it seems like we didn't need everything we produced.
We can just compute `Fd` directly, starting from `d` and pre-pending any number (including zero) of `(a M^?)`.
This also solves the problem we have with the identity relation: `Fd` always has a `d` in it.
```
-M(?l2, ?l1) :- Fd(?v3, ?l1), Fd(?v3, ?l2) .
-M(?l2, ?l1) :- Fd(?l3, ?l1), -M(?l4, ?l3), Fd(?l4, ?l2) .
Fd(?val, ?unk) :- -a(?loc, ?val), Fd(?loc, ?unk).
Fd(?val, ?unk) :- -a(?loc, ?val), -M(?loc2, ?loc), Fd(?loc2, ?unk).
Fd(?val, ?loc) :- -d(?loc, ?val).
```
Now we only have to key in that last rule, and get
```
> Fd(?val, ?loc) :- -d(?loc, ?val).
162.082759041s

> .list
        -M:     92806768
        -a:     362799
        -d:     1147612
        .temp-1-1-mid:  73474947
        .temp-3-1-mid:  73474947
        Fd:     1859886
        a:      362799
        d:      1147612
34.333µs

> 
```
We're seeing exactly the same thing, which is that the intermediate result `-M Fd` is computed twice, and is pretty chonky.
Let's fix that.
For reasons that are fundamentally unfair to me, we'll want `MFd` without a `-` in front of it.
```
MFd(?l1, ?l2) :- -M(?l3, ?l1), Fd(?l3,?l2).
-M(?l2, ?l1) :- Fd(?v3, ?l1), Fd(?v3, ?l2) .
-M(?l2, ?l1) :- Fd(?l3, ?l1), MFd(?l3, ?l2).
Fd(?val, ?unk) :- -a(?loc, ?val), Fd(?loc, ?unk).
Fd(?val, ?unk) :- -a(?loc, ?val), MFd(?loc, ?unk).
Fd(?val, ?loc) :- -d(?loc, ?val).
```
As usual, the last rule kicks off the work
```
> Fd(?val, ?loc) :- -d(?loc, ?val).
119.34187225s

> .list
        -M:     92806768
        -a:     362799
        -d:     1147612
        Fd:     1859886
        MFd:    73474947
        a:      362799
        d:      1147612
31.666µs

> 
```
And we're sitting at 5.32GB.
That's about a 10x improvement in memory from our first attempt at the plan, and nearly a 10x improvement in running time as well.
I have no idea if we've computed anything of value, but I'm pretty confident that we've computed the *original thing*, only a fair bit faster.
In fact, I think we've largely gone backwards through time, performing the technique that Zheng and Rugina departed from: explicit points-to sets that we intersect.

---

This was a not-unpleasant afternoon for me!

We did a bit of stress testing the tool, and managed to explore query optimization without needing to rewire the tool at all (except for a few bug fixes).a
We saw a series of program transformations that were *mostly* mechanical, and you could totally imagine a query optimizer doing these for you.
An order of magnitude improvement from naively stated rules is an amazing testament to the potential of query optimizers.

We also learned that you can get any bushy-tree join plan you like by naming and forming the indexed intermediate results, like we did with `Fd`.
A lot of "declarative" languages have the frustrating defect that you can't always get the plan you want, but that doesn't seem to be the case here.
We also saw the corresponding downside, that if you name a thing (e.g. `V`) that you don't actually need we'll still produce it for you, potentially at great cost.

All in all, I'm pleased with this experiment so far.
Up next, I'd like to grab more examples of program analysis in Datalog and see how they work out.
Maybe collect a bit more experience with the space of bespoke optimizations before trying to automate any of them.

---

**Update:** I forgot to talk about "demand-driven" queries.
We'll get there in time, but the answer is to a first approximation "magic sets", a query transformation that embeds target literals into the query.
Think for example of starting `Vd` not from *all* `d`, just the ones you are interested in.
That would be wrong, it turns out, but there is a general way to transform queries to only explore facts that are "demanded", hence "demand-driven".
It turns out that magic sets are not the optimal answer, [several](http://logic.stanford.edu/logicprogramming/readings/tekle.pdf) [papers](http://logicprogramming.stanford.edu/readings/ullman.pdf) explain how they could be more efficient.
I'll be reading them looking for the *simplest* way to do things, but in the future.

## Spilling facts to disk

## Scaling rules horizontally

## Streaming facts through rules

## Specializing to compiled code

## Specializing to custom logic