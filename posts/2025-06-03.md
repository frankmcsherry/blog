# Datalog in Rust

Over the Memorial Day weekend [Kris Micinski](https://kmicinski.com) hosted a logic programming workshop in the delightful [Minnowbrook Conference Center](https://minnowbrook.org) in upstate New York.
I, a notorious villain, was invited for what I was half sure was my long-due comeuppance.
As it turned out it was delightful, with lots of friendly and supportive people with shared goals.

We had one requirement, which was that we write a blog post reflecting on the event, which Kris would collect and share out with the wider world.
You are going to get a bunch of thoughtful, considerate takes on the workshop, so I thought I'd go in a different direction.
As great as the time was, it struggled with a standard academic problem: smart people working on challenging problems that don't always directly connect to the challenges at hand.
Outputs without outcomes, as they say in Product-speak.

After a few days of Datalog (and other logic programming languages) talks, the one that stood out was the last talk by [Denis Bueno]() on [ctdal](https://github.com/sandialabs/ctadl).
It's program analysis in Datalog!
But .. boy was it a struggle, to hear it.
Many tools didn't work; [Soufflé](http://souffle-lang.github.io) did, but you have to hold it right.
Many challenges, and .. to be honest the bad news in the best news if you want to make progress.

I figured I should put my money-time where my mouth-brain is, and see what it is like to build a thing that is meant to be usable as well as "performant".
Plus, I figure everyone benefits from seeing what it's like to build such a thing too, and you'll even get to learn about logic programming!

## Let's build an interactive Datalog, in Rust!

I've built a few things like Datalog in my day, but one thing I haven't done is try and make a thing that is all three of simple, useable, and performant.
I thought we might try and do that here, with Datalog!
You can follow along in the [datatoad repository](https://github.com/frankmcsherry/datatoad).

I have previously built much of [datafrog](https://crates.io/crates/datafrog) (with help!), which provides the guts of a Datalog engine that you are then invited to wire together your own damned self.
Understandably, this was not the Datalog renaissance you might imagine.
We aren't going to start from datafrog, but we'll use a lot of the same algorithmic ideas.
If you are familiar with it you should be able to follow along, and if you are not familiar with it you're doing just fine.

I had an outline and post that started by building a bad Datalog, a slow and awkward Datalog, that we would then improve together as we learned things.
It was sufficiently bad that I think we should just start with the good version, and we can talk through the load-bearing moments in more detail as we go.

By the end of this post, we'll have an interactive Datalog that we can bulk load facts into, add rules on the go, and have it keep up with pretty good performance.
Here's an example that does a nullability analysis on an `httpd` dataflow (in the PL sense) graph;
```
> .list
    e:	9905624
    n:	138331
30.708µs

> m(?b, ?a) :- n(?a, ?b) .
52.027834ms

> .list
    e:	9905624
    m:	138331
    n:	138331
45.916µs

> m(?c, ?a) :- m(?b, ?a), e(?b, ?c) .
8.360353834s

> .list
    e:	9905624
    m:	9393283
    n:	138331
49.042µs

> 
```
This takes about 2s in datafrog, vs 8.3s here, though the datafrog example uses `(u32, u32)` data, but here with `Vec<String>` data and no compiled queries it takes only 4x longer.
I hope to get that down in the future.

I haven't confirmed that this does a great job on all problems yet.
I haven't even confirmed that it is generally correct, though on these reachability problems it produces the same number of output tuples as the datafrog implementation.
So for the moment, no too worried about performance, and more worried about ergonomics, explainability, and future extensibility.


Here's an overview of my plan for what we'll cover.
I'm aspirationally describing each of the sections having not yet done all of the work, but I have high hopes!
Much of this derives from prior work on the [datafrog](https://crates.io/crates/datafrog) project, but we'll start a bit slower and in more depth, and end up going further as well.

My plan is to go top-down, starting with "what is Datalog", and building the framework for an interactive evaluator.

0. introducing datalog.

Once we have overview in place, we'll start to build up what are currently the three loci of functionality.

1. Parsing Datalog
2. Representing and maintaining sets of facts
3. Planning and evaluating Datalog rules.

There are some very natural next steps.
I haven't done these yet, but I'm certain they can be done without a great deal of effort.
I'll put them here, and if you are especially excited about them and for some reason I haven't already done them, use this paragraph to compel me to do so.

4. Spilling everything to disk.
5. Scaling out to multiple workers and processes.
6. Streaming and worst-case optimal joins.

I have a few more tricks planned, as the project is also partly about unlocking an ability to experiment with new data-parallel computation patterns.
It turns out most of what we'll be doing is mechanically very similar to [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), though substantially simpler (and less capable).

## Introducing Datalog

[Datalog](https://en.wikipedia.org/wiki/Datalog) is a language in which you describe simple logical rules, and it derives all of the consequences that can be reached from them.
The rules are each [Horn clauses](https://en.wikipedia.org/wiki/Horn_clause), which are a grim way of describing rules that takes a set of input facts to an output fact.
These logical statements have placeholder variables, and part of the fun of Datalog is that it will fill all of the possible settings of the values.

Datalog rules are shaped as a "head" which is the thing (or things) that could be inferred, followed by `:-` and then a "body" which is a list of sufficient conditions.
For example, we might write a rule that says that a triangle (a, b, c) exists if each of three edges exist:
```
tri(a, b, c) :- edge(a, b), edge(b, c), edge(a, c).
```
The names `tri` and `edge` are relations, and the variables `a`, `b`, and `c` are free variables.
The rule is implicitly universally quantified over the variables, meaning that for any setting of the variables, if the body is true for those settings then the head is true for those settings as well.
We are going to exclude rules that have variables in a head atom that are not present in the body.

We often forget when writing Datalog, but there are also "facts" that are like rules with an empty body, meaning they are true unconditionally.
For example, we might state that the following three edges exist through the facts
```
edge(1, 2) :- .
edge(1, 3) :- .
edge(2, 3) :- .
```

We could also write this using "multiple heads" as:
```
edge(1, 2), edge(1, 3), edge(2, 3) :- .
```


When rules and facts combine, more facts are inferred.
With these facts, we would expect to infer that `tri(1, 2, 3)` is true.
This now becomes a fact itself, and might lead to more facts if we had rules that relied on `tri`.

Datalog has an appealing monotonicity property: as you add more rules (including facts), the set of things that are true only increases.
Moreover, Datalog is confluent, in that no matter what order you put these rules in you'll end at the same set of facts for any set of input rules.

At this point, I hope you can imagine typing rule after rule into a Datalog shell, and watching the consequences of these rules spill back out at you.
And, I hope just after imagining this you are imagining them spilling out so quickly because we have done such a good job making this work well, that actually you want to read from and write to files, and use this for all sort of real (or at least plausible) applications.

### How to represent facts and rules

Let's start with rules, as facts end up as a special but different case of rules.

A rule is two lists of *atoms*, the list before the turnstile called the "head" and the list after the turnstile the "body".
Whenever all atoms in the body evaluate to true, the atoms of the head must also be true.
```rust
struct Rule {
    head: Vec<Atom>,
    body: Vec<Atom>,
}
```
An atom is a named relation, as well as a number of *terms* that matches the arity of the relation.

```rust
struct Atom {
    name: String,
    terms: Vec<Term>,
}
```
Each term is either a named variable or, a simplifying choice we'll make, a string literal.
```rust
enum Term {
    Var(String),
    Lit(String),
}
```
There's no strong reason to make it a string literal.
Any types can work, and often folks use integers or other compact types.
In fact, it's an outright lie and we'll really use `Vec<u8>` as the type, and if you'd like those bytes to mean `String` or `(u32,u32)` or something else, you'll be welcome to do so.
The only properties we'll use are literal equality and an arbitrary order, and using `Vec<u8>` intentionally doesn't close any doors.

Facts are rules that have an empty body, and heads with no variables.
We can treat facts as data, recording the list of literal strings in each position.
Although not where we will end up, you can think of them for now as 
```rust
type Fact = Vec<String>;
```
We'll also collect lists of facts and associate them with names, as our representation of relations.
Again, not exactly where we'll end up, but a great starting intuition.
```rust
type Facts = BTreeMap<String, Vec<Fact>>;
```
Finally, we'll bundle our facts and rules into our Datalog interpreter state.
```rust
struct State {
    /// Rules that are not facts.
    rules: Vec<Rule>,
    /// Rules with an empty body and only literals in the head.
    facts: facts::Facts, // as yet undisclosed
}
```

Up next, what do you even do with all these things?

### An overview of operation

We're going to write a fairly simple skeleton of our interactive Datalog shell.
It will set up some `State`, repeatedly read rules from the input, and apply them to our facts until no new facts emerge.
If you type a name rather than a rule, we'll print out the relation with that name.

The code uses a few methods on `State` we haven't defined yet.
We'll unpack each of these in coming sections.
```rust
fn main() {

    let mut state = State::default();

    use std::io::Write;
    println!("");
    print!("> ");
    let _ = std::io::stdout().flush();

    let mut text = String::new();
    while let Ok(_size) = std::io::stdin().read_line(&mut text) {

        if let Some(parsed) = parse::datalog(&text) {
            state.extend(parsed);
            state.update();
        }

        else {
            match text.trim() {
                ".list" => {
                    for (name, facts) in state.facts.iter() {
                        println!("\t{}:\t{:?}", name, facts.len());
                    }    
                }
                _ => {
                    println!("Parse failure: {:?}", text);
                }
            }
        }

        println!("");
        print!("> ");
        let _ = std::io::stdout().flush();
        text.clear();
    }

}
```

There are three methods in there we haven't seen yet: `parse::datalog`, `State::extend`, and `State::update`.
The Datalog parsing is its own self-contained bit, going from lines of text to lists of rules; we'll cover it in the next section.
Before heading to that, let's see the `extend` method and discuss the `update` method which is the beating heart of Datalog.

```rust
impl State {
```
The `extend` method mostly just looks for facts, which are rules whose heads have only literals in them, and with an empty body.
We store them explicitly as lists of data, in the `facts` member variable by the name of the head atom (the name of the relation).
```rust
    /// Adds new rules to the state.
    pub fn extend(&mut self, rules: impl IntoIterator<Item=Rule>) {
        for rule in rules.into_iter() { self.push(rule); }
    }

    /// Adds a new rule to the state.
    pub fn push(&mut self, rule: Rule) {
        if rule.body.is_empty() {
            for atom in rule.head.iter() {
                let mut lits = Vec::with_capacity(atom.terms.len());
                for term in atom.terms.iter() {
                    if let Term::Lit(text) = term {
                        lits.push(text.to_string().into_bytes());
                    }
                    else { continue; }
                }
                let mut builder = facts::FactBuilder::default();
                builder.push(lits);
                self.facts
                    .entry(atom.name.to_owned())
                    .or_default()
                    .add_set(builder);
            }
        }
        else {
            let rule_plan = crate::join::JoinPlan::from(&rule);
            join::implement_plan(&rule, &rule_plan, self.rules.len(), true, &mut self.facts);
            self.rules.push(rule);
        }
    }
}
```
There are some `join::` things going on here.
Hold your breath for a bit; they are a necessary step to kick off some work when a new rule shows up.
We'll get there, but for the moment just read them as "start the derivations for that new rule".

The `update` method is the beating heart of Datalog, where we repeatedly derive facts continuing as long as any new fact has been derived.
Once we fail to derive any new facts, well we won't derive any more by just trying again, so we are done for the moment.
```rust
/// Applies all rules to all facts, and indicates if new facts were
pub fn update(&mut self) {
    self.advance();
    while self.active() {
        for (index, rule) in self.rules.iter().enumerate() {
            let rule_plan = crate::join::JoinPlan::from(rule);
            join::implement_plan(rule, &rule_plan, index, false, &mut self.facts);
        }
        self.advance();
    }
}
```
There are some helper methods in there that we use to move our understanding of what we've learned forward.
Informally, `active` tells us whether there are new facts still to process, and `advance` checks all new facts against the old facts for novelty and gets us ready to go again.
```rust
/// True iff any relation has new facts
fn active(&self) -> bool {
    self.facts
        .values()
        .any(|x| x.active())
}

/// Checks new facts against old facts.
pub fn advance(&mut self) {
    for facts in self.facts.values_mut() {
        facts.advance();
    }
}
```
It turns out there is a lot of secret stuff going on in `facts.advance()`, and we'll get there once we unpack more about how we plan to represent sets of facts.
Before that, a segue into parsing Datalog.

## How to parse rules from text

We are going to write a Datalog parser.
If I took an undergraduate class on lexing and parsing, I can't remember it, so this is going to be a not-great way to get Datalog rules from lines of text.

The code that follows is currently `parse.rs` in the project.

This is also the moment where I reveal the actual grammar, which is lifted from Soufflé.
In particular, variables start with `?` to distinguish them from literals.
I could imagine prefering the other way around, using `"` to indicate string literals, but I figured tracking Soufflé might make everyone else's lives easier.

### Tokenization

The first step in many a parser is "tokenization": turning the sequence of characters into a sequence of more meaningful symbols.

For Datalog, we have just a few significant tokens: `.`, `,`, `(`, `)`, `:-`, and `?` (used to distinguish variables from literals).
Everything that is not one of these will be a string token, describing an atom or term name.
```rust
/// Text translated to a sequence of tokens.
#[derive(Debug, PartialEq)]
enum Token {
    Period,
    Comma,
    LParen,
    RParen,
    Turnstile,
    Question,
    Text(String),
}
```
We will need to add `Exclamation` whenever we get to adding negation to our rules.
Not today!

The tokenizer I wrote removes all whitespace, and converts `:-` into `←` to have a single symbol to scan for.
More ambitious individuals could find a better way to write this.
```rust
fn tokenize(text: &str) -> Option<Vec<Token>> {

    let mut text = text.replace(":-", "←");
    text.retain(|c| !c.is_whitespace());

    let mut result = Vec::new();

    let pattern = ['.', ',', '(', ')', '←', '?'];
    for token in text.split_inclusive(&pattern) {
        let mut split = token.split(&pattern);
        let prev = split.next().unwrap();
        if !prev.is_empty() {
            result.push(Token::Text(prev.to_owned()));
        }
        let next = token.chars().rev().next().unwrap();
        result.push (
            match next {
                '.' => Token::Period,
                ',' => Token::Comma,
                '(' => Token::LParen,
                ')' => Token::RParen,
                '←' => Token::Turnstile,
                '?' => Token::Question,
                _ => { None? } 
            }
        );
    }
    
    Some(result)
}
```
As you can see, some horror around finding the key moments for these symbols and breaking things apart into tokens.
The challenge I had with Rust was convincing it to show me these locations, without also consuming the symbols.
Again, I can imagine a better implementation.

### Parsing

We'll parse a token sequence by repeatedly peeking at the next token, and then attempting to extract one of our types: rules, made of atoms, made of terms.
```rust
fn parse(tokens: Vec<Token>) -> Option<Vec<Rule>> {
    let mut tokens = tokens.into_iter().peekable();
    let mut rules = Vec::new();
    while tokens.len() > 0 {
        rules.push(parse_rule(&mut tokens)?);
    }

    Some(rules)
}
```
Rule parsing extracts atoms until a turnstile, then atoms until a period.
```rust
fn parse_rule<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Rule> {
    let mut head = Vec::new();
    while &Token::Turnstile != tokens.peek()? {
        if &Token::Comma == tokens.peek()? { tokens.next(); }
        head.push(parse_atom(tokens)?);
    }
    let Token::Turnstile = tokens.next()? else { None? };
    let mut body = Vec::new();
    while &Token::Period != tokens.peek()? {
        if &Token::Comma == tokens.peek()? { tokens.next(); }
        body.push(parse_atom(tokens)?);
    }
    let Token::Period = tokens.next()? else { None? };

    Some(Rule { head, body })
}
```
Atom parsing extracts a name and left parenthesis, terms as long as there are commas, then expects a right parenthesis.
```rust
fn parse_atom<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Atom> {
    let Token::Text(name) = tokens.next()? else { None? };
    let Token::LParen     = tokens.next()? else { None? };

    let mut cols = Vec::new();
    cols.push(parse_term(tokens)?);
    while let Token::Comma = tokens.peek()? {
        tokens.next();
        cols.push(parse_term(tokens)?);
    }
    let Token::RParen     = tokens.next()? else { None? };
    Some(Atom { name: name.to_owned(), cols })
}
```
Term parsing checks for an optional question mark, and then expects a text name.
```rust
fn parse_term<I: Iterator<Item=Token>>(tokens: &mut Peekable<I>) -> Option<Term> {
    if let Token::Question = tokens.peek()? {
        tokens.next()?;
        let Token::Text(term) = tokens.next()? else { None? };
        Some(Term::Var(term.clone()))
    }
    else { 
        let Token::Text(term) = tokens.next()? else { None? };
        Some(Term::Lit(term.clone()))
    }
}
```
Malformed rules result in a `None` result, with no great effort made to clearly communicate what went wrong.
As you might expect by this point, we could have tried harder and made this better.

## How to represent sets of facts

We said above that facts will `Vec<String>`, and sets of facts are `Vec<Fact>`.
That wasn't entirely accurate.

The problem is that `Vec<Vec<String>>` is a memory management nightmare.
Allocations of allocations of allocations.
Memory everywhere .. else; not where you were just looking.
We are going to use a different physical representation of the same information.

This section corresponds to `facts.rs` in the project.

### Data oriented design / columns

Our project has (at this moment) one external dependence: [`columnar`](https://crates.io/crates/columnar): a crate that converts Rust types (like `Vec<Fact>`) into a flat layout of only a few linear allocations.
Informally, it packs all the strings together in one massive `Vec<u8>`, and then it records the offsets that delineate `String` boundaries, and then again offsets in that list that delineate `Fact` boundaries.
But it does this all for you with the magical incantation
```rust
<Fact as Columnar>::Container
```
This is the type we will use to represent sets of facts.
It's such a good idea, we'll even give it a name.
```rust
/// A sorted list of distinct facts.
struct FactContainer {
    ordered: <Fact as Columnar>::Container,
}

impl std::ops::Deref for FactContainer {
    type Target = <Fact as Columnar>::Container;
    fn deref(&self) -> &Self::Target {
        &self.ordered
    }
}
```
We are following a Rust idiom of "wrapper types".
The `FactContainer` type wraps a list of facts, but also indicates that the sequence is sorted and deduplicated.
Any time we have one of these, we'll be assured of these properties (as long as we build them carefully; implementation coming up in a moment).

A key downside of columnar containers is that they are basically append only.
In principle with a `Vec` you could swap a fact around halfway through a list, or change a string literal it contains.
We aren't going to want to do that though, so we are mostly good.

One thing we'll want to be able to do is add facts to a list, and if we need to keep it sorted and deduplicated we now have a bit of a problem.
We can't change the list, other than by appending to it, and appending may not preserve the sorted order.
We'll need a different technique to accumulate sets of facts than just the `FactContainer`.

Were going to use a [log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) to represent our sets of facts.
This may sound complicated, but it really just means "a list of `FactContainer`s".
```rust
/// A list of sets of fact that double in length.
#[derive(Clone, Default)]
pub struct FactLSM {
    pub layers: Vec<FactContainer>,
}
```
The key property with an LSM is that the "layers" grow in size geometrically.
To add some facts, you just put your `FactContainer` into the list, and then we tidy the list by merging layers whose sizes are within 2x of each other.
Here's one way you can do that.
```rust
impl FactLSM {
    /// Adds a layer and restores the invariant.
    fn push(&mut self, layer: FactContainer) {
        self.layers.push(layer);
        self.tidy();
    }

    /// Ensures layers double in size.
    fn tidy(&mut self) {
        self.layers.sort_by_key(|x| x.len());
        self.layers.reverse();
        while let Some(pos) = (1..self.layers.len()).position(|i| self.layers[i-1].len() < 2 * self.layers[i].len()) {
            while self.layers.len() > pos + 1 {
                let x = self.layers.pop().unwrap();
                let y = self.layers.pop().unwrap();
                self.layers.push(x.merge(y));
                self.layers.sort_by_key(|x| x.len());
                self.layers.reverse();
            }
        }
    }
}
```
There is a `FactContainer::merge` function we use here, which does the expected thing on two sorted lists of distinct facts: it merges the two into one sorted list of distinct facts.

The LSM is a handy way to maintain a set of facts as you work with it, especially if you do not need it in its final form right yet.
We'll eventually want to convert all of those layers into one `FactContainer`, but we can put that off until we are ready.
It's such a useful idiom that we'll also take a moment to introduce a helper type we'll use to do the building for us:
```rust
/// Staging area for collecting fact sets.
pub struct FactBuilder {
    /// Un-sorted, possibly repeated facts.
    active: <Fact as Columnar>::Container,
    /// Sorted, deduplicated facts.
    layers: FactLSM,
}
```
This type has space in `active` for disorderly facts, but once they get numerous enough it will convert them into a `FactContainer` and add them to its `layers`.

As a final bit of fact-organization, we'll want a way to reflect the fact "lifecycle", from a new but potentially repeated fact, to a recent novel fact, to facts that we have fully processed.
We'll do that with three distinct collections, .. ah .. two of which are LSMs for some reason, and not the third.
```rust
pub struct FactSet {
    /// Newly arrived facts that may not be novel.
    pub to_add: FactLSM,
    /// Distinct facts that are due to be processed.
    pub recent: FactContainer,
    /// Distinct facts that have been fully processed.
    pub stable: FactLSM,
}
```
In fact, `FactSet::advance` is one of the missing functions from the initial section.
It promised to do something about deduplicating facts, and "getting us ready to go again".
Let's see what that actually means.
```rust
/// Moves recent into stable, then to_add into recent.
pub fn advance(&mut self) {
    // Move recent into stable
    if !self.recent.is_empty() {
        self.stable.push(std::mem::take(&mut self.recent));
    }

    // Convert the LSM to one list of facts.
    if let Some(to_add) = self.to_add.flatten() {

        // Tidy stable for the work we are about to do.
        self.stable.tidy_through(2 * to_add.len());

        // Remove from to_add any facts already in stable.
        let mut starts = vec![0; self.stable.layers.len()];
        let stable = &self.stable;
        self.recent = to_add.filter(move |x| {
            starts.iter_mut().zip(&stable.layers).all(|(start, layer)| {
                crate::join::gallop::<Fact>(layer.borrow(), start, |y| y < x);
                *start >= layer.borrow().len() || layer.borrow().get(*start) != x
            })
        });
    }
}
```
That last closure is a mess and you should complain.
It's checking for each element of `to_add` whether it can be found anywhere in the stable LSM.
It's just .. kinda hard to read.

The lifecycle of facts, going from `to_add` to `recent` to `stable`, is what will guide our hand as we work to derive new facts, without redoing work we've already done for existing facts.
That's what's coming up next.

## How to apply rules to facts

Our last (finally!) step is to put down the logic that takes us from rules and facts, to even more facts.
It's here that several seemingly arbitrary choices come together.
But it's also pretty head stuff, as we try and understand what Datalog rules even mean.

This is the subject of the `join.rs` file in the project.

### Datalog rules are Joins

When we have a rule like our triangle rule, updated with the new `?` notation,
```
tri(?a, ?b, ?c) :- edge(?a, ?b), edge(?b, ?c), edge(?a, ?c).
```
we'll want to first try to apply it with the facts we have, from `edge`.
This might seem overwhelming at first, but we can approach it a few ways.

First, observe that there are only so many settings of `?a`, `?b`, and `?c` to actual literal values.
Each of the literal values need to come from a fact involving `edge`.
The `?a` values need to come from the first coordinate, and the `?c` values need to come from the second coordinate.
We could plausibly just consider all settings of these values.
It's a lot, but not infinite or anything.

Second, observe that all possible settings of these values is not infinite but it is really too many so we'll have to be smarter.
Let's try taking a smaller bite at a time, and start by finding the values a, b, and c that satisfy the last two `edge` constraints.
Let's give that the name `vee`, and rewrite our rule in terms of it:
```
vee(?a, ?b, ?c) :- edge(?b, ?c), edge(?a, ?c).
tri(?a, ?b, ?c) :- edge(?a, ?b), vee(?a, ?b, ?c).
```
I chose last two rather than first two, and it's totally arbitrary but the code works out better if we collapse towards the head.
If you want it the other way around, you can call `body.reverse()` just after you parse the rule.

We now have two rules, each of which has a body with a pair of relations.
I assure you that this produces the same results, and now we can think about just solving the problem for bodies containing only two atoms.
Specifically, let's try and find literal triples that work for `vee`.

We are looking at a thing called an ["equijoin"](https://en.wikipedia.org/wiki/Join_(relational_algebra)) in relational databases.
We have two relations, some coordinates of which must match, and we'd like to find all assignments of values where that can happen.
The unlock with equijoins is noticing that `?c` is present in both atoms, and we can start from it and explore outwards to find `?a` and `?b` that work.

For any two relations, how do we find the facts where certain key columns match?
By sorting the relations by those key columns, and merging them!
(That's why we sorted these things, in addition to helping to deduplicate).

### Generalizing

We need to figure out how to do this in general, rather than just for triangles based on edges.

Our plan of attack will be to go through the body right-to-left, turning the last pair of relations into a join, and placing their result at the end of the list.
Once we have only two relations remaining, we'll do a special-cased join that puts the results in the form the head requires.
If we only have one relation to start with, we skip all this and just transform that relation into the shape the heads require.

The concrete parts we'll need to bring together are:
1.  For each atom in the body, a transformation of the facts so that they start with the columns we'll need to equate.
    Also, if the atom requires filtering (e.g. literal columns or repeated variable names), we would do this here.
2.  For each join we will perform, the arity (number of columns) of the key, to know which prefix of each fact to equate.
    Also for each join other than the last, the order of columns it should produce, so that the relation it deposits is ready to be joined.
    We won't have to worry about applying filtering here.
3.  For each head atom, the "layout" facts need to be in to be inserted into the relation.
    Each coordinate may be either a bound variable, or a literal from the atom itself.

Here's the shape of the join plan we'll use.
I should stress, this is .. a very naive plan, and there are cooler ones I hope to get to in the future.
This is a "right linear" join plan, and they don't get much simpler (maybe left linear is).
Simpler doesn't mean easy in this case, though.
```rust
/// A description of a linear join plan for a Datalog rule.
///
/// The plan goes right to left, joining pairs of atoms until there are at most two left.
/// With one or two body atoms, we either:
/// 1. map the single body atom onto the head atoms, or
/// 2. join the two body atoms and map the result onto the head atoms.
#[derive(Debug)]
pub struct JoinPlan {
    /// For each body atom, the pre-work to put it in the right shape.
    bodys: Vec<Plan>,
    /// For each join other than the leftmost, the key arity and projection to apply to its output.
    ///
    /// This should land the results keyed appropriately, and pass along all demanded bindings.
    joins: Vec<(usize, Vec<usize>)>,
    /// For each head atom, the action to perform in the final join to correctly shape the result.
    ///
    /// An `Ok(index)` indicates a coordinate to copy, and an `Err(lit)` indicates a literal to copy.
    /// If `joins` is empty there is no final join, and each are applied to the rows of the body atom.
    heads: Vec<Vec<Result<usize, String>>>,

    /// The join arity for the final head-producing join, if such a join exists.
    arity: Option<usize>,
}
```

The two hard steps are now 1. create this join plan from a `Rule`, and 2. implement this join plan using joins.

### Create a `JoinPlan` from a `Rule`

This is unfortunately just a wall of text.

We can start off on the right foot by noticing for each variable the leftmost atom and rightmost atom the variable appears in.
The leftmost occurrence is the last moment (right-to-left) the bound variable needs to be retained; after this moment we can discard the values.
The rightmost occurrence tells us when a variable first enters play, which is important for other body atoms that mention the variable to know if they must plan to include it in their key columns.

With this information, each body atom can look at its own variables and the leftmost and rightmost numbers to partition its columns into 
1. dead columns that no one wants to hear about, 
2. key columns that will need to be equated with the incoming relation,
3. value columns that must be presented and carried on, but are not equated.

Similarly, for each join we can partition the columns of the two participating relations similarly:
1. dead columns that no one wants to hear about (perhaps they were just joined),
2. key columns that will need to be equated with the incoming relation,
3. value columns that must be presented and carried on, but are not equated.

This thinking gives us the `bodys` and `joins` members, as well as the awkward `arity` member (a special case because of the leftmost join).

The final member, `heads`, is just about reading through the head atoms, recording either a literal or the location of the named variable.
There is also some chaos in dealing with the possibility that there is no join at all, and what we need is to map the contents of the single body atom.

The code that does all of this, likely incorrectly, is in `joins.rs` nicely packaged up in a `From<&str>` implementation for `JoinPlan`:
```rust
impl<'a> From<&'a Rule> for JoinPlan {
    fn from(rule: &'a Rule) -> Self {
        ...
    }
}
```
And that's all I'm going to show you of it here!

### Implement a `JoinPlan` using Joins

We are getting down to business.
Up until this point everything was either pretty abstract, or pretty concrete but just involving some text or rules or things.
We are now heading in to the performance sensitive part of things.

Join plan implementation starts through a method that looks like this:
```rust
/// Implements a provided rule by way of a provided plan.
///
/// Additionally, a rule-unique identifier allows the logic to create new relation names.
/// The `stable` argument indicates whether we need to perform a full evaluation or only the changes.
pub fn implement_plan(
    rule: &Rule, 
    plan: &JoinPlan, 
    pos: usize, 
    stable: bool, 
    facts: &mut Facts) 
{
    ...
}
```

There are a few secrets being revealed here.
At least, you may have some questions about the `stable` argument and I'll now have to explain.

When `stable` is set we are going to join together the relations contents including `stable + recent`.
Both of those are valid distinct facts, and we'll want pretty much everything.
This is the mode we use when we introduce a new rule.
Even if facts are stable, the rule is new and we have to do everything just to get started.

When `stable` is unset, we want the join results that don't derive exclusively from `stable` facts.
We have already seen anything that can be redived from stable facts, so don't waste our time!
Not yet clear how we are going to accomplish this though.

Joins, and binary joins in particular, which is what we are implementing, are "bi-linear", meaning in this case that they distribute over addition.
If we have fact sets `A` and `B`, each of which have had some facts `a` and `b` "recently" added, we can write their join as:
```
  (A + a) ⋈ (B + b)
=
   A ⋈ B + A ⋈ b + a ⋈ B + a ⋈ b
```
The first term in the second line is `A ⋈ B` which is exactly the join results from stable facts.
Yuck!
We don't want that!

What we really want is the join results including recent facts, minus the join results on stable facts.
We can rewrite the above into that:
```
  (A + a) ⋈ (B + b) - A ⋈ B
=
   A ⋈ b + a ⋈ B + a ⋈ b
```
This is great news!
To get what we want, only the new derivations, we can perform three joins that each always involve a small letter, a "recent" addition.
With that `stable` flag in mind, we'll implement our join with the ability to toggle between "full" and "incremental" join.

#### Apply the body plans

The first step in our join implementation is to apply the plans in `JoinPlan::bodys`.
These are the things that tell us for each input how to re-lay out the columns, and how to filter them if necessary.
```rust
// New name for handy use.
let name = format!(".temp-{:?}", pos);
// Stash names of relations to draw facts out of.
let mut names = Vec::with_capacity(rule.body.len());

// For each body plan apply the plan to a newly named relation.
for (index, (plan, atom)) in plan.bodys.iter().zip(rule.body.iter()).enumerate() {

    if plan.identity() {
        facts.entry(atom.name.clone()).or_default();
        names.push(atom.name.clone());
    }
    else {
        let new_name = format!("{}-{:?}-in", name, index);
        let mut builder = FactBuilder::default();
        let found = facts.entry(new_name.clone()).or_default();
        if stable {
            for layer in found.stable.layers.iter() {
                plan.apply(layer, &mut builder);
            }
        }
        plan.apply(&found.recent, &mut builder);

        facts.get_mut(&new_name).unwrap().add_set(builder);
        names.push(new_name);
    }
}
```
There is a cute moment here where if the plan is a no-op, and we try to make them a no-op if at all possible, we can just skip the step.
In that case, we'll just use the source fact list as reported!

Otherwise, we use that trusty `FactBuilder` we described earlier, and apply the plan to `recent` and if asked to each layer of `stable`.
The builder is handed off to the `FactSet`, which knows how to integrate the `FactLSM` into its `to_add` member.

#### Apply the joins: right to left

We now have prepared body inputs, each in the sorted order they need to be in to perform a merge join.
The next step is to apply these merge joins until there are at most two relations left.
Each time we apply a join we need to indicate an `arity`, and what to do with the matching results.
We will just put them into a `FactBuilder`, and stash that associated with a new temporary name.

```rust
let mut r_name = names.pop().unwrap();
if names.len() > 1 {
    // Burn down the joins until there are at most two collections left.
    for (index, (arity, projection)) in plan.joins.iter().enumerate().rev() {

        let l_name = names.pop().unwrap();
        let mut builder = FactBuilder::default();
        join_with(facts.get(&l_name).unwrap(), facts.get(&r_name).unwrap(), stable, *arity, |v1, v2| {
            builder.push(projection.iter().copied().map(|i| {
                if i < v1.len() { v1.get(i) } else { v2.get(i - v1.len()) }
            }));
        });

        // Stash the results under the name we'll use next.
        r_name = format!("{}-{:?}-mid", name, index + 1);
        facts.entry(r_name.clone())
             .or_default()
             .add_set(builder);
    }
}
```
Clearly the join magic is in another castle.
The castle named `join_with`.
We'll finally get there, but it's basically just ripped off of `datafrog`, so don't get too excited.
Before then, one last step in this method.

#### Apply the last join: populate the heads

This last part is just horrible, at least in proportion to how little is going on.

If there are two body atoms to join, we are going to do the same thing as above but we'll use different instructions to lay out the facts.
We'll also use multiple builders, because there could be multiple heads.
If there is only one body atom we'll do the same thing but without a join, just by reading the relation.

Somehow I couldn't write this code very cleanly, so it's the same thing repeated three times.
You'll have to check out the repo to read it, I'm sorry.

#### Getting down to joining

Let's knock out that `join_with` function and be done!

```rust
/// Joins `body1` and `body2` using the first `arity` columns.
///
/// Matching elements are subjected to `action`.
/// When `stable` is set, we join the stable plus recent elements of each input;
/// when it is unset we exclude pairs of terms that are both stable.
pub fn join_with(
    body1: &FactSet,
    body2: &FactSet,
    stable: bool,
    arity: usize,
    mut action: impl FnMut(<Fact as Columnar>::Ref<'_>, <Fact as Columnar>::Ref<'_>),
)
{
    // Compare elements by their first `arity` columns only.
    // This is a moment where compile-time information about types would help us; perhaps column introspection can recover.
    let order = |x: <Fact as Columnar>::Ref<'_>, y: <Fact as Columnar>::Ref<'_>| { (0 .. arity).map(|i| x.get(i)).cmp((0 .. arity).map(|i| y.get(i))) };

    if stable {
        for layer1 in body1.stable.layers.iter() {
            for layer2 in body2.stable.layers.iter() {
                join::<Fact>(layer1, layer2, order, &mut action);
            }
        }
    }

    for stable2 in body2.stable.layers.iter() {
        join::<Fact>(&body1.recent, stable2, order, &mut action);
    }

    for stable1 in body1.stable.layers.iter() {
        join::<Fact>(stable1, &body2.recent, order, &mut action);
    }

    join::<Fact>(&body1.recent, &body2.recent, order, &mut action);
}
```

Ok this was apparently all a lie and now there is some other `join` function?

So all we are doing here is handling that `stable` bit.
We are performing the three joins that involve recent facts, and if asked the one join that involves only stable facts.
We don't pass `stable` along to the `join` function, which is join really going to join things I swear.
```rust
/// Match keys in `input1` and `input2` and act on matches.
fn join<'a, T: Columnar<Ref<'a> : Ord+std::fmt::Debug>> (
    input1: &'a T::Container,
    input2: &'a T::Container,
    mut order: impl FnMut(T::Ref<'a>, T::Ref<'a>) -> std::cmp::Ordering,
    mut action: impl FnMut(T::Ref<'a>, T::Ref<'a>),
) {
    use std::cmp::Ordering;

    let input1 = input1.borrow();
    let input2 = input2.borrow();

    let mut index1 = 0;
    let mut index2 = 0;

    // continue until either input is exhausted.
    while index1 < input1.len() && index2 < input2.len() {
        // compare the keys at this location.
        let pos1 = input1.get(index1);
        let pos2 = input2.get(index2);
        match order(pos1, pos2) {
            Ordering::Less => {
                // advance `index1` while strictly less than `pos2`.
                gallop::<T>(input1, &mut index1, |x| order(x, pos2) == Ordering::Less);
            },
            Ordering::Equal => {
                // Find *all* matches and increment indexes.
                let count1 = (index1..input1.len()).take_while(|i| order(input1.get(*i), pos1) == Ordering::Equal).count();
                let count2 = (index2..input2.len()).take_while(|i| order(input2.get(*i), pos2) == Ordering::Equal).count();

                for i1 in 0 .. count1 {
                    let row1 = input1.get(index1 + i1);
                    for i2 in 0 .. count2 {
                        let row2 = input2.get(index2 + i2);
                        action(row1, row2);
                    }
                }

                index1 += count1;
                index2 += count2;
            },
            std::cmp::Ordering::Greater => {
                // advance `index2` while strictly less than `pos1`.
                gallop::<T>(input2, &mut index2, |x| order(x, pos1) == Ordering::Less);
            },
        }
    }
}
```
Straight out of datafrog, which took it straight out of somewhere else.
I didn't invent joins.

This is a merge join, in that it is moving sequentially through sorted inputs and looking for keys that match.
When it finds matching keys, it calls the `action` function, which as we know just packs facts into a `FactBuilder`.
When it doesn't find matching keys, it calls the `gallop` function, which I definitely stole from the [EmptyHeaded](https://ppl.stanford.edu/papers/emptyheaded.pdf) folks, and which is a bit like a binary search to move forward quickly to the next possible match.

You want to see `gallop`, you say?
```rust
/// Increments `index` until just after the last element of `input` to satisfy `cmp`.
///
/// The method assumes that `cmp` is monotonic, never becoming true once it is false.
/// If an `upper` is supplied, it acts as a constraint on the interval of `input` explored.
#[inline(always)]
pub(crate) fn gallop<'a, T: Columnar>(input: <T::Container as Container<T>>::Borrowed<'a>, index: &mut usize, mut cmp: impl FnMut(<T as Columnar>::Ref<'a>) -> bool) {
    let upper = input.len();
    // if empty input, or already >= element, return
    if *index < upper && cmp(input.get(*index)) {
        let mut step = 1;
        while *index + step < upper && cmp(input.get(*index + step)) {
            *index += step;
            step <<= 1;
        }

        step >>= 1;
        while step > 0 {
            if *index + step < upper && cmp(input.get(*index + step)) {
                *index += step;
            }
            step >>= 1;
        }

        *index += 1;
    }
}
```
That is literally the very end of `join.rs`.
We are now done with the code tour.

### Experimentation

This is coming up next! 
Rest assured I've done some of this, but doing a decent job requires a bit of care and checking of results to make sure they are accurate.
Going fast is easy if you don't check the results!

I have some data from the [Graspan](http://web.cs.ucla.edu/~harryxu/papers/wang-asplos17.pdf) project, acquired at the time but [still available on Google Drive](https://drive.google.com/drive/folders/1DRj-cfISV9v34vU7DH1PKEu13PaMIf71).
If you want to follow along, you can load the `httpd` dataflow analysis in as facts, using this fragment at the top of the `main.rs` file.
```rust
for arg in std::env::args().skip(1) {

    // Read input data from a handy file.
    use std::fs::File;
    use std::io::{BufRead, BufReader};

    let mut dict: BTreeMap<String, facts::FactBuilder> = BTreeMap::default();
    let filename = arg;
    let file = BufReader::new(File::open(filename).unwrap());
    for readline in file.lines() {
        let line = readline.expect("read error");
        if !line.is_empty() && !line.starts_with('#') {
            let mut elts = line.split_whitespace().rev();
            if let Some(name) = elts.next() {
                dict.entry(name.to_string())
                    .or_default()
                    .push(elts.rev().map(|x| x.as_bytes()));
            }
        }
    }
    for (name, facts) in dict { 
        state.facts
             .entry(name)
             .or_default()
             .add_set(facts); 
    }
}
state.update();
```

With that in hand, you can start things up (loading the data takes a moment), and type things like:

```
> .list
    e:	9905624
    n:	138331
30.708µs

> m(?b, ?a) :- n(?a, ?b) .
52.027834ms

> .list
    e:	9905624
    m:	138331
    n:	138331
45.916µs

> m(?c, ?a) :- m(?b, ?a), e(?b, ?c) .
8.360353834s

> .list
    e:	9905624
    m:	9393283
    n:	138331
49.042µs

> 
```

This maths out to ~900ns per derived fact, and at 8.3s it's about 80x faster than Graspan.
It is about 4x *slower* than datafrog, which seems to be largely because the variable shape of our facts makes sorting and deduplication harder.

More in a bit!