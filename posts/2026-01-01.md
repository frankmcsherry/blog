# The Demand Transform

The [demand transform](https://www3.cs.stonybrook.edu/~tuncay/papers/TL-PPDP-10.pdf) is a way to rewrite Datalog programs that tricks their "bottom-up" evaluation into "top-down" derivation.
It's related to "magic sets", which I cannot find a good link to.
In many ways, it's like [the Yannakakis algorithm](https://en.wikipedia.org/wiki/Yannakakis_algorithm) for recursive computation, performing semijoins to restrict data production to facts that will be of use to someone.
I've used ad-hoc versions for a while, but I wanted to take some time to go through the details more carefully to understand them better.

In my opinion the demand transform is **essential** for declarative recursive programming, but it's often ignored.
It has some caveats, and can (asymptotically) harm rather than improve performance, but it can also improve performance by ridiculous amounts.
By the end of the post I hope to have explained the transform to you, motivated its application even in batch data processing, and optimize a Datalog formulation of the Rust borrow checker almost to zero.
We'll also see an example where it makes things much worse, and try to explain why (it bakes in atom-at-a-time joins, rather than worst-case optimal joins).

First, let's discuss **imperative** versus **declarative** programming, to motivate the transformation.

## Imperative / Declarative

Imperative programs tell the computer what to do: the computer follows the (largely) unambiguously stated instructions, and an unsurprising sequence of operations leads to the output.
Imperative programs are relatively easier to understand as mechanisms, what they are doing at any moment and why, but they can be harder to build layers of abstraction upon, as they only detail the steps to take and leave it to the user to understand the "meaning" of the results.
Higher-level thinking is usually someone else's job, intentionally.

Declarative programs tell the computer what needs to be true of the output, but leave it up to the computer to actually find the answer, which has the potential to result in a surprising path to arrive at the output (which can be delightful or upsetting).
Declarative programs can be easier to build abstractions around, and evolve in certain ways, but their behavior is usually much harder to understand; the computer does what it thinks is best, for reasons that may not be evident in your instructions.
Lower-level control is something that you need to understand carefully, or bake into the logic.

In reality, pretty much all programming languages are a bit of both.
Compilers take your "imperative" program through a language specification and translate it to a likely different program potentially with a built-in runtime managing things like garbage collection that you didn't detail in your program.
Intepreters take your "declarative" program and evaluate what you've entered in the order you entered it, because at some point the work actually has to get done.
The best tools lie on a spectrum of imperative vs declarative, helping you when they can and leaving you in charge when they cannot.

For me, one of the appealing parts of declarative programs is building layers of abstractions.
I want to define broadly useful concepts and then specialize them as needed, rather than repeatedly write apparently unrelated specialized versions.
At the same time, I really want to be able to **automatically specialize** the program reliably, getting an implementation similar too the one I might hand-write.

Let's see an example of abstraction and specialization, introducing the demand transform.

## Ancestry

A classic Datalog program describes "ancestry", because "transitive closure" is too dorky.
```prolog
Ancestor(X, Y) :- Parent(X, Y).
Ancestor(X, Z) :- Parent(X, Y), Ancestor(Y, Z).
```
These two "rules" are read right-to-left and say that
1. If `X` is a parent of `Y`, then `X` is an ancestor of `Y`.
2. If `X` is a parent of `Y` and `Y` is an ancestor of `Z`, then `X` is an ancestor of `Z`.

There are a lot of consequences to these rules.
If you have many parent relationships, for example for the 8.2B people currently alive, as well as lots of historical data, you could imagine the totality of `Ancestor` growing substantially as we go back in time.
Only some 100B people have ever lived, so your family tree eventually folds in on itself, but even your ancestors by themselves are really quite a lot of data, to say nothing of the ~8B other people like you.

Ancestry is an important *concept*, but often what you want is to find a specific instance of it.
```prolog
MyAncestor(X) :- Ancestor(X, 'Frank').
```
This is easy enough to read out of `Ancestor` using a filter on the second term that identifies me.
Super easy: step one, compute probably trillions of facts, step two fish out the eight-billionth of them that relate to me.

Here's a different program, a *specialized* program, that computes `MyAncestor` directly:
```prolog
MyAncestor(X) :- Parent(X, 'Frank').
MyAncestor(X) :- Parent(X, Y), MyAncestor(Y).
```
These two rules identify my parents as my ancestors, and any parents of my ancestors.
It's very similar to the first rule for all ancestors, but specialized to me and my query.
At the same time, it doesn't take advantage of any of the foundation laid by the `Ancestor` rules; it just re-invents them.
Perhaps fine for a simple concept like ancestry, and a simple constraint on my search, but less fine for more complex rules like the Rust borrow checker.

## The Rust Borrow Checker

[Polonius](https://github.com/rust-lang/polonius) is a next-generation Rust borrow checker, which early days (seven years ago) was framed in Datalog.
There were great reasons then, for example that it was easier to explain than the corresponding imperative code, and probably easier to debug and evolve as well, at least as long as we mean the "logic" of rules rather than the behavior.
The behavior left a fair bit to be desired, to my understanding.
Performance was (and still is?) one of the primary blockers of moving forward with this formulation, and I imagine a "more imperative" version is what's actually under way.

The rules are (were) relatively concise, if not simple, and start from seven external tables (EDBs in Datalog-ese).
```
 facts input name
 ----- -----
  1886 borrow_region
 51896 cfg_edge
 76712 invalidates
   980 killed
534231 outlives
782146 region_live_at
     6 universal
```
The rules develop two recursive quantites, `subset` and `requires`, the latter depending on the former, as well as some other intro and outro rules.
We would write them in `datatoad` like so:
```prolog
region_live_at(R, P) :- cfg_edge(P,_0), universal(R).
region_live_at(R, Q) :- cfg_edge(_0,Q), universal(R).

subset(R1, R2, P) :- outlives(R1, R2, P), :noteq(R1,R2).
subset(R1, R3, P) :- subset(R1, R2, P), subset(R2, R3, P), :noteq(R1,R3).
subset(R1, R2, Q) :- subset(R1, R2, P), cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2).

requires(R, B, P) :- borrow_region(R, B, P).
requires(R2, B, P) :- requires(R1, B, P), subset(R1, R2, P).
requires(R, B, Q) :- requires(R, B, P), !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q).

borrow_live_at(B, P) :- requires(R, B, P), region_live_at(R, P).

errors(B, P) :- invalidates(B, P), borrow_live_at(B, P).

```
The output at the end that we are interested in is `errors`, though .. heart of the problem here .. that key information isn't evident from the Datalog itself.

This really is the problem, in that it's only when we say "is `errors` non-empty?" that we have the ability to think more carefully about the work.
Until that point, we have rules for producing all facts that might be of interest, including all of `errors` but also others, and it is very natural for a Datalog engine to leap into action and do all of that work.
This is exactly what Polonius's `naive.rs` does, some Rust code that uses [`datafrog`](https://crates.io/crates/datafrog) to compute answers.

Unfortunately `datafrog` itself is **imperative**.
It is an engine that helps you run Datalog rules, in the order you ask it to, and .. that's the extent of the help it provides.
It does not take a beat and say "oh, only `errors` you say? Let's do this better then".
This is left to the programmer to figure out.

For a reference input derived from `clap-rs`, it takes about 18s to run these rules in `datatoad` on my laptop, generating all values of all relations.
This is slower (:sob:) than `naive.rs` in the Polonius project, which is closer to 16s.
Both of these times are not acceptable for borrow checking in Rust.

## Declarative / Imperative

At this point it should be clear that there is a potential expectation mismatch between declarative and imperative programs.
We wrote a bunch of things declaratively, in Datalog, but got stuck with imperative performance.
We could further specialize the things we wrote, but then we lose some of the value of the original declarative program.

In SQL this all happens largely behind the scenes and for free.
If I have some complicated business logic, I write it as a view and bind that view to some name.
When I want to use that logic in the future, I reference it by name and I can add additional constrants and embellishments.
The database leaps into action when I query the result, and optimizes everything together likely pushing my query-specific constraints as deep into the view as possible.
It does the hard work of preserving the declarative experience (at least for queries; don't get me started on transactions).

It feels very bad that the same does not happen in most Datalog engines.

I want to define `Ancestor` from `Parent`, and then show up with a query `Ancestor(X, 'Frank')`, and have the system behave as if I had defined `MyAncestor`, specialized to the query literal `'Frank'`.
If I show up instead with 1,000 people whose ancestors I want to find, I'd like it to do the specialization for me from these base facts.
If I want to find descendants of some few individuals instead of ancestors, the system should help me do that too.

This is not as easy as in SQL, which is generally non-recursive, and when recursive is generally delighted to just give up and try and throw recursion itself under the bus.
However, there is a way to approach the problem for recursion, using recursion itself as a tool.
This is where we introduce [the **demand transform**]((https://www3.cs.stonybrook.edu/~tuncay/papers/TL-PPDP-10.pdf)).

## The Demand Transform

The demand transform rewrites a Datalog program around the "seed crystal" that is your query, and follows rules backwards, from heads to bodies, searching for derivations that might produce something relevant for the query.
The simplest example is with the `Ancestor` relation, though we'll talk through the general construction as well, as this will be needed for the Rust case.

We'll develop the motivation informally, and generally wave our hands until we get to the more formal telling of the transform.
If you get stressed by

### Ancestor (Easy)

If we start with `Ancestor` rules like so
```prolog
Ancestor(X, Y) :- Parent(X, Y).
Ancestor(X, Z) :- Parent(X, Y), Ancestor(Y, Z).
```
and "demand" `Ancestor(X, 'Frank')`, perhaps by naming a new rule with a head that we are going to mark somehow as important.
```prolog
.output
MyAncestor(X) :- Ancestor(X, 'Frank').
```

Let's take a moment and see where these `MyAncestor` facts could possibly come from.

The first `Ancestor` rule will produce relevant facts only when `Y` is `'Frank'`, and we could rewrite that rule as
```prolog
Ancestor(X, 'Frank') :- Parent(X, 'Frank').
```
The second `Ancestor` rule will produce relevant facts only when `Z` is `'Frank'`, and we could rewrite that rule as
```prolog
Ancestor(X, 'Frank') :- Parent(X, Y), Ancestor(Y, 'Frank').
```
These are essentially the rules for `MyAncestor` up towards the top of the post, except we remove the literal from `Ancestor`, as it is redundant.
It turns out this is "all we need to do".

This feels good, but it turns out we got very lucky.
We missed a very important, and very hard, caveat about how to determine what we "need" to do.
Let's try `Ancestor` from a different direction, and see how things could go much more wrong, motivating a more careful treatment of the transform.

### Ancestor (Hard)

Same rules as before

```prolog
Ancestor(X, Y) :- Parent(X, Y).
Ancestor(X, Z) :- Parent(X, Y), Ancestor(Y, Z).
```
but this time we'll demand `Ancestor('Confucius', X)` to look for smart descendants.
```prolog
SmartDescendant(X) :- Ancestor('Confucius', X).
```

As before the first rule will produce relevant facts only when `X` is `'Confucius'`, and we could rewrite that rule as
```prolog
Ancestor('Confucius', Y) :- Parent('Confucius', Y).
```

It might be tempting to repeat our prior success by applying the same caveat as from the easy case.
Just restrict the second rule to cases where `X` is `'Confucius'`.
This is incorrect, and does not work.

The problem is that the second rule activates based on a match in `Y`, and for that to happen they need to be present in the first argument of `Ancestor`.
For example, the first descendants of `'Confucius'` would be our first step, but .. they aren't yet present in the first position of `Ancestor`.
We need to expand the set of interesting parents from just `'Confucius'` to their children, and then onward.

We aren't going to be able to directly produce only `Ancestor('Confucius', Y)` facts, but we may be able to narrow the set of literals in that first position.

Informally, let's introduce a new variable `Ancestor_X(X)`, which is meant to indicate the `X` values we'll need to see for our query.
Initially there is only one rule that populates this variable:
```prolog
Ancestor_X('Confucius') :- .
```
Our query started with that value, and we know we're going to need it.

But there are other values we might need.
Each child of Confucius, child of theirs, and so on, spelling out all the descendants.
The rule, and we'll work to automatically derive this in a bit, is
```prolog
Ancestor_X(Y) :- Ancestor_X(X), Parent(X, Y).
```

This rule already smacks of the `MyAncestor` rules written the opposite way around, following the `Parent` relation from a different direction.
That's great, because it is what we are hoping for, but we aren't done just yet.

The `Ancestor_X` relation is used to "gate" the derivations of `Ancestor`.
Before we produce a fact we need to make sure it is demanded, and a way to do this is to add `Ancestor_X` as a clause to each `Ancestor` rule:
```prolog
Ancestor(X, Y) :- Ancestor_X(X), Parent(X, Y).
Ancestor(X, Z) :- Ancestor_X(X), Parent(X, Y), Ancestor(Y, Z).
```
If we combine this with our two rules to derive `Ancestor_X` in the first place, we end up with a derivation of the descendants of Confucius.
It's likely that with some smarter thinking, we could further optimize `Ancestor` away entirely, and just have `Ancestor_X` as `SmartDescendant`.

### Automating the Demand Transform

There was a lot of hand-waving above, but behind the scenes there *is* an automatic way to apply the demand transform.
I have to admit, I'm not in love with it, in that it bakes in an evaluation order that corresponds to left-to-right atom-by-atom evaluation, which I am trying to move away from, but let's start here and discuss improvements at a later date.

I am largely copying out of [the paper introducing the demand transform](https://www3.cs.stonybrook.edu/~tuncay/papers/TL-PPDP-10.pdf), and if you end up confused you can refer to it.
I found myself lightly confused by reading the paper, and I'm hoping that typing things out (and ultimately implementing them) will resolve that confusion.

The first concept the paper introduces is the **demand pattern**.
A demand pattern is a pair of relation name and a Boolean sequence as long as the relation has columns.
The sequence denotes either "bound" or "free" for each position in the relation, and we'll follow the paper and use `'b'` and `'f'` for "bound" and "free" respectively.
A demand pattern is a rendezvous point for various moments in our logic where the query expresses the opinion that:

> I will only need to see values from this relation for which these bound columns have values that I will provide.

A relation can be named in multiple demand patterns; that is fine.

To produce demand patterns from a query, there are two rules, which we apply repeatedly:
1. Your query itself produces a demand pattern, for the named relation and with a sequence with "bound" where you use literals and "free" where you use variables.
2. For any rule whose head has a demand pattern `(Foo, s)`, add a demand pattern for each `bi` in the rule body, where its sequence has "bound" in a position if any of;
    1.  that position has a literal constraint in the body atom, or
    2.  that position has a variable that appears in a body term to the left, or
    3.  that position has a variable that appears in the head, and is marked "bound" in `s`.

This use of "left" will come back to annoy me.
It encodes an evaluation order, left to right, and is a thing I'm hoping to unpick and potentially relax or alter.
(Future Frank reports: this absolutely bites us in some queries with cyclic joins, making things much worse than without the transform).

The next step is to introduce an auxiliary variable for each demand pattern.
For a named relation `Foo` and Boolean string `s` with `k` bound positions, we'll declare
```prolog
Foo_s(v1, v2, .., vk)
```
We don't know how to use it just yet, but this will hold the values of the corresponding bound values of `Foo` that are of interest, to folks who have opinions on this exact set of variables.

The variable is used in a new copy of each rule that derives `Foo` from the original program.
Let's imagine they are originally
```prolog
Foo(X, Y, Z, ..) :- G1(..), G2(..), ..
Foo(X, Y, Z, ..) :- H1(..), H2(..), ..
Foo(X, Y, Z, ..) :- ..
```
we will create new rules
```prolog
Foo(X, Y, Z, ..) :- Foo_s(v1, v2, .., vk), G1(..), G2(..), ..
Foo(X, Y, Z, ..) :- Foo_s(v1, v2, .., vk), H1(..), H2(..), ..
Foo(X, Y, Z, ..) :- Foo_s(v1, v2, .., vk), ..
```
where the `vi` variable terms are matched up to their corresponding variable in `X, Y, Z, ..` based on `s`.
If for example `s` is `[free, bound, bound, free, bound]` then `v1` would be `Y` as the first "bound" entry is found in the second position of `s`, and `Y` is the second variable term of the head.
Correspondingly `v2` would be `Z` as the second "bound" term is in the third position.
There is one other bound term (`vk` I guess) and it would be the fifth variable term in the head (elipsis in our example) .

These rules say:

> When you go and derive `Foo`, at least in the context of folks whose demands are encoded by `s`, take a moment to verify that we have an interest in it.

We'll make copies of these rules for each demand pattern pair of `Foo` and `s`, which means we'll have similar rules multiple times if `Foo` occurs in multiple demand patterns.
This means we may produce values for `Foo` in one rule that are of no interest to folks in another context.

I think this is one of the differences between the demand transform and the "magic set" transform, from the 1980s when computer science articles were written on tanned hides.
The magic set transform produces distinct `Foo` for each `s`, which the demand transform folks find bad because they believe Datalog engines will have constant-time look-ups for all keys and so the differentiated collections are redundant.
I'm not sure I believe this argument yet.
I do like the idea of not storing many copies of potentially large datasets, in the case that the transform doesn't reduce the data.

Finally, we need to introduce the rules that populate the `Foo_s` relations.
These will correspond to the left-to-right evaluation order in the definition of demand patterns, so bear that in mind if you want to change anything.
For each of the new rules we added, that each start with a `Foo_s`, we're going to add new rules for each of the other atoms in the body.

Let's take this rule for example
```prolog
Foo(X, Y, Z, ..) :- Foo_s(..), G1(..), G2(..), ..
```
For each `Gi`, we'll add a rule that starts from `Foo_s`, and moves left to right until just before `Gi`, at `Gi-1` let's call it.
A demand pattern `(Gi, si)` was induced for this body atom, and we'll create the rule
```prolog
Gi_si(..) :- Foo_s(..), G1(..), .. Gi-1(..).
```
There are a lot of `..` in there, but for all of the body atoms it is just copying the variables as they occur in the rule above, and for `Gi_si` the `..` are the bound variables referenced in `si`.

Informally, this rule is saying

> Hearing from `Foo_s` about its needs, the `Gj` before me establish the needed values of my bound variables, which I record in that head atom there.

And that's the whole transform.
It should produce the same results.

## Experimentation

We are going to try this by hand with the Polonius query.
I'd very much like to get this automated, but I also want to try it manually a few times to make sure I understand it.
Along the way, we'll see how it does for Polonius.

Let's restate the Polonius rules, with a few modifications made to them:
```prolog
region_live_at(R, P) :- cfg_edge(P,_), universal(R).
region_live_at(R, Q) :- cfg_edge(_,Q), universal(R).

subset(R1, R2, P) :- outlives(R1, R2, P), :noteq(R1,R2).
subset(R1, R3, P) :- subset(R2, R3, P), subset(R1, R2, P), :noteq(R1,R3).
subset(R1, R2, Q) :- cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2), subset(R1, R2, P).

requires(R, B, P) :- borrow_region(R, B, P).
requires(R2, B, P) :- subset(R1, R2, P), requires(R1, B, P).
requires(R, B, Q) :- !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q), requires(R, B, P).

borrow_live_at(B, P) :- region_live_at(R, P), requires(R, B, P).

errors(B, P) :- invalidates(B, P), borrow_live_at(B, P).
```
I've put the recursive term at the end of the body of each rule.
This is meant to give other external collections a chance to impose more opinions first;
we'll see (or not) whether this was a good idea.
I made one other change; let's see if you notice.

I'm going to ignore doing the demand transform for external data.
The `invalidates` collection is fully loaded; we don't need to think about how little we should bring in from disk.
Similarly, the `region_live_at` collection has some rules that create additional facts, but let's just leave it as it is for now.
I'm realizing only now that the *a la carte* nature of the demand transform is appealing; you don't have to evert your whole program; you can choose to pre-materialize some of your logic, and defer some other part until later (shill moment: not unlike [Materialize](https://materialize.com)!).

### Determine Demand Patterns

We start by demanding `errors(B, P)`, with no literals.
We want the whole thing.
This starts us off with a demand pattern of `(errors, 'ff')` because both terms are free.
```
(errors, 'ff')
```

This creates demand patterns of `(invalidates, 'ff')` and `(borrow_live_at, 'bb')`.
The first because `invalidates` has no literals, no preceding body atoms, and none of the terms are bound in the demand pattern for `errors`.
The second because `borrow_live_at` has each of its variable terms occur in a preceding body literal, `invalidates`.
Informally, we are telling `borrow_live_at` "we'll have `invalidates` tell you the `(B, P)` pairs to worry about".
```
(errors, 'ff')
(invalidates, 'ff')
(borrow_live_at, 'bb')
```

The `(borrow_live_at, 'bb')` demand pattern introduces demand patterns for `region_live_at` and `requires`.
The first demand pattern is `(region_live_at, 'fb')` because its second term appears bound in the head.
The second demand pattern is `(requires, 'bbb)` as each term appears either in a preceding body atom (`R`, `P`) or appears bound in the head (`B`, `P`).
```
(errors, 'ff')
(invalidates, 'ff')
(borrow_live_at, 'bb')
(region_live_at, 'fb')
(requires, 'bbb')
```
I hope these are right!

We won't chase down the `region_live_at` demand patterns, but perhaps you can do that as homework to test your understanding!
Instead, let's move on to the `requires` demand patterns, and their consequences.
There are three rules with `requires` at the head:
```prolog
requires(R, B, P) :- borrow_region(R, B, P).
requires(R2, B, P) :- subset(R1, R2, P), requires(R1, B, P).
requires(R, B, Q) :- !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q), requires(R, B, P).
```
We have only `(requires, 'bbb')` at the moment, but this might grow.

1.  The first rule produces a demand pattern of `(borrow_region, 'bbb')` which we will ignore.
    It's another example of an input which we've already loaded.
2.  The second rule produces demand patterns of
    1.  `(subset, 'fbb')`, as `R2` and `P` are bound in the head.
    2.  `(requires, 'bbb')` as each of `R1`, `B`, and `P` occur bound in the head or in a prior body atom.
        This is not a new demand pattern for `requires`, which is a relief!
3.  The third rule is complicated by an antijoin, which I'm going to assume the worst about; let's imagine that it doesn't bind anything, and cannot act as justification for a later atom introducing a 'b'.
    We would also produce demand patterns of
    1. `(cfg_egde, 'fb')`,
    2. `(region_live_at, 'bb')`,
    3. `(requires, 'bbb)`.

Thankfully, it looks like we don't introduce any additional demand patterns for `requires`, but we could have (for example if the body atom orders had `requires` up front).

We'll ignore all of these patterns other than `(subset, 'fbb')`, as they correspond to inputs or near-inputs.
```
(errors, 'ff')
(invalidates, 'ff')
(borrow_live_at, 'bb')
(region_live_at, 'fb')
(requires, 'bbb')
(subset, 'fbb')
```

Recall the `subset` rules, which are
```
subset(R1, R2, P) :- outlives(R1, R2, P), :noteq(R1,R2).
subset(R1, R3, P) :- subset(R2, R3, P), subset(R1, R2, P), :noteq(R1,R3).
subset(R1, R2, Q) :- cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2), subset(R1, R2, P).
```
Let's build out the demand patterns here, ignoring patterns for inputs (everything except `subset` itself).
1.  The first rule doesn't produce a demand pattern for `subset`. Ignored!
2.  The second rule produces patterns
    1. `(subset, 'fbb')`
    2. `(subset, 'fbb')`
3.  The third rule produces the pattern `(subset, 'bbb')`.
    I'm going to weaken this to `(subset, 'fbb')` to avoid going around again, and we'll see if this is a problem.
    My sense is that it should always be legal to weaken a constraint, though we may derive facts we don't need.

That "other change" I made was to re-order the second rule.
If the terms were in the original order we'd have new `subset` demand patterns with `'ffb'` and `'bbb'` and I didn't want things to get too complicated.

I think these are all of the demand patterns to create.
Minus several that correspond to external inputs, or near enough, the list of demand patterns is:
```
(errors, 'ff')
(borrow_live_at, 'bb')
(requires, 'bbb')
(subset, 'fbb')
```
Our next step is to declare a few variables.

### Declare Demand Variables

In datatoad at least you don't have to declare things, you just use them.
But let's write out what shape we think each of these folks should have.
```prolog
errors_ff()
borrow_live_at_bb(B, P)
requires_bbb(R, B, P)
subset_fbb(R2, P)
```
Recall that each of these folks needs to have as many variables as there are `'b'` in its corresponding demand pattern string.
These are what is going to hold the concrete values we'll ship around.
In the paper their names are all prefixed with a `d_` which I didn't do because it breaks the Prolog code block styling, atoms must start with capital letters, but the Polonius logic also goes against that so .. argh.
Look for a `d_` prefix if you are following the paper.

### Use the Demand Variables

The next step is to create new rules from our starter rules that use the demand patterns.
We have one demand pattern for each named relation, so this shouldn't bloat much.
For the rules we ignored the demand transform for, we'll just leave them in place.

First up, `errors`:
```prolog
errors(B, P) :- errors_ff(), invalidates(B, P), borrow_live_at(B, P).
```
This is pretty boring.
The `errors_ff` has no fields, and is just a bit saying "yes please".

The `borrow_live_at_bb` rule is more interesting.
```prolog
borrow_live_at(B, P) :- borrow_live_at_bb(B, P), region_live_at(R, P), requires(R, B, P).
```

Actually none of these are interesting yet because I'm just copy/pasting an atom to the front of the body.
Let's speed it up.

```prolog
subset(R1, R2, P) :- subset_fbb(R2, P), outlives(R1, R2, P), :noteq(R1,R2).
subset(R1, R3, P) :- subset_fbb(R3, P), subset(R2, R3, P), subset(R1, R2, P), :noteq(R1,R3).
subset(R1, R2, Q) :- subset_fbb(R2, Q), cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2), subset(R1, R2, P).

requires(R, B, P)  :- requires_bbb(R, B, P), borrow_region(R, B, P).
requires(R2, B, P) :- requires_bbb(R2, B, P), subset(R1, R2, P), requires(R1, B, P).
requires(R, B, Q)  :- requires_bbb(R, B, Q), !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q), requires(R, B, P).
```
All done!

These collections are empty without the rules that populate them, so let's do that next.

### Populate the Demand Variables

This is the part that I worry will be fiddly.
(Future Frank: correct!)

For each of the rules we just added above, for each body atom with a demand variable, we'll want to follow the instructions.

First `borrow_live_at` appears in the `errors` rule, so we'll need a rule
```prolog
borrow_live_at_bb(B, P) :- errors_ff(), invalidates(B, P).
```
I'm going to scratch the `errors_ff()` because I don't know if datatoad handles 0-ary relations yet, and I don't want to find out here.
(Future Frank: it didn't, but I fixed it.)

The rule with `borrow_live_at` as its head references a few things, but only informs `requires_bbb`.
```prolog
requires_bbb(R, B, P) :- borrow_live_at_bb(B, P), region_live_at(R, P).
```

We have many rules now with `requires` in their head

The first `requires` rule would say something about `borrow_region` but we aren't doing that.
The second `requires` rule gives rise to two demand rules
```prolog
subset_fbb(R2, P) :- requires_bbb(R2, B, P).
requires_bbb(R1, B, P) :- requires_bbb(R2, B, P), subset(R1, R2, P).
```
The third `requires` rule gives us
```prolog
requires_bbb(R, B, P) :- requires_bbb(R, B, Q), !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q).
```

The several rules that have `subset` in their head give us several derivations.

The first `subset` rule would inform `outlives` but we aren't transforming it.

The second rule gives us two `subset_fbb` producing rules due to the two occurrences of `subset`.
```prolog
subset_fbb(R3, P) :- subset_fbb(R3, P).
subset_fbb(R2, P) :- subset_fbb(R3, P), subset(R2, R3, P).
```
but the first of these is clearly silly so we remove it.

The third rule gives us
```prolog
subset_fbb(R2, P) :- subset_fbb(R2, Q), cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2).
```

That's all of the rules we need to produce to populate the demand variables!

### All Together Now

Let's bring these rules into one program.
We'll keep various rules for head atoms we are not "demand transforming", and replace the rules whose heads are demand transformed.

```prolog
.note unchanged rules
region_live_at(R, P) :- cfg_edge(P,_0), universal(R).
region_live_at(R, Q) :- cfg_edge(_0,Q), universal(R).

.note demand transformed derivations
subset(R1, R2, P) :- subset_fbb(R2, P), outlives(R1, R2, P), :noteq(R1,R2).
subset(R1, R3, P) :- subset_fbb(R3, P), subset(R2, R3, P), subset(R1, R2, P), :noteq(R1,R3).
subset(R1, R2, Q) :- subset_fbb(R2, Q), cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2), subset(R1, R2, P).
requires(R, B, P)  :- requires_bbb(R, B, P), borrow_region(R, B, P).
requires(R2, B, P) :- requires_bbb(R2, B, P), subset(R1, R2, P), requires(R1, B, P).
requires(R, B, Q)  :- requires_bbb(R, B, Q), !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q), requires(R, B, P).
borrow_live_at(B, P) :- borrow_live_at_bb(B, P), region_live_at(R, P), requires(R, B, P).
errors(B, P) :- invalidates(B, P), borrow_live_at(B, P).

.note demand population rules
borrow_live_at_bb(B, P) :- invalidates(B, P).
requires_bbb(R, B, P) :- borrow_live_at_bb(B, P), region_live_at(R, P).
requires_bbb(R, B, P) :- requires_bbb(R, B, Q), !killed(B, P), cfg_edge(P, Q), region_live_at(R, Q).
requires_bbb(R1, B, P) :- requires_bbb(R2, B, P), subset(R1, R2, P).
subset_fbb(R2, P) :- requires_bbb(R2, B, P).
subset_fbb(R2, P) :- subset_fbb(R2, Q), cfg_edge(P, Q), region_live_at(R1, Q), region_live_at(R2, Q), :noteq(R1,R2).
subset_fbb(R2, P) :- subset_fbb(R3, P), subset(R2, R3, P).
```
We can put this into datatoad, first loading up the data and time it!
```
time:   234.75Âµs        ["reset"]
time:   897.106958ms    ["loaded"]
time:   37.587542ms     ["complete"]
```
Huh.
37ms.
That's pretty fast.
Much faster than 18s.
Let's look and see.

This is a list of the relations datatoad has, their sizes, and the ways we have them laid out.
```
      0 borrow_live_at
  76712 borrow_live_at_bb
   1886 borrow_region
  51896 cfg_edge
      0 errors
  76712 invalidates
    980 killed
 534231 outlives
1076158 region_live_at
      0 requires
      0 requires_bbb
      0 subset_fbb
      6 universal
```

This is very suss.

It's the right answer, that `errors` is empty, but that's not deeply reassuring.
We can see that `borrow_live_at_bb` has a bunch of records, so we are demanding something, exactly `invalidates` if you've been following along.
But immediately `requires_bbb` goes to zero.

Unfortunately for our story, that's correct.

Let's look at the last two rules again
```prolog
borrow_live_at(B, P) :- requires(R, B, P), region_live_at(R, P).
errors(B, P) :- invalidates(B, P), borrow_live_at(B, P).
```
For `errors` to be non-empty, there needs to be a `P` satisfying
```prolog
check(P) :- invalidates(B, P), region_live_at(R, P).
```
and there is none
```
0 check
```
I'm not sure if this is a data bug, or a logic bug, but it does seem to be correct that independent of the contents of `subset` or `requires`, on the `clap-rs` input there can be no errors, because no borrow is invalidated at the same CFG point that a region is live at.

Obviously, I'm reaching out to the Polonius folks to look for other inputs.
Unfortunately, the ones that come with the repo all seem to be for a more advanced analysis (at least, they have different files in their directories).
The more advanced analyses are not things that I'm going to do manually, which means I'm also going to code up the demand transform.

I was always going to code it up.

## Coding up the Demand Transform

Not yet accomplished!

## Problems with the Demand Transform

One anxious concern I've had about the transform is that it bakes a join order by binding values moving from left to right.
This corresponds with a left-to-right evaluation order, and makes a lot of sense if your plan is to do that.
However, if your plan is to do that you might get burned by cyclic joins.

Worst-case optimal joins specifically work around limitations in atom-at-a-time planning by thinking more carefully how to approach joins.
In particular, there are cases where every atom-at-a-time plan does asymptotically worse than a worst-case optimal join.
I have a concrete example from the FlowLog evaluation where this ends up being the case.

The `cvc5` workload is based on `DDISASM`, something I know literally nothing about.
It has several rules, and in particular spends most of its time in this one:
```prolog
Stack_def_use_def_used(zEA_def,VarDefr,VarDefp,Next_zEA_used,VarUsedr,VarUsedp) :-
    Stack_def_use_live_var_at_prior_used(zEA_used,NextUsedBlock,Varr,Varp),
    Stack_def_use_def_used(zEA_def,VarDefr,VarDefp,zEA_used,Varr,Varp),
    Stack_def_use_live_var_used(NextUsedBlock,Varr,Varp,VarUsedr,VarUsedp,Next_zEA_used,_0,_1).
```
If you, like me, cannot read this .. then we agree on something in the intersection of Datalog and software engineering.

This join takes all the time, and we might wonder "do we really need all of it?"
The one *use* of `Stack_def_use_def_used` is in this other recursive rule:
```prolog
Def_used_for_address(zEA_def,Reg1,Type) :-
    Def_used_for_address(EALoad,Reg2,Type),
    Arch_memory_access(332573,EALoad,Reg2,RegBaseLoad,332575,StackPosLoad),
    Stack_def_use_def_used(EAStore,RegBaseStore,StackPosStore,EALoad,RegBaseLoad,StackPosLoad),
    Arch_memory_access(333071,EAStore,Reg1,RegBaseStore,332575,StackPosStore),
    Reg_def_use_def_used(zEA_def,Reg1,EAStore,_0).
```
Also a bit of a mouthful, for me at least.

We can perform the demand transform to restrict our interest in `Stack_def_use_def_used` to (initially) facts that would contribute to this join.
If we put that atom last in the rule, to maximally bind variables, we would demand
```prolog
demanded(EAStore,RegBaseStore,StackPosStore,EALoad,RegBaseLoad,StackPosLoad) :-
    Def_used_for_address(EALoad,Reg2,Type),
    Arch_memory_access(332573,EALoad,Reg2,RegBaseLoad,332575,StackPosLoad),
    Arch_memory_access(333071,EAStore,Reg1,RegBaseStore,332575,StackPosStore),
    Reg_def_use_def_used(zEA_def,Reg1,EAStore,_0).
```
There is an additional demand rule to add based on the recursive use of `Stack_def_use_def_used` in its own definition, but we already have hints of a problem here.

The `Def_used_for_address` rule is cyclic, by way of `Stack_def_use_def_used`, and if we compute `demanded` without access to `Stack_def_use_def_used` we are forced to evaluate the join in a particular way that .. explodes the data outward.
If our point of comparison was a join that did left-to-right evaluation, we wouldn't feel bad as we've already paid that costs.
Since our point of comparison is a worst-case optimal join that goes much faster, this ends up as a marked regression.
```
   15460632 demanded
    2065369 Stack_def_use_def_used
```
This is already many times larger than the collection we were hoping to minimize, just using the first rule (not even the one from recursive use).
I added the recursive rule, and .. it's still running (it completed 215s later).

We can do something useful with a restricted form of the demand transform, essentially being less aggressive and only binding the first three arguments.
This reduces the volume of opinions we have, and ends up being helpful without much cost, though not as transformational as with Rust's Polonius.
```prolog
demanded(EAStore,RegBaseStore,StackPosStore) :-
    Def_used_for_address(EALoad,Reg2,Type),
    Arch_memory_access(332573,EALoad,Reg2,RegBaseLoad,332575,StackPosLoad),
    Arch_memory_access(333071,EAStore,Reg1,RegBaseStore,332575,StackPosStore),
    Reg_def_use_def_used(zEA_def,Reg1,EAStore,_0).
```
Introducing this variable, and using it to gate the production of facts, results in an improvement from 19s to 15s.
It's good, and definitely an improvement, but it also called out a risk along the way, that the demand transform as stated could be making things worse in the presenced of cyclic joins.

## Up Next

I'll need to code this up and build up a better intuition.
The improvements for the Rust problem, even if a fluke, show that for some queries and inputs there are orders of magnitude to recover.
The regressions for the DDISASM problem show that some caution is warranted, and perhaps we should restrict the transform to acyclic components.

It seems like there could be space for a worst-case optimal demand transform, but there is a tension.
The worst-case optimal techniques avoid looking at disproportionately large data, making choices based on counts.
The demand transform draws out *data* we might need, but doesn't have a way to draw out information like counts without drawing out all of the data itself, at which point we are dead.
So, some tension between the demand-driven work, and the algorithmic richness of the worst-case optimal approaches.