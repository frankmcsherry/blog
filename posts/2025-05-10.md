# Asynchronous All-Reduce

The beating heart that powers timely dataflow is "progress tracking".
At it's core, timely dataflow's progress tracking is a distributed reference counter, tracking the "counts" of outstanding units of work, across locations in the dataflow and at various timestamps.
This information is the control plane of timely dataflow, it is what communicates forward progress amongst the participating workers and unblocks their future work.

Although the control plane, this progress information can be high volume and high velocity.
Lots of things are continually happening in timely dataflow.
Our *implementation* ends up using tools from timely's data plane: high-throughput point-to-point channels.

These tools end up leaving a great deal of performance on the table, and we'll talk through those missed opportunities in this post.
Fortunately, each of the opportunities retains the performance characteristics of the data plane: high throughput and low latency.
Unfortunately, I haven't yet figured out how to achieve all of them at the same time.

The post's title, "Asynchronous All-Reduce", is what I think you would name the primitive that timely's progress tracking requires, and is the target we'll aim at in this post.

## Background

Timely dataflow's progress tracking is a protocol among workers jointly performing work that occurs at nodes in a directed acyclic graph.
When a worker retires a task at some node, it may create any number of tasks at other nodes referenced by that node.
As workers work, the pending tasks "flows downhill" along the directed acyclic graph.

---

**ASIDE**: Timely dataflow graphs themselves needn't be acyclic, but the graph we will be working with has as its nodes pairs of `(operator, timestamp)`.
This graph is a copy of the dataflow graph for every timestamp, connected within one timestamp by dataflow edges, and at each operator by directed edges to future timestamps.
This graph will be acyclic, because timely dataflow requires that to go around a dataflow cycle the timestamps involved must strictly advance.
If there is a path from `(op, t1)` to `(op, t2)` then `t1 < t2`, preventing a cycle.

---

The team of workers would like to maintain an ongoing understanding of the number of outstanding tasks at each graph node.
More specifically, they want to know the set of graph nodes that both 1. have tasks themselves, and 2. have no antecedants who have tasks.
These are the graph nodes that are "on the frontier" of completing work, meaning both that they are ready to go (waiting on nothing) and also that we had best get on them if we want to move that frontier forward.

The way progress tracking works in timely dataflow is that workers summarize the net outcome of their work, in terms of the changes to counts of tasks at each graph location.
If the worker retires five work items at `(A, 5)`, and produces two items for `(B, 5)`, one for `(C, 5)`, and three for `(A, 6)`, it would summarize its work as:
```
(A, 5), -5
(B, 5), +2
(C, 5), +1
(A, 6), +3
```
A summary tells others, without detailing the work performed, about the net effect of the worker's work.

Each worker produces a sequence of these summaries.
The sum of each of these summaries tells us what each worker has accomplished in net.
If we add up these sums across all workers we learn what the team of workers has accomplished.
From an agreed-upon starting point, in timely it is one task per worker per dataflow graph node at the least timestamp, the accumulated sum of summaries tells us what tasks are outstanding now.

As long as each worker sees a sum that corresponds to a prefix of each worker's sequence of summaries, the sum provides them with a conservative view of the remaining tasks in the system, and the frontier of progress.
As long as each worker eventually sees each update from each worker, any frontier progress seen by one worker will eventually be observed by all others.
This "coordination" is asynchronous: the workers are never certain if they have the same view as the others, and indeed they may never have exactly the same view.
But they all eventually reach the same conclusions.

Things go very wrong if you duplicate, drop, or re-order summaries.
There are ways to avoid that, but let's not discuss that here.

This communication primitive, sharing sequences of updates that can be accumulated, so that all participants see sums of increasing but unrelated prefixes, is what I mean when I say "asynchronous all-reduce".
There could be a better name for it; please let me know if it is known to you and already has a name.

## A naive implementation

The most naive implementation is to have each worker send its summaries to each other worker directly along a FIFO channel.
One FIFO channel for each pair of workers.
Timely dataflow already knows how to set up FIFO channels, as part of its data plane, so this is a natural first implementation.

This naive implementation reveals a lot more information that we strictly needed to reveal.
It shows each worker the exact sequence of summaries from each other worker, when all we really needed was to see partial sums of some prefixes of updates.
This additional provided detail comes with inherent performance limitations, which we respectively do not need and do not want.

## Opportunity 1: Buffering

A worker that produces a sequence of summaries doesn't need to send all of the summaries immediately.
It is not incorrect to accumulate multiple summaries and transmit them as one.
This can be especially helpful if parts of the summaries cancel, such as when a worker creates a task that it then immediately retires.

The risk is that naive buffering delays the communication of information.
Our progress channels aim to be *prompt* as well as correct, and we would love to avoid delaying actionable information.
Simply accumulating until some timeout fires isn't the sort of buffering we are hoping to accumplish.

Fortunately, there is a form of buffering that is very helpful.
We know that workers are interested in the "frontier of completing work": the earliest graph nodes that still have tasks.
Each worker can buffer its summaries as long as none of them affect nodes in the current frontier.
If the worker produces a summary that does mention a frontier node it should flush the buffer (we need to present a prefix).
If the worker receives a summary that cause the frontier to move forward and now intersect its buffer, it should flush the buffer (as it is now holding back progress).

This form of buffering is present in timely dataflow, and resulted in several orders of magnitude reduction in traffic when it was implemented.
A very common pattern of summary in timely is a worker receiving and processing messages for a dataflow operator.
Those summaries will look like `[(B, T) -X, (C, T) +Y]`. 
However, the continual receipt of messages at `B` implies some worker is still processing `(A, T)`, and as long as that is the case workers can accumulate their summaries involving only `B` and `C`, and ship them only once they hear that `A` is finished (and `B` enters the frontier).

The potential downside of this approach is that it extends the critical path of progress.
Because workers delay sending summaries until they hear from other workers, there may need to be a *sequence* of events that lead to the communication of progress.
We haven't seen this be a problem in practice, but it seems like it could be a problem in theory.

## Opportunity 2: Aggregation

One subtle defect of many distributed protocols, including our naive progress protocol, is that idle workers may use their free time creating protocol work for busy workers.
The busy workers then fall further behind, because they have so much protocol work to catch up on.

Imagine three workers X, Y, and Z.
Perhpas X is quite busy, maybe it got most of the data to process, and Y and Z are relatively idle with light message load.
In their idleness, Y and Z may be creating work for each other, retiring that work, and creating detailed summaries explaining what they've done.
Every utterance they ship is something X will have to process when it gets time, which it is already short of.

An alternate design is to replace the point-to-point FIFO queue with a shared accumulating buffer.
Imagine a type `Vec<(T, isize)>`: you can extend it with new summary information, but you can also accumulate the updates for each `T` and discard any that add to zero.

Concretely, we could use an `Arc<Mutex<Vec<(T, isize)>>>` instead of the point-to-point FIFO channels.
These would allow the *writer* to perform compaction that would relieve the *reader* from having to perform the same work later.
If the reader stays current there is no work for the writer (the buffer is mostly empty), and if the reader does not stay current the writer's compaction will be helpful.

This is a more complicated communication primitive than a FIFO channel, which can be implemented with some compare and swaps.
With a `Mutex` workers could actually block each other by holding a lock and doing something unfortunate (like re-allocating, when extending the buffer).
The reader can always nope out of acquiring the lock, as it isn't required to receive the information immediately, but we would need to avoid having the writer starve out the reader.
Several things to consider, but there is promise that idle writers can help busy readers.

## Opportunity 3: Sharing

Everything thusfar has been point-to-point, which feels bad because everyone wants to see the same thing.
Shouldn't they all just read the same actual in-memory thing?

A FIFO channel expects folks to consume from it, and each time this happens the consumed thing is gone.
Multiple people consuming from the same channel will *distribute* the summaries, rather than *broadcast* them.
This is not what we want.
It is close, though!

A common implementation of FIFO channels is as a linked list, with a list head that the writers compare-and-swap into, and a list tail that readers compare-and-swap out of.
We want to change that implementation so that readers just advance along the list, *reading* but without *taking* anything.
As readers advance the reference counts to the tail of the list go to zero and content everyone has read just drops.

With this alternate queue, all of the point-to-point queues could be replaced by one shared queue.
All writers write to the same queue, and all readers read from it.

In fact it isn't clear that one queue is the right answer.
Not much is gained by having writers contend on the same queue, and we could just have one queue for each writer, read by all readers.
This removes the contention among writers, and allows us to use primitives that already exist in timely dataflow (a single-writer, multiple-reader channel).
The sharing among writers would be appealing if we understood better how to consolidate the writes, as in the previous opportunity, but the queue-based approach doesn't seem to help with this.

## Reflecting on the opportunities

To be clear, I don't know how to bring these techniques together for the perfect solution.
They represent various opportunities, reducing communication, offloading computation, and deduplicating information.
Some seem to compose well (the first opportunity with the other two), but I can't yet see how all three come together.

But I do like pondering the problem, as thinking about it gets at the heart of coordination in timely dataflow.
Just how little effort is required of a team of workers to stay up to date on their progress?
Additionally, who pays for that effort, and what does it mean about when the team first arrives at certainty about some form of progress?
How does concurrent work ends up as collaboration rather than contention, and how do you navigate that boundary?

Part of me would love to figure out the perfect communication primitive for timely progress tracking.
Another part of me is equally pleased to have something I don't understand but continue to learn from.
But seriously, if you have heard of how to do this asynchronous all-reduce thing, please let me know.