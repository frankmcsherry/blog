## Strings in differential dataflow

In the course of using differential dataflow, you may find yourself using types that contain strings. In [one case of program analysis](https://github.com/frankmcsherry/differential-dataflow/tree/master/doop) that I've been working with, the input records look like tab-separated lines of text:

```
<sun.font.SunFontManager$14: void <init>()> <init>  ()  sun.font.SunFontManager$14  void    ()V 0
<sun.awt.image.VSyncedBSManager$SingleVSyncedBSMgr: void <init>()>  <init>  ()  sun.awt.image.VSyncedBSManager$SingleVSyncedBSMgr   void    ()V 0
<javax.swing.AbstractAction: boolean shouldReconfigure(java.beans.PropertyChangeEvent)> shouldReconfigure   (java.beans.PropertyChangeEvent)    javax.swing.AbstractAction  boolean (Ljava/beans/PropertyChangeEvent;)Z 1
<sun.font.SunFontManager$14: void <init>()> <init>  ()  sun.font.SunFontManager$14  void    ()V 0
...
<java.awt.geom.AffineTransform: void transform(double[],int,double[],int,int)>  transform   (double[],int,double[],int,int) java.awt.geom.AffineTransform   void    ([DI[DII)V  5
<java.awt.geom.AffineTransform: void transform(double[],int,double[],int,int)>  transform   (double[],int,double[],int,int) java.awt.geom.AffineTransform   void    ([DI[DII)V  5
<java.awt.geom.AffineTransform: void transform(double[],int,double[],int,int)>  transform   (double[],int,double[],int,int) java.awt.geom.AffineTransform   void    ([DI[DII)V  5
...
<org.gjt.sp.jedit.pluginmgr.MirrorList: void readXml()> readXml ()  org.gjt.sp.jedit.pluginmgr.MirrorList   void    ()V 0
<sun.nio.cs.StandardCharsets$Aliases: void init(java.lang.Object[])>    init    (java.lang.Object[])    sun.nio.cs.StandardCharsets$Aliases void    ([Ljava/lang/Object;)V  1
<sun.nio.cs.StandardCharsets$Aliases: void init(java.lang.Object[])>    init    (java.lang.Object[])    sun.nio.cs.StandardCharsets$Aliases void    ([Ljava/lang/Object;)V  1
<org.gjt.sp.jedit.pluginmgr.MirrorList: void readXml()> readXml ()  org.gjt.sp.jedit.pluginmgr.MirrorList   void    ()V 0
...
```

Barf.

These particular records have a type that is in essence `[String; 6]` (an array of 6 strings), and we need to perform maps, filters, joins, grouping, and even iteration, all using types like this (with other values of `6`).

### Strings work!

Before anything too exciting, the `String` type totally works in differential dataflow. You can go and do graph computations where the node type is `String`, or `(Vec<String>, bool)`, or whatever crazy types you kids use to do graph processing in Javascript (lol, I know... "types").

The `String` type works but it can be a bit inefficient when compared to `usize`, which is just a much simpler type to work with when we want to hash-distribute records, or sort records, or group them by keys. It is also much faster to clone a `usize` than a `String`; the latter means we must allocate some memory and most likely de-allocate it at some future point.

### Interning Strings

While `String` works, we might like to perform something like [string interning](https://en.wikipedia.org/wiki/String_interning), in which each of the string instances is replaced by one representative instance, deduplicating the actual strings (allocations) being used.

Instead, let's try replacing each string with a unique integer identifier. We do lose the ability to do string-y operations, like substrings and converting to upper and lower case, but we can still do equality testing and hash-distribution, and many of the core things we need to do in our data-parallel compute lifestyle.

In the context of the project linked above, we can still execute Datalog programs that only really care about exact matches between fields.

Single-threaded, this would be super easy:

```rust
/// Ensure `string` exists in `map`, return unique identifier.
fn intern_in_map(string: String, map: &mut HashMap<String, usize>) -> usize {
    let len = map.len();
    *map.entry(string)
        .or_insert(len)
}
```

Whenever we intern a string, we either return its existing identifier or add it with a new distinct identifier and return that.

This skips a few fun issues like do we ever *remove* interned strings (not here), how to we look up strings from their identifiers (second map, I guess), and maybe a few other issues. We aren't going to stick with this approach, but it is good to see that it isn't all that painful.

### Interning Strings *IN DIFFERENTIAL DATAFLOW*

Let's write a fairly simple computation that will intern strings using differential dataflow.

Can't we just use the above fragment? Sure, sure. I did that in the code linked up above. It works, it's totally fine, but it just feels a bit gross and non-robust. For example, we have to do all of the data loading on one thread, because we can't intern the strings in parallel. If the set of strings changes dramatically, we keep all of the pre-existing strings around grotting up the place. Also c'mon, this is going to be neat and you might learn something. :D

As a first, guess, couldn't we just intern strings by hashing each of them and using the hash as the integer identifier? Almost, but there could be collisions. How about, if there are collisions, we pick one winner and re-hash the others?

Winner!

```rust
/// Assigns a unique identifier to each element of `collection`.
fn intern<G, D>(collection: &Collection<G, D>) -> Collection<G, (D, usize)>
where
    G: Scope,
    G::Timestamp: Lattice,
    D: Data,
{
    collection
        // initialize each string at "round" zero.
        .map(|record| (0, record))
        .iterate(|temp| {
            // propose a candidate hash from (round, string),
            // group by hash and pick at most one winner,
            //       defer non-winners to the next round.
            temp.map(|pair| (pair.hashed(), pair))
                .group(|_hash, input, output| {
                    // first (round, string) wins!
                    output.push((input[0].0.clone(), 1));
                    // if any losers, increment their rounds.
                    for ((round, record),_count) in input[1..].iter() {
                        output.push(((*round+1, record.clone()), 1));
                    }
                })
                .map(|(_hash, pair)| pair)
        })
        .map(|pair| (pair.1, pair.hashed()))
}
```

This works great. Each string in `collection` gets a fairly random hash in round zero, and if there are any collisions we pick a winner (the earliest round, breaking ties lexicographically by string) and promote the losers to the next round. We don't really expect many collisions, and we really don't expect any one string to repeatedly collide with other strings (assuming our `.hashed()` method is worth anything).

What happens when we have a change in our input strings?

Not very much, which is great news! There was so little interaction of strings, that there are similarly sparse changes to their interactions. Perhaps if we remove a winner we end up finding the string or two it collided with and naming them winner, but most of the time we add a string and give it an identifier or remove a string and remove its binding to an identifier.

### Using interned strings

Way back up there we had some horrible `[String; 6]` folks that reminded us about Java and that evil still walks the Earth. How do we wire together a string interning fragment (like just above) with our stream of Java six-tuples?

I'll show you a pretty naive version, and I point out how to do it more smartly and leave it as homework!

The most naive way to do things is to i. extract the candidate strings to intern, and then ii. repeatedly join the six-tuples with the interned strings to extract identifiers for each coordinate at a time.

```rust
// Each six-tuple offers six strings to intern.
let strings = six_tuples.flat_map(|x| x);

// Use that method up above.
let interned = intern(&strings);

let interned_six =
six_tuples
    // map each to (next_key, ([hashes], [strings])).
    .map(|x| (x[0], ([], x[1..6])))
    .join_map(interned, |_k,h,(hs,ss)| (ss[0], ((hs,h), ss[1..5])))
    .join_map(interned, |_k,h,(hs,ss)| (ss[0], ((hs,h), ss[1..4])))
    .join_map(interned, |_k,h,(hs,ss)| (ss[0], ((hs,h), ss[1..3])))
    .join_map(interned, |_k,h,(hs,ss)| (ss[0], ((hs,h), ss[1..2])))
    .join_map(interned, |_k,h,(hs,ss)| (ss[0], ((hs,h), ss[1..1])))
    .join_map(interned, |_k,h,(hs,ss)| (hs,h));
```

Ok, now all those `join_map` closures aren't actually legit Rust, but I hope you get the gist about what is going on. Six times, we extract the first remaining string and use `join_map` to find us an integer hash value to replace it, and keep the remaining strings.

---

**HOMEWORK**: The approach above has the defect that five times, we arrange and maintain the strings that we have not yet interned, which is pretty silly. It's not the end of the world, but seriously...

The good news is that we recently got [some sweet join tech](https://github.com/frankmcsherry/differential-dataflow/tree/master/dogsdogsdogs) that lets us implement "delta queries" in differential dataflow, those being join evaluation strategies that do not materialize intermediate relations, and which could potentially maintain *only* the `interned` collection in indexed form.

---

### Update (2018-11-21)

Hey I tried that homework up above. I bet you did too, right? Pretty hard, huh?

Despite being pretty sure that the `dogsdogsdogs` project has positive implications, I think you still need to keep around the original `[String; 6]` data somewhere, because if any string identifiers change you'll need to push the changed `[usize; 6]` records. If we naively indexed each `[String; 6]` keyed by each of the six fields we are actually doing *worse* than above, recording each string six times.

To tidy this up, we will need to be a bit more clever.

### Row Identifiers *IN DIFFERENTIAL DATAFLOW*

Sorry, wait what?

In a great many database systems and schemas, when you introduce new records to a relation that does not otherwise have a primary key, you automatically get an auto-incremented "row identifier" added in that acts as a primary key. This row identifier acts as a key that lets you speak about the rows without slogging their data around all over the place. When performing joins and such a database system can just track the row identifiers involved (and perhaps the subset of attributes) rather than the potentially kilobytes of record payload.

Let's do that *IN DIFFERENTIAL DATAFLOW*!

Specifically, let's do it in differential dataflow *so that we can more efficiently intern strings*, but at the same time we'll come up with a great way to implement relational joins. The first one is probably more important, right?

Let's imagine we start with some collection of string six-tuples:

```rust
let horror: Collection<_, [String; 6]>;
```

and what we want most is to assign unique identifiers to each six-tuple.

```rust
// Use the string interning method up above.
let horror_ids = intern(&horror);
```

Yeah, we already invented it. Oops.

### Decomposing records

Let's use these record identifiers to break apart the six-tuples into six collections, one for each attribute, associating each record identifier with the value of its attribute.

```rust
let attr_row_value =
horror_ids
    .flat_map(|(horror, id)|
        horror
            .into_iter()    // six-element iteration.
            .enumerate()    // pre-pend with attr_id.
            .map(|(attr, value)| (attr, (id, value)))
    );

// Partition collections by attr_id (timely magic).
let attr_collections: Vec<Collection<G, (usize, String)>> =
attr_row_value
    .inner                  // inner timely stream
    .partition(6, |((a,(i,v)),t,r)| (a, ((i,v),t,r)))
    .into_iter()
    .map(|stream| stream.as_collection())
    .collect::<Vec<_>>();
```

Ok, now we have six collections. One for each attribute, each containing records like `(row_id, value)`. That was amazingly fun, of course, but why did we do this?

There are at least two reasons that I can think of; let's talk each of them out.

#### String interning

Obviously this is why we are here. We can independently intern each of the six collections, like so:

```rust
let attr_interned =
attr_collections
    .iter()
    .map(|collection|
        collection
            .map(|(row_id, value)| (value, row_id))
            .join_map(&interned, |_, row_id, hash| (row_id, hash))
    .collect::<Vec<_>>();
```

Now we have six collections containing `(row_id, hash)` pairs, and we can use those more-or-less as if they were `(row_id, value)` pairs! How was that again?

#### A la carte columns

Our decomposition of our relation into the attributes allows us to do a bit of a la carte columnar join processing, which can be fun.

We can reassemble our collection on subsets of its attributes (performing a projection, but skipping irrelevant data) like so:

```rust
let attrs_034 =
attr_collections[0]
    .join(&attr_collections[3])
    .join(&attr_collections[4]);
```

Because differential dataflow is rly smort, when we `arrange` each collection in `attr_collections` they are managed sorted by their key, `row_id`, and the joining is essentially a merge. It is similarly easy to break out the [dogsdogsdogs](https://github.com/frankmcsherry/differential-dataflow/tree/master/dogsdogsdogs) delta queries and worst-case optimal joins.

We can filter each of the attributes, and then join, which allows us to tap dance through only the relevant records in each of the relations. For example:

```rust
let attrs_034 =
attr_collections[0]
    .filter(|(row_id, value)| value.len() > 4)
    .join(&attr_collections[3])
    .join(&attr_collections[4]);
```

This works less well once we've rocked things down to interned string identifiers, but .. well maybe this section should have come before that section. Still working on that.
