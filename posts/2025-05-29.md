# Binary joins can be worst-case optimal

Convential wisdom in the database community is that query plans that can only use binary joins can require asymptotically more computation than the number of output records they might possibly produce.
This led to a class of new join algorithms called ["worst-case optimal"](https://cs.stanford.edu/~chrismre/papers/paper49.Ngo.pdf) joins, whose running time is asymptotically bounded by the number of output records the query could possibly produce on a dataset of a fixed size.
The conventional wisdom is .. potentially misleading.

To quote from the abstract of the paper linked above

> [W]e show in this paper that any project-join plan is polynomially slower than the optimal bound for some queries.

and from the conclusions

> We establish optimal algorithms for the worst-case behavior of join algorithms. We also demonstrate that the join algorithms employed in RDBMSs do not achieve these optimal bounds.
> Moreover, we demonstrate families of instances where join-project algorithms are asymptotically worse by factors close to the size of the largest relation.

Reading between the lines, you might conclude that conventional databases whose only join operator is the conventional binary join may be at an asymptotic disadvantage over databases that have specialized worst-case optimal join operators and implementations.
This conclusion is incorrect, for the simple reason that you can implement a worst-case optimal join algorithm using existing binary join operators.

The quoted statements above are not false.
The load-bearing modifier is when they say "project-join".
If you can only join and project, you will run afoul of their results.
However, the statements are not true if we instead say "project-join-count".
Conventional databases are all able to count things (except Datalog which, as they say, doesn't count as a database).

You don't need new join **implementations**, you only need new **query plans** for joins; ones that use counts.

## Counting undirected triangles with binary joins

The archetypical example of a query where binary and worst-case optimal joins differ is counting triangles in an undirected graph.
You occasionally hear "any join plan for counting triangles using binary joins has an input that is asymptotically worse than a worse-case optimal join".
I have said this out loud in talks.
It is incorrect.

Given a symmetric relation `edge(a, b)`, a triangle `(a, b, c)` exists when edges exist between each of the three pairs of identifiers.
We might write this in Datalog syntax as
```
tri(a, b, c) : edge(a, b), edge(a, c), edge(b, c)
```
If you found all possible values of a, b, and c you'd have all of the triangles in the graph.

If you only have so many edges, let's denote it `|edge|`, then a famous theorem of Atserias, Grohe, and Marx says that there can be at most `|edge|^{3/2}` triangles.
However, if all of those edges are incident on some vertex `a`, then the intermediate join term
```
{ (a, b, c) } : edge(a, b), edge(a, c)
```
can have size as large as `|edges|^2`.
This is polynomially larger than the AGM bound, and if your plan was to just compute this binary join, you have already spent more computational effort than you could possibly produce results.

Doing this binary join will absolutely result in poor outcomes.
If you have only a conventional database, don't do this join as part of computing triangles.

### From symmetric to undirected

Everything above, other than the section heading, is for *symmetric*, rather than *undirected*, graphs.
If you count then number of elements in `tri(a, b, c)` you'll get six times the number of triangles in an undirected graph, because there are six ways you can write any triple of values.
You could of course just divide the number by six and you are good to go.
Moreover, it's only a factor of six, so you couldn't go asymptotically faster if you only computed one sixth of the results directly.

That last sentence is not correct, and that's where we are going to lean in and focus.

One way to avoid the factor six blow-up is to require an order on the a, b, and c.
We can do this by adding a new constraint involving the relation `<`.
```
tri(a, b, c) : edge(a, b), edge(a, c), edge(b, c), a < b < c
```
This leads us to discover each triple exactly once, but it doesn't solve our problem.
All edges could be incident on the lowest identifier, and we would still have a quadratic number of candidates a, b, c after the first binary join.
This constraint does reduce the count by a factor of six, but does not reduce the size of the intermediate result.

### Worst case optimal undirected triangles

Let's use a different constraint then.
Rather than ordering the identfiers, let's order the degrees of the identifiers.
Given a relation `deg(a, da)` that relates an identifier and its degree, the number of edges that mention the identifier, we can write the Datalog
```
tri(a, b, c) : edge(a, b), edge(a, c), edge(b, c),
               deg(a, da), deg(b, db), deg(c, dc),
               (da, a) < (db, b) < (dc, c)
```
This asks for a potentially different representative for each triangle out of the six.
It will also lead us to a worst case optimal plan.
This is the moment where you should say "who let you use that `deg` relation?" to which I say "my conventional database did".
This relation is simply 
```sql
SELECT a, COUNT(*) FROM edge GROUP BY a
```

You might now worry that I've only added things to the end of the plan above, so wont it still produce all the many intermediate results?
These constraints can be ordered however we like, and I'll block them out more evocatively as
```
tri(a, b, c) : edge(a, b), deg(a, da), deg(b, db), (da, a) < (db, b),
               edge(a, c), deg(a, da), deg(c, dc), (da, a) < (dc, c),
               edge(b, c), deg(b, db), deg(c, dc), (db, b) < (dc, c)
```
Each row is really the same, orienting each edge from lower degree to higher degree.
We could alternately block this out as two rules:
```
dir(a, b) : edge(a, b), deg(a, da), deg(b, db), (da, a) < (db, b)

tri(a, b, c) : dir(a, b), dir(a, c), dir(b, c)
```
A naive left-to-right binary join evaluation will now be worst-case optimal.
It ends up being a special case of a classical algorithm for triangle counting:

> For each node, if its degree is less than `|edges|^{1/2}` enumerate all pairs of neighbors and test for an edge between them, and otherwise `<WE DON'T CARE>`.

All *directed* degrees in `dir` are at most `|edges|^{1/2}`, because to have a high degree you must have as many neighbors with degree at least yours, meaning their total degree is at least the square of yours.
That total degree cannot exceed the number of edges.
There is a factor of two somewhere because each edge has two endpoints, but we are ignoring all constants other than six.

So. Counting undirected triangles worst-case optimally using only binary joins.
Or, binary joins plus counting.

## Generalizing from triangles

The first worst-case optimal join I'm aware of was [leapfrog triejoin](https://arxiv.org/abs/1210.0481).
It uses a trie-structure representation of data, and performs a depth-first search of matching bindings to query terms.
In essence, it performs an ongoing "merge" of the tries of each relation, each laid out in a common attribute order.
That is a massive oversimplification, and I recommend you read the paper for more details.

The algorithm in the paper is bespoke, using new code rather than using existing database operators.
This is great, as I love new algorithms.
But you don't have to do it this way.

There is a class of join algorithms called [generic join](https://arxiv.org/pdf/1310.3314e), which includes leapfrog triejoin as a special case.
To handwave (intuition thanks to Oliver Kennedy), you can explore the same space of solutions using many search techniques, with depth-first leading to leapfrog triejoin, and breadth-first leading to .. some other algorithm.
That other algorithm (name pending, but I've been calling it ["treefrog leapjoin"](https://crates.io/crates/datafrog)) can be written only in terms of joins and counts.

First up, credit to lots of other people.
I didn't invent any of these algorithms.
They were revealed to me through interactions with smarter people.
Specifically, Semih Salihoğlu and Chris Ré.
Back in 2014 I was at MSR in Silicon Valley and they were at Stanford.
We had a dataflow engine that didn't support random access to trie-structured data, and instead needed to explicitly stage and shuffle data.
That shift was from depth-first to breadth-first, but what I didn't realize at the time was that it was also to binary joins.

I'll explain the algorithm here.
It's not complicated, but I recommend [this paper](http://www.vldb.org/pvldb/vol11/p691-ammar.pdf) if you'd like a more complete description.

Our goal is to produce all valid bindings of some query relation that results from the intersection of constraints.
```
query(a, b, .. z) : bound1(a, b .. z), bound2(a, b .. z), ..
```
The conventional approach would be to pick up pairs of bounds with variables in common and join them, until we have included all constraint bounds.
This approach is bad, because the intermediate results might blow up, and we took care above to not do exactly that.

Instead we are going to iterate through the bound variables: first a, then b, then .. ultimately z.
For each prefix of bound variables, we'll compute the exact query answer when the constraints are restricted to this prefix.
In other words we'll compute the sequence of intermediate results `query_x` as
```
query_a(a)          : bound1(a),         bound2(a),         ..
query_b(a, b)       : bound1(a, b),      bound2(a, b),      ..
..
query_z(a, b, .. z) : bound1(a, b .. z), bound2(a, b .. z), ..
```

It turns out `query_a` will be really valuable for starting `query_b`, which will be valuable for starting `query_c`, and so on.
Generally to go from `query_i` to `query_j`, where `i` directly precedes `j` (like in the alphabet, if not all of mathematics), we can write out `query_j` with a seemingly redundant `query_i` term:
```
query_j(a, .. j) : query_i(a, .. i), bound1(a, .. i, j), bound2(a, .. i, j), ..
```

To produce the tuples in `query_j` we'll start from `query_i`, asking for each of its member bindings "what values of `j` can be added that satisfies each of the bounds?".
To find the valid `j` we could join with any of the bounds, to propose `j` values, and then intersect with the other bounds.

The key insight, not mine, is that we would like to use the bound that proposes the fewest values, and then intersect against the other bounds.
A proposal requires work proportional to the number of values proposed, but an intersection only requires work proportional to the minimum of the two intersected sets of values.
The critical bonus insight is that we can make this call on a binding-by-binding basis, rather than statically for all bindings.
How do we make that call? 
We *count* the number of distinct bindings that would be proposed by each bound, for each prefix.

For each bound, for each prefix length `i`, we compute
```sql
-- number of distinct extensions to each prefix.
SELECT a, b, .. i, COUNT(DISTINCT j) 
FROM bound
GROUP BY a, b, .. i
```

This SQL fragment describes a new relation we'll call `count_j`, and we suffix with some numbers to distinguish between the different `bound` inputs.

The worst-case optimal algorithm results from joining `query_i` with each of the `count_j`, to determine the bound that will propose the fewest distinct extensions `j`, having that bound propose extensions, and then intersecting with all other bounds.
Let's try and write it out using Datalog-style joins.
```
% start from bound1, track the index and count of the bound with fewest proposals for j.
best_j1(a .. i,  1, nc) : query_i(a .. i), count_j1(a .. i, nc), pc >= nc

% consider bound2, and overwrite when there are fewer proposals
best_j2(a .. i, pi, pc) : best_j1(a .. i, pi, pc), count_j2(a .. i, nc), pc < nc
best_j2(a .. i,  2, nc) : best_j1(a .. i, pi, pc), count_j2(a .. i, nc), pc >= nc

% repeat for each bound
..

% conclude by projecting away the count leaving only the index.
best_j(a .. i, pi) : best_jxyz(a .. i, pi, pc)
```
This looks complicated, but it's a pretty easy multiway join.
All relations are being joined on the same key, `(a, .. i)`, and we don't need to shuffle any data as we go.
If each of the counts were in a map, this would just be a for-loop over `query_i`, and a look-up in each of the maps.
But that is the same as a binary join. Phew.

We now know for each `a .. i` which bound will propose the fewest candidates, and we'll produce the proposals by joining each against their indicated bound.
```
% collect proposals from the most constraining bounds.
prop_j(a .. i, j) : best_j(a .. i, 1), bound1(a .. i, j)
prop_j(a .. i, j) : best_j(a .. i, 2), bound2(a .. i, j)
..
```

This looks like a lot of joins, but it represents a partitioning of `best_j`: every element joins with one bound, using the best index it found.
Each of the binary joins are just binary joins; no complicated plans.
The results are unioned together (so, "union" is another operator we need beyond "count").

These are only proposals, and they need to be validated by all relations.
We write that query the same way we described `query_j` up above.

```
% intersect all proposals against all relations.
valid_j(a .. i, j) : prop_j(a .. i, j), bound1(a .. i, j), bound2(a .. i, j)
...
```
A vanilla left-to-right binary join plan works great here.

Now .. we just use `valid_j` as `query_j`.
It has exactly the extensions that all bounds agree on.
We only used binary joins, left-to-right, as well as counts and unions.
Although wordier, certainly, the joins are simple and we can use (and re-use) maintained indexes on bounds.
In fact tries are a great way to support all of the counts, proposals, and validations, but independent indexes work too if your database doesn't use tries under the hood.

## Conclusions

If you are worried about worst-case optimal joins, you may not need a new database as much as a new query plan.
Conventional databases can represent worst-case optimal joins using only binary joins, if they also use counts and unions.
That being said, these databases may choose terrible plans for these joins.
You may need to explicitly stage the joins to force good behavior, because I suspect none of these databases are thinking as carefully as the above about how to perform a worst-case optimal join.

They would probably just compute all the triangles, and then filter by degree.

If you would like to read more, [differential dataflow supports worst-case optimal joins](https://github.com/TimelyDataflow/differential-dataflow/tree/master/dogsdogsdogs), both with advanced custom primitives, but also just using its built in binary `join` and aggregating `reduce` methods.
You can see the implementation for triangles [here](https://github.com/TimelyDataflow/differential-dataflow/blob/master/dogsdogsdogs/examples/ngo.rs).
Surprisingly, to me, this has been true for eight years, and even since then I have been saying that databases built around binary joins are fundamentally broken.

I was totally wrong.
It's the query planners that are broken.
Mea culpa.

Also, don't leap to the wrong conclusion about the work.
We need the new algorithms to show us how to do joins efficiently.
It's just amazingly good luck that we don't need new database primitives.
It's not like the conventional databases knew this ahead of time.
Tragedy averted, with thanks to the folks who produced the theory of worst-case optimal joins.