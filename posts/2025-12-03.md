# Worst-Case Optimal Datalog

This post is a check-in on [datatoad](https://github.com/frankmcsherry/datatoad), which I'm going to provocatively call a "worst-case optimal Datalog".
It's possible that none of that makes any sense to you, and I'll try and explain what is meant, but 100% I'm not sure that "worst-case optimal" applies to anything other than [joins](https://en.wikipedia.org/wiki/Worst-case_optimal_join_algorithm).
There is a good reason though, which I'll attempt to back up, but at the same time there's also a strong possibility that I'm just wrong.

I'll write the things down, and we can see what smarter people think.

I should also stress that while datatoad is a fun project I've enjoyed working on, upon reflection much of what is going on in this post isn't especially new or novel.
Much of it is a creative reinterpretation of previous work on [streaming worst-case optimal joins](https://www.vldb.org/pvldb/vol11/p691-ammar.pdf), and in particular leaning on properties that a co-author (Semih Salihoğlu) identified.
Unless I'm wrong, in which case that wrongness is totally mine.

## Background: Datalog

[Datalog](https://en.wikipedia.org/wiki/Datalog) is a logic programming language whose evaluation is most commonly based on repeated application of relational equijoins.
A Datalog program is a collection of rules, each of which describes conditions under which some facts being true leads to other facts being true.
When combined with a starting set of facts, rules lead to an iterative process in which more facts can be inferred, iteratively, until eventually no further novel facts can be found.

A classic Datalog example is the `Ancestor` relationship, which can be described from a `Parent` relationship as
```
Ancestor(X, Y) :- Parent(X, Y).
Ancestor(X, Z) :- Parent(X, Y), Ancestor(Y, Z).
```
The two rules here indicate respectively that any parent relationship implies an ancestor relationship, but also that the parent of an ancestor is also an ancestor.
The presence of `Y` twice in the second rule is a constraint that the two terms must be equal, and when this happens the "head" of the rule becomes true with the corresponding bindings of `X` and `Z`.
Having seeded the ancestor relationship with the parent relationship, the second rule enables the derivation of further facts, and then further facts, and so on until saturation (when no novel facts are derived).

A standard way to implement the second rule is a relational equijoin between `Parent` and `Ancestor`, equating the second attribute of the first with the first attribute of the second.

Things get more complicated in "more realistic" Datalog programs.
Here is one of the rules from [an inferencing task on medical ontologies](https://www.vldb.org/pvldb/vol7/p1993-liagouris.pdf):
```
q(A, B, C) :- q(A, X, Y), r(X, Z, B), q(Y, Z, C).
```
If you squint your eyes just right, you can see that the `X`, `Y`, and `Z` terms form a "triangle", and the `A`, `B`, and `C` terms in the "output" just dangle off of those triangle edges (they are not otherwise constrained).
This is not just syntactically more complicated, as the presence of a triangle subquery makes the join "cyclic", which is a class of joins that until recently (well, ten years ago) folks didn't really have a great story for how to process efficiently.

## Background: Worst-Case Optimal Joins

As far back as the 1980s, the databases community has known how to process "acyclic" queries using time asymptotically bounded by the size of the input plus the size of the output.
The [Yannakakis algorithm](https://en.wikipedia.org/wiki/Yannakakis_algorithm) is the standard way to do this, and it's a very reassuring result: you have to read the input and produce the output, so really not much time wasted.

Unfortunately, the requirement of acyclicity means that queries as simple as a triangle query are not covered.
That rule up above with `q(A, B, C)` is cyclic, owing to the `X`, `Y`, `Z` triangle subquery.
The exciting positive result of the Yannakakis algorithm does not apply here.
And actually this is fine because I think the properties of the Yannakakis algorithm end up being weaker than we want, in any case.
Not that it's bad, just .. we won't be able to afford the "size of the input" term in each round of derivation.

Just over ten year ago there were two neat papers, introducing the concept of ["worst-case optimal joins"](https://en.wikipedia.org/wiki/Worst-case_optimal_join_algorithm).
The property of these joins is that they take time bounded by the largest possible output the query could have, if the inputs were adversarially chosen among relations with the same size.

For example, in a graph with `m` edges, there can be at most `m^{3/2}` triangles: triples where all three edges exist.
At the same time, one can show that there are inputs such that any plan based only on binary equijoins will produce at least `m^2` intermediate results.
That is asymptotically larger than the "worst case", whose exponent of 3/2 is markedly less than an exponent of 2.

The worst-case optimal join algorithms show that you can achieve the bound of `m^{3/2}` for the triangle query, and generally you can track the worst-case output size for arbitrary conjunctive queries (I think).

The worst-case optimal bounds are good insurance, but they aren't inherently great.
If you do the math for the `q(A, B, C)` query up above, on `m` input facts the output could be as large as `m^3`.
An approach based on two binary equijoins would produce at most `m^3` facts, which is the same.
Both of them are pretty terrible bounds.
No one wants to take time that is cubic in the input data.

The good news is that many of the worst-case optimal algorithms are also more robust in practice.
We'll discuss one of them in more detail, and it may be clear that at the expense of a bit of constant-overhead pfaffing about, we protect ourselves against certain classes of problematic input.
I'm not aware of a crisp way to articulate the practical robustness, though others may be.

## Background: Streaming Worst-Case Optimal Joins

I got caught up in worst-case optimal mania, and implemented one instance of the GenericJoin worst-case optimal join framework in [a dataflow setting](https://github.com/TimelyDataflow/differential-dataflow/tree/master/dogsdogsdogs).
That work developed and crisped up with collaborators from the University of Waterloo, and resulted in a [VLDB paper on streaming worst-case optimal joins](https://www.vldb.org/pvldb/vol11/p691-ammar.pdf).
The "streaming" modifier just means that the facts of the relations arrive as a stream, over time, rather than existing from the outset.
Nonetheless, we are able to produce (and maintain over time) the results of the join using effort that matches the worst-case optimal bounds as if we had started with the full input datasets.

The algorithm is an instance of GenericJoin, which is a framework for worst-case optimal join algorithms.
It is not the most popular instance, however, which is instead likely [Leapfrog Triejoin](https://arxiv.org/abs/1210.0481).
One can interpret Leapfrog Triejoin as a "depth-first" instantiation of GenericJoin, and the algorithm we implement (I have unhelpfully called it "Treefrog Leapjoin", but the linked paper has other less fun names) is a "breadth-first" instantiation.
Let's sketch the algorithm from the paper, which to be extra clear is just an instance of the GenericJoin framework.

A conjunctive query has some number of "atoms", the mentions of relations in the body of the rule.
In our complicated three-way join above, there are three atoms: `q(A, X, Y)`, `r(X, Z, B)`, and `q(Y, Z, C)`.
To avoid self-inflicted madness, let's give them new placeholder names `R0`, `R1`, and `R2`, because the re-use of `q` isn't going to make it any easier to understand.

Should any of the atoms gain new facts, written potentially as `dR0` for `R0` say (the `d` prefix being the cunning notational novelty), we can *update* the join with the rule:
```
dQ(A, B, C) :- dR0(A, X, Y), R1(X, Z, B), R2(Y, Z, C)
```
That is, each new fact in `dR0` may join with existing facts in `R1` and `R2` to produce a new output fact for `Q`.
We can do this for `dR1` and `dR2`, creating a total of three rules that each contribute new results to `dQ`.
Integrated over all input changes, we end up accounting for all output changes, amounting to the correct output for the query.

Each of these `dR` rules will have a different implementation, based on GenericJoin.
For each `dR` we'll pick an order on the terms, one that starts with the terms bound by `dR`.
For `dR0` for example, we'll start with `A`, `X`, `Y`, and then pick an order on the remainining terms `B`, `C`, and `Z`.
As it turns out, the worst-case optimal bounds hold no matter the order you choose, but different orders will be differently compelling.

For each term in order, we will compute all bindings to the terms so far, when the query atoms are restricted to the terms in scope.
For example, we'll start by determining the values of `A`, `X`, `Y` that occur in `dR0(A, X, Y)`, but also for which `R1(X, _, _)` and `R2(Y, _, _)` exist for some placeholder `_` values.
The trick is in how we move from all of the possible values on the first j terms to all of the possible values on the first j+1 terms.

To go from any j terms to j+1 terms, adding say term `Z` (the hard one in this example), we'll iterate over each of the bindings of the first j terms, asking each atom how many distinct values of the j+1st term they would propose for the current bindings to the first j terms.
In our example, for each triple `(a, x, y)` present in `dR0`, we'll ask:
1. "Hey, `R1`: how many distinct values of `z` do you have satisfying `R1(x, z, _)`?"
2. "Hey, `R2`: how many distinct values of `z` do you have satisfying `R2(y, z, _)`?"

Whichever of the atoms reports the smallest number of distinct values, we'll have them propose each of their values, and then we'll have each of the other atoms validate the proposals.
The important observation, again from GenericJoin not us, is that the proposals take time proportional to the number of distinct values, and the validation can also take time proportional to that number of proposals (e.g. a look-up in a hashset).
We avoid doing work proportional to the larger numbers of proposals, which both generally feels good and is also the condition required for worst-case optimality.

One way to think about this is that rather than choosing a join order (on atoms) statically, for the entire query, we are choosing the order in which to join atoms on a fact-by-fact basis.
What a smart idea!

We repeat this for each term until all terms have been incorporated.
And we repeat the whole thing for each of the `dR` terms, which may need different term orders, because they start from different terms.

The paper has a few additional observations, including the description of how to implement the algorithm across multiple distributed workers, with each worker needing state proportional only to the inputs divided by the number of workers, independent of the output size or skew that may exist in the input.
For our purposes, we only need the streaming worst-case optimal joins, and everything else is just bonus.

## Worst-Case Optimal Datalog

There is a connection between streaming joins and Datalog's iteration.
Each round of derivation in Datalog produces new facts, which should then be "added" to the atoms that make up the body of the rules.
This addition is mechanically similar, in our case identical, to "streaming" in the facts.

If we use the streaming worst-case optimal join as our mechanism to respond to newly derived Datalog facts, we end up with a thing I'll call "worst-case optimal Datalog".
The amount of time taken deriving all of the facts of any Datalog program is at most proportional to the largest number of satisfying bindings of any rule, for adversarially chosen relation contents of the same size.

Importantly, and in my opinion weirdly, the bound is relative to the worst-case instance of the atoms in the query, whose size matches the *output* of the Datalog program.
The *input* relations are all well and good, but the program itself through its iterative nature may expand them dramatically.
We arrive at potentially much larger *output* relations, the accumulation of all newly added / streamed facts, and it is these accumulations that the worst-case optimal bounds are framed in terms of.

So that's what I meant by "worst-case optimal Datalog".
Fingers crossed that it both 1. makes any sense, and 2. is even remotely accurate.
The good news is that independently, the algorithm avoids bad behavior and ends up being the most efficient way I am aware of to solve problems like the medical ontology inferencing task above.

### Light evaluation (GALEN)

The inferencing task, or rather the fragment of the task that I am familiar with, has six rules.
```
p(?x,?z)    :- p(?x,?y), p(?y,?z).
p(?x,?z)    :- p(?y,?w), u(?w,?r,?z), q(?x,?r,?y).
p(?x,?z)    :- c(?y,?w,?z), p(?x,?w), p(?x,?y).
q(?x,?r,?z) :- p(?x,?y), q(?y,?r,?z).
q(?x,?q,?z) :- q(?x,?r,?z), s(?r,?q).
q(?x,?e,?o) :- q(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o).
```
The last rule is the one with the triangle used above, but the second and third rules also have triangle fragments.

The [GALEN](https://github.com/frankmcsherry/dynamic-datalog/tree/master/problems/galen) dataset is one of the inputs you can use to exercise these rules.
If you use a naïve algorithm, one based on binary joins, you have a very good chance of waiting a very long time.
It happens that there are binary join orders you can use for this to be efficient, but you kind of need to try them out to figure out which ones are viable, and which ones are trash.
The worst-case optimal Datalog avoids that problem, and doesn't have the problem of trash plans.

The worst-case optimality doesn't meant that Datalog will run fast, but for `datatoad` on these rules and this dataset, the worst-case optimal implementation now outperforms the best linear join plans I can find.
Not by an epic amount, but by enough that it is different, and the peace of mind in not being one transpostion away from a trash plan that takes 100x as long lets me sleep better.

I stole some numbers from [a preprint of the recent FlowLog paper](https://arxiv.org/pdf/2511.00865), which used the GALEN data and inferencing rules.
I want to stress that I've almost certainly over-fit to this problem and data, so don't get too excited about these numbers other than the obvious light excitement that perhaps there is something good to talk about here.
Their measurements are also on their computers, and mine on my laptop, which are different.

|   system | cores |   time |
| --------:| -----:| ------:|
| FlowLog  |    64 |   8.7s |
| Soufflé  |    64 |  36.8s |
| RecStep  |    64 | 667.9s |
|   DDlog  |    64 |  64.6s |
| datatoad |     1 |  11.9s |

The other systems do various clever things, and it would be wrong to think of the good datatoad times as diminishing their work.
My sincere expectation is that their ideas will likely compose, and e.g. the FlowLog system should be able to adopt the streaming worst-case optimal join algorithm, or at least incorporate it in its list of options (about which it thinks harder than datatoad).

### Other considerations

The performance of datatoad is not exclusively about the worst-case optimal bounds.
It benefits from at least three additional contributing factors, which I'll attempt to unpack in more detail in the near future.

1.  The system is fully columnar, performing single sequential passes over columns each round of derivation.
    For the same reason that array languages can go fast, optimized array kernels, datatoad is able to invoke specialized column kernels without giving up its interpreted evaluation.

2.  The system hybridizes worst-case optimal and conventional binary joins.
    The join plans are sequences of stages that add any number of atoms and terms, including one term and all of its atoms (worst-case optimal), and one atom and all its terms (conventional).
    This is similar in spirit to [FreeJoin](https://arxiv.org/abs/2301.10841), which has a lot of terms in common but I have to admit to not fully understanding yet.
    It may be that FreeJoin is the same thing, only earlier and better.

3.  The approach to evaluation is absolutely abhorrent.
    I just re-run things and write down the most recent time I've seen.
    This introduces a tremendous amount of bias, one imagines toward success and away from failure ("I won't be running *those* queries again").
    Don't believe anyone's numbers until you measure them yourself on your own terms.

There are many next steps, some of which I expect to teach me something and some of which are just tedious.
I'm planning to add in antijoins, on account of you kind of need them in a lot of real logic programs, and they also exercise the generality of the streaming worst-case optimal join algorithm (they fit just fine, having implemented them as part of Treefrog Leapjoin in [datafrog](https://github.com/rust-lang/datafrog)).
I'll likely want to investigate distributed execution, because I have a few computers connected by wires, and I tend to like doing that sort of thing.
I'd really like to generalize the Datalog to an array language, with e.g. `map`, `filter`, `flat_map`, and various reshaping operations; there's a very nice connection to [factorized databases](https://www.cs.ox.ac.uk/dan.olteanu/papers/os-sigrec16.pdf).

All told, what a treat working on getting computers to do things faster, and learning at the same time.
I've picked up a bunch of tricks and insight I hope to transport back to [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow), and from there on to [incrementally maintained SQL](https://materialize.com).
We've even started to see folks showing up with graph-shaped problems and ontologies, where the Datalog itself may prove valuable!
