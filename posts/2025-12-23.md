# (Worst-Case Optimal) Relational Programming

[Relational programming](https://en.wikipedia.org/wiki/Logic_programming#Relationship_with_relational_programming) is (I think) the name given to the idea that you can generalize unidirectional "functions" to multi-directional "relations", and in so doing enlarge the space of behavior a program has access to.

Conventionally, for example, one could use a Datalog-y syntax with a magical `:plus` relation to determine the sums of pairs of input `arguments`, like so.
```
sums(z) :- arguments(x,y), :plus(x,y,z).
```
Unpacking a bit, these programs look for all bindings to the variables that satisfy the relations on the right hand side, and contribute them to the left hand side.
The re-use of `x` and `y` on the RHS means that each pair in `arguments` that also is in `:plus` with some value `z` contributes that value to `sums` on the LHS.
We could then look at `sums` or use it in further logic.

We often think of `:plus` as mapping from `(x,y)` to `z`, which is great in this example because we start with `arguments(x,y)`.
It often goes without saying that this interpretation of `:plus` is "unidirectional": from inputs to outputs.
Of course it is, because that is how functions work.

But the `:plus` *relation* is actually **multi-directional**.
In addition to mapping `(x,y)` to `z`, it can also map `(y,z)` to `x` and map `(x,z)` to `y`.
We could as easily type (if not yet evaluate)
```
diffs(x) :- arguments(y,z), :plus(x,y,z).
```
This describes finding the values `x` such that `x + y = z` for `(y,z)` in the input `arguments`,
or equivalently finding `x = z - y`, but without having to invent a `:minus` relation.

Of course, where we understood how to implement `:plus` for the forward direction, it's now more complicated to explain what `:plus` is and how it works.
It will get a bit more complicated when we stretch to relations that can't always be viewed as functions on subsets of their terms, and might produce multiple values.

My understanding is that [miniKanren](https://minikanren.org/index.html) is a standard reference for relational programming, but I don't get it yet.
We'll build up something that might be subsumed by miniKanren, but in any case will loop back around to [worst-case optimal joins](https://en.wikipedia.org/wiki/Worst-case_optimal_join_algorithm).
We'll emerge with an extended Datalog and evaluation strategy that feels if nothing else much more powerful than Datalog, and we'll have to wait on the relationship to miniKanren.

## (Worst-Case Optimal) Relational Joins

The database community has been working on worst-case optimal joins, a class of strategies for enumerating valid bindings to conjunctive logic programs (lists of relations with common variable names) with the property that they enumerate no more bindings than the program might require for worst-case inputs.

It's a bunch of background to catch up on, but for our purposes there is a fairly simple instance of the algorithm we'll use.

The algorithm builds up a list `bind` of bound values to an increasingly long prefix of variables in the program.
For example, if our variables are `x`, `y`, and `z`, we would build in order
1. the satisfying assignments to `x`, ignoring `y` and `z`.
2. the satisfying assignments to `(x,y)`, ignoring `z`.
3. the satisfying assignments to `(x,y,z)`.

At the end, we have the answer we want.

Each of the steps informs the next, and we'll only need to detail how we go from the bindings to a prefix of variables, to the bindings with one additional variable added.
This is done by asking each atom that mentions the new variable, for each binding how many values of that variable would it contribute.
Then, for each binding the atom that indicates the fewest values enumerates the new values, and each other atom has the option to reject the extended binding.
The result is a list of bindings to one further variable, which satisfies each of the atoms that mentions any of the terms.

This is not how conventional database joins work, which typically go atom-by-atom, producing candidate values for all variables referenced by the atom.

## Atomization

The algorithm above goes binding-by-binding, asking questions of each atom.
For reasons that should become clear, we are going to re-order the loops to put the iteration over bindings in the inner-most loop, and first iterate over atoms.
The new loop structure will go:

```
For each variable in some order,
    For each atom that mentions this variable,
        For each binding of values to the prior variables,
        Count the number of distinct values that extends the binding.
    For each atom that mentions the variable,
        For each binding of values to the prior variables,
        If this atom had the least count, enumerate the new values.
    For each atom that mentions the variable,
        For each binding of values to the prior and new variables,
        If the binding is not in the atom, discard the binding.
```

This re-orientation to atoms is exciting (to me) because it hoists a complicated question, "what do each of these atoms think about this binding" out of the inner-most loop.
For the same reason that SIMD, vectorization, and array languages hoist "control" logic, this approach gives each atom a chance to go to town on a bunch of bindings, without being interrupted.

There is another benefit in the context of "relations" as generalizations of "functions".
By hoisting the atoms to the top, we can more easily do dynamic dispatch on the atoms, choosing among varied implementations.
For example, the atom could be a pile of data, or the atom could be a pile of code, or it could be a mix of the two.
Rather than resolve this variance repeatedly for each binding, we can resolve it just a few times for each variable.

This allows us to generalize the worst-case optimal join planning to any relations that implement an interface supporting counting, proposing, and validating.
They may be backed by data and we are performing look-ups into that data, or they may be backed by code that we are running, or a bit of both (antijoins).
They will all work out equally well, and (done right) the algorithm itself doesn't even need to know which atom is which type.

## Generalization

The abstractions used in `datatoad` ask relations to say progressively more about themselves, moving from rough information used for planning on to specific information about concrete bindings.
Informally, that progression looks like (with alphabetic names to help me remember):
1. **Arity**: The number of columns in the relation.
2. **Bound**: For any subset of these columns, the other columns that could be produced from concrete values of these input columns.
3. **Count**: For any binding to input columns, the number of bindings to an output column.
4. **Delve**: For any binding to input columns, the enumeration of bindings to an output column.

The first two steps in the progression, notably without bindings yet, are used in planning the joins.
They tell us first how to approach the relation, but also whether and how the relation can help us advance through variables.
The sequence of variables we explore will want to follow relations that have opinions about the next values based on prior bindings, rather than just guessing all possible bindings and validating them.

The second two steps in the progression, now with bindings, are used to execute the joins.
From collections of actual bindings, they are the methods needed to extend the bindings with additional values, following the worst-case optimal rubric.
We use them as we would any data-backed relation, to scope, fully enumerate, and validate values for new variables.

We are now at a point where we could create some Rust traits and box them up, concealing the details of the implementation from the rest of the system.
Here are the two traits as they are in `datatoad` at the moment, with some (substantial) simplifying liberties taken:
```rust
/// A type that can be planned as a relation.
pub trait PlanAtom {
    fn arity(&self) -> usize;
    fn bound(&self, terms: &BTreeSet<usize>) -> BTreeSet<usize>;
}

/// A list of data and indexes, or not.
type Arg = Option<(Terms, Vec<usize>)>;

/// A type that can be joined as a relation.
pub trait ExecAtom {
    fn count(&self, columns: &[Arg], output: usize) -> Vec<usize>;
    fn delve(&self, columns: &[Arg], output: usize) -> Lists<Terms>;
}
```

Some of the types, e.g. `Terms` and `Lists`, may be new if you haven't been staring at the code.
When we say `Terms` you can think "list of values", and for `Lists<Terms>` you can think "list of lists of values".
The two amount to `Vec<Vec<u8>>` and `Vec<Vec<Vec<u8>>>` respectively, but each with only a few allocations.

The `PlanAtom` trait looks perhaps as you might expect it, with an `arity` and `bound` function that roughly track what we've described.

The `ExecAtom` trait looks weirder to me, in part because it accepts whole columns of input arguments at a time, and needs to produce whole columns of outputs.
Each `Arg` is either a sequence of concrete values, given by a list of values and indexes into it, or .. it is simply absent.
The methods can be invoked with only subsets of columns, with the intent that we learn about an absent column, but this is communicated column-wise.

Except that I've concealed a bit of grot, generics and lifetimes and such, these are the traits.

## Implementation

We've spent a bit of time previously talking through the implementation for *data*, and I thought instead we'd look at examples from *code*.
That is, relations backed by logic rather that data.

I have another helper trait `Logic` that supports `arity`, `bound`, `count`, and `delve` for single bindings, rather than long lists, with a wrapper type that extends the implementation to batches of bindings.
The wrapper type will inline the `Logic` implementation rather than box it, and then the wrapper itself is boxed as a `PlanAtom` and `ExecAtom`.
It makes it easier to spin up new functions, though you don't get the best performance (you can, but with direct implementations).

Let's start with an easy relation: "not equal".
```rust
/// The relation R(x,y) : x != y.
pub struct NotEq;
impl super::Logic for NotEq {
```

The arity of the relation is two, although we could generalize it if we wanted to support more than two terms.
```rust
    fn arity(&self) -> usize { 2 }
```

The relation is never able to propose values of new columns from values of other columns, as there are an unbounded number of values not equal to any one value.
We don't want the planner to rely on this relation to propose new values, so let's avoid inviting that.
```rust
    fn bound(&self, args: &BTreeSet<usize>) -> BTreeSet<usize> {
        Default::default()
    }
```

The `count` method needs to return `Some(_)` values when all inputs are bound, and is allowed to return `None` when the only bound inputs are ones that the type did not advertise as inputs through its `bound` method.
In our case, when both values are bound we should indicate that the counts are either 0 or 1, when the values are the same or different, respectively.
```rust
    fn count(
        &self,
        args: &[Option<<Terms as columnar::Container>::Ref<'_>>],
        output: usize,
    ) -> Option<usize> {
        match (&args[0], &args[1]) {
            (Some(x), Some(y)) => { if x.as_slice() == y.as_slice() { Some(0) } else { Some(1) } },
            _ => None,
        }
    }
```

The `delve` method should never be invoked, as the relation should never "win" the right to enumerate.
If there's a term that the planner cannot find a way to enumerate, the plan should fail rather than reach this code.
The signature re-uses a `&mut Terms` rather than allocate to produce an output, for reasons of performance.
```rust
    fn delve(
        &self,
        args: &[Option<<Terms as columnar::Container>::Ref<'_>>],
        output: (usize, &mut Terms)
    ) { unimplemented!() }
}
```

I think this is the whole of `NotEq`, which seems like it might actually work.
It's all rather new still, so there are some rough edges and planning oversights that manifest oddly.

### More relations

I started with a `Range(x,y,z)` relation for `x <= y < z`, and we'll revisit it in the evaluation section.
This one had some appeal as a data generator, but also there's imo an interesting story about how it unifies two syntactically different SQL idioms.
This relation is interesting in that you can go from `(x,z)` to a list of `y`, but not much else.
There is the option to reject `(x,y)` or `(y,z)` that are already mis-ordered, and potentially to enumerate `x` and `y` for small `z` (these are unsigned integers).
Several options to explore "surprising" emergent behavior.

I also added `Plus(x,y,z)` and `Times(x,y,z)` mostly because the FlowLog evaluation calls for them.
Also, just adding random functions is a great way to explore what is hard, and try out new ideas.
For example, given `z` the `Times` relation can only propose `sqrt(z)+1` pairs `(x,y)`, or .. maybe twice that.
Perhaps that is small enough to offer up as an option.

Finally, I added a `Print(..)` variadic relation that contains all tuples, but needs to be asked.
As part of being asked, it prints the tuple, allowing us to see it.
I'm not sure that this is the right way to approach relational programming, but again very easy to try, so that was nice.
The whole of `Print` is ten lines implementing `Logic`.

## Evaluation

I have a little story behind the evaluation that I liked, but when I started with it it just makes me sound crazy.
No motivation, no connection to anything else I'm doing, what a mess.
But now with the context from above it may make some sense, and we can discuss whether it should have been the first thing in the post after all.

Imagine you have a fleet of sensors, identified somehow.
Each sensor has a collection of measuruments, say `data(key, value)`.
Each sensor also has a set of ranges of values that should prompt a warning, say `warn(key, lower, upper)`.
We're interested in sensor measurements inside a range:
```
alert(key, value) :-
    data(key, value),
    warn(key, lower, upper),
    :range(lower, value, upper).
```
Unfortunately `datatoad` doesn't support multiline Datalog yet, but it's easier to read huh?

Let's say you wanted to do the same thing in SQL.
There are two ways I could think to do this, that work well in different regimes.

The first joins `data` and `warn` by key, and then filters the results.
I would guess this is the most common way to write this.
```sql
SELECT key, value
FROM data, warn
WHERE data.key = warn.key
  AND warn.lower <= data.value
  AND data.value <  warn.upper;
```

A second approach is to use `generate_series` on `warn` to produce a range of values that might be of interest, and then join with `data`.
```sql
SELECT key, value
FROM data
WHERE value IN (
    SELECT generate_series(lower, upper-1)
    FROM warn
    WHERE warn.key = data.key
);
```
Pardon my SQL if this isn't grammatical; I can never hold these table-valued functions correctly.

The first approach is great when you have wide ranges and not a lot of data; you just look at each row in `data` and do a quick test.
The second approach is great when you have narrow ranges and a lot of data; you find the targets and then directly look them up.

Unfortunately, if the data are a 50-50 mix of these two cases, sensors with lots of data and narrow ranges, and sensors with little data and wide ranges, neither query is great.
Or rather, neither of the implied evaluation plans that use `<=` and `<` as predicates, and `generate_series` as a table-valued function.

But, we know know how to use `:range` as a *relation*, which can move bindings in various directions.
It can articulate how expensive it would be to enumerate `value` from given `(lower,upper)`, and either do that or act as a predicate if we draw values from `data` instead.

Let's start by populating `data` with one noisy sensor and one chill sensor:
```
.note two sensors, with ranges of data values: (key, lower, upper).
args(0x01, 0x00000037, 0x05000000) :- .
args(0x02, 0xDEADBEEE, 0xDEADBEEF) :- .

data(key, val) :- args(key, lower, upper), :range(lower, val, upper).
```
The result is that `data` has 83,886,026 facts, all but one for sensor `0x01`.
It takes about three seconds to generate the data, which could probably be greatly improved.

Let's similarly put together `warn` but with narrow and wide bands for the noisy and chill sensors.
```
.note two sensors, with ranges of query values.
warn(0x01, 0x000000EF, 0x000000FF) :- .
warn(0x02, 0x00000000, 0xFFFFFFFF) :- .
```
The first constraint contains only sixteen values, and the second is any eight-byte pattern at all.

Let's now run our query bracketed by timing operations.
```
.time reset
alert(key, val) :- warn(key, lower, upper), data(key, val), :range(lower, val, upper) .
.time done
```
We get out the other end
```
> time: 77.875Âµs        ["done"]
```
Pretty prompt, and as far as I can tell something that a conventional SQL engine might trip over itself trying to do.
Let's look at the output, because we wrote that also:
```
foo(key, val) :- alert(key, val), :print(key, val).
```
This does nothing but push `alert` through `:print`, which produces
```
0x01    0x000000ef
0x01    0x000000f0
0x01    0x000000f1
0x01    0x000000f2
0x01    0x000000f3
0x01    0x000000f4
0x01    0x000000f5
0x01    0x000000f6
0x01    0x000000f7
0x01    0x000000f8
0x01    0x000000f9
0x01    0x000000fa
0x01    0x000000fb
0x01    0x000000fc
0x01    0x000000fd
0x01    0x000000fe
0x02    0xdeadbeee
```
which do seem to be the seventeen measurements we were looking for.
TBD on how you would preferred to have written this; perhaps with the `:print` on the left-hand side.

### PostgreSQL

I did try this with PostgreSQL, not the snappiest of SQL engines but one that I have on my laptop.
It didn't get off to a great start:
```
mcsherry=# create table data(key int, value int);
CREATE TABLE
Time: 4.899 ms
mcsherry=# insert into data select 1, generate_series(1, 83000000);
INSERT 0 83000000
Time: 129929.464 ms (02:09.929)
mcsherry=#
```
It did pick up on the query evaluation though, where the "standard" query took ~11s
```
mcsherry=# SELECT data.key, value
FROM data, warn
WHERE data.key = warn.key
  AND warn.lower <= data.value
  AND data.value <  warn.upper;
 key | value
-----+-------
   1 |    16
   1 |    17
   1 |    18
   1 |    19
   1 |    20
   1 |    21
   1 |    22
   1 |    23
   1 |    24
   1 |    25
   1 |    26
   1 |    27
   1 |    28
   1 |    29
   1 |    30
   1 |    31
   2 |    10
(17 rows)

Time: 11778.170 ms (00:11.778)
mcsherry=#
```
I tried the `generate_series` form of the query with a much narrower band (1M rather than 4B) because .. time.
```
mcsherry=# SELECT key, value
FROM data
WHERE value IN (
    SELECT generate_series(lower, upper-1)
    FROM warn
    WHERE warn.key = data.key
);
 key | value
-----+-------
   1 |    16
   1 |    17
   1 |    18
   1 |    19
   1 |    20
   1 |    21
   1 |    22
   1 |    23
   1 |    24
   1 |    25
   1 |    26
   1 |    27
   1 |    28
   1 |    29
   1 |    30
   1 |    31
   2 |    10
(17 rows)

Time: 80856.840 ms (01:20.857)
mcsherry=#
```

So, not neccesarily the only SQL database out there, but at least a hint that this "relational programming" stuff isn't trivially done by all databases already.
If you know that it is done better by any specific databases, let me know!

## Conclusion

I arrived at this relational programming stuff through the weird route of generalizing columnar worst-case optimal joins to logic-backed relations.
It may be that there's already a whole bunch of people here, in which case .. welcome to the present, Frank.
But in any case, it feels like a really rich opportunity to combine the superpowers of relational joins (to align data) with array languages (performance, extensibility), all in the framing of this neat relational programming idiom.

It also feels like this moves back into the space of program analysis, e-graphs, and fun things like this.
With more "logic" expressed in a consistent framework, there are more opportunities for manipulation, optimization, and generally being clever.
This contrasts with frameworks (e.g. Materialize) that currently have strong distinctions between "relational" and "scalar" expressions;
perhaps they shouldn't, and various SQL "functions" should be unpicked into "relations" to integrate better with a more holistic join planner.

I should also go read about miniKanren next, because there's likely a fair bit to learn.
