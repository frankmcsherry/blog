# Tracking motifs in changing graphs

Imagine you are the sort of person who gets to sit and watch high-throughput streams of social signals fly past. There is probably lots of really interesting information here, but how can you find it? Maybe you've realized that PageRank is kinda silly, and what you want to find are instances of surprising, non-trivial structure in your graphs.

**In this post we are going to look at the problem of finding and tracking *[motifs](https://en.wikipedia.org/wiki/Network_motif)* in graphs that change.**

A "motif" is a small connected subgraph whose nodes are variables. For example, the "triangle motif" is a graph that looks like so:

	(x0, x1), (x1, x2), (x0, x2)

A motif "exists" in a larger graph whenever we can assign actual node identifiers to the node variables so that all edges identified by the motif exist in the graph. There is some debate about whether the non-edges should non-exist; we'll talk about both. Each assignment of these variables is an instance of the motif.

Returning to the example of a triangle, we will write it the way we might write a Datalog rule, announcing some variables on one side and what needs to be true about the relationships between them on the other.

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

A triangle is a small graph, but we are interested in the sets of assignments to `x0`, `x1`, and `x2` so that all three edges exist. There could be *lots* of these in a larger graph, and they chould also change substantially as we add and remove edges. It would be great to have all of them brought to our attention. 

Perhaps you aren't as interested in abstract "triangles", because seriously who cares? In which case, the motif above applied to a directed graph is an instance of a [feed-forward loop](https://en.wikipedia.org/wiki/Network_motif#Feed-forward_loops_.28FFL.29), where signal from `x0` gets to `x2` both directly and through `x1`. There are lots of other motifs that are plausibly interesting, a subject on which I have only limited credibility (determining motifs which are meaningful, if any, seems to be open research).

For the busy people among you, we are going to end up describing **a data-parallel system that tracks all occurrences of arbitrary motifs in arbitrarily changing graphs, with (i) low latency, (ii) worst-case optimal throughput, (iii) a memory footprint linear in the size of the graph.** I'm also experimenting with bolding important things, rather than writing paragraphs explaining them.

This is all joint work with [Khaled Ammar](https://khaledammar.com) and [Semih Salihoglu](https://cs.uwaterloo.ca/~ssalihog/), both at Waterloo. It is an instance of a more general approach to efficiently maintaining relational algebra queries over continually updating relations, if you prefer to think of things that way. We are writing it up now, and experimenting with the question of "what happens to academics when they tell people what they are working on before they publish it". 

## An outline

There are some steps to follow to get where we need to go, so we will do this in parts. 

1. [Motifs in graphs. What are they, and how might we compute them?](https://github.com/frankmcsherry/blog/blob/master/posts/2016-09-17.md#1-motifs-in-graphs)
2. [Worst-case optimal join computation; the coolest thing to happen to joins in 40 years.]()
3. [Streaming implementations of worst-case optimal join processing on static data.]()
4. [Streaming implementations of worst-case optimal join processing on dynamic data.]()
5. [Evaluating this; show me some numbers!]()

I'm going to use the [`livejournal`](http://snap.stanford.edu/data/soc-LiveJournal1.html) graph dataset for most of the experiments, so if you want to grab that and follow along, sweet. I'll also report some statistics for the [`twitter_rv`](https://an.kaist.ac.kr/~hosung/papers/2010-www-twitter.pdf) graph dataset, though mainly to demonstrate the asymptotic difference between various algorithms, as it is just too painful to run the slower algorithms on this larger (and more skewed) dataset.

I'm going to use the directed version of these graph, and all of the work we will develop apply to motifs in directed graphs. This means that "triangles"

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

are really the feed-forward loops, because the edges have directions and we need to respect them. In this setting, a feed-forward loop is totally different from a directed 3-cycle (here written oddly),

	3cycle(x0, x1, x2) := edge(x0, x1), edge(x2, x0), edge(x1, x2)

Independently, there are a parade of tricks one can apply to motifs in undirected graphs, from only representing the edges once to observing symmetries in the motif to re-labelling graph identifiers to smooth out some issue we will run into with directed motifs. Our take is that you can and should (i) develop general infrastructure for directed graphs and relations generally, and (ii) then apply your collection of cute tricks. This isn't going to be the place to learn about the absolute best way to determine triangles in an undirected graph, but it will be pretty good nonetheless.

## 1. Motifs in graphs.

One way to look for meaning in graphs is to look for interesting repeated structural patterns.

Triangles are an interesting subgraph because they appear relatively infrequently by chance. In a graph with *m* edges placed randomly among *n* nodes, there are *n^3* possible directed triangles each of which has a (roughly) *(m/n^2)^3* chance of existing. This gives us roughly *(m/n)^3* directed triangles, in expectation. In real graphs triangles appear much more often than this and act as signals of structure, places where things are not completely random.

More generally, specific subgraphs on *k* vertices with at least *k* edges are unlikely to appear by chance. When they do occur, it is usually a sign that something interesting is going on in the graph. One can project various semantic interpretations of each of these subgraphs, but several lines of work are interested in tracking the arrival and departure of these subgraphs, or "motifs" as we will call them here, as graph edges arrive and depart.

There are several ways to go about finding motifs in a graph, so let's start with the simplest and get progressively smarter.

### Enumerate everything

From a graph on *n* nodes, we could just enumerate all *n* choose *k* possible assignments of nodes identifiers to the *k* variables, and see if the corresponding edges exist. Even for small *n* this could be an enormous amount of work. We want to compute the answer that this would compute, we just don't want to compute it this way.

Concretely, the `livejournal` graph has some 4.8 million vertices, in which the number of possible directed triangles is

	4.8m^3 = 1.10592x10^20

The `twitter_rv` graph has some 42 million vertices, and so has about 670x as many possible directed triangles as does `livejournal`.

Explicitly enumerating these possibilities and checking them is not reasonable, and we aren't going to implement and evaluate that for obvious reasons. It is nonetheless good to understand that what we want *can* be computed, and we just need to optimize the implementation.

Tracking changes explicitly might be easier: if an edge `(x0, x1)` changes (i.e. arrives or departs), two of the variables are already bound and we only need to look at all of the `x2`. There are as many settings of `x2` as nodes in the graph, which is still a lot but less crazy than when we had to cube that number. However, if we go to motifs on more variables the craziness returns. Let's hold off on this for now.

### Relational joins

One way to represent the `triangle` motif is by a relational query, representing the set of edges by a relation `edge(x,y)` and joining three instances of the `edge` relation. We've actually been writing things this way already.

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

This notation says that if we have a relation (set) of pairs `edge(x0, x1)` then we can define `triangles` by the triples `(x0, x1, x2)` so that each of the corresponding pairs exists in `edge`.  

More generally, if we have a motif on *k* variables with edges `(xi, xj)` we can reframe the motif as a relational query (like, SQL) where we use the `edge` relation once for each edge we require between variables. 

	motif(x0, x1, ..) := edge(xi0, xj0), edge(xi1, xj0), ..

This may seem like we've just re-though how we write things down, but once we write the motifs as relational queries over an `edge` relation, we can evaluate each query as if we were a vanilla database system. 

The typical approach to evaluating relational joins is to cerate a tree of binary joins, where the leaves are relations in the join and each internal node corresponds to the relational join of whatever relations are produced by its two children. This approach has a certain simplicity, and has been the standard way that databases have resolved joins for quite some time. If your motif doesn't have any cycles (i.e. has *k-1* edges on *k* variables), you can even [implement this optimally](https://en.wikipedia.org/wiki/Mihalis_Yannakakis#cite_note-8), in the sense that the amount of work done is at worst linear in the size of your input (number of edges) plus the output (number of motif instances found). 

At the same time, if your motif has cycles (and the most non-trivial ones will) this approach can be much worse. Let's look at the triangles example, which uses the `edge` relation three times, and choose as the first join to perform:

	candidates(x0, x1, x2) := edge(x0, x1), edge(x0, x2)

Our next step will join `candidates(x0, x1, x2)` with `edge(x1, x2)`, an intersection, but as part of doing this we end up enumerating at least the contents of `candidates`; how big could it possibly be?

Each graph node `x0` contributes each pair of its out-neighbors to `candidates`. The maximum degrees in `livejournal` and `twitter_rv` are 

	max_deg(livejournal) = 20,293
	max_deg(twitter_rv)  = 2,997,469

which mean that they could produce lots of candidates, especially `twitter_rv`. In fact we can just evaluate the size of `candidates` for each by summing the squares of out-degrees, giving

	candidates(livejournal) = 7,292,467,251
	candidates(twitter_rv)  = 243,932,072,881,466

The `livejournal` graph has quite a few candidates, but I can't even remember how to pronounce how many candidates `twitter_rv` has.

Of pedagogical import, there are more candidates in `twitter_rv` than could possibly exist with its 1,468,365,182 edges. If we used all of those edges to make a complete directed graph on `sqrt(1.5b)` nodes (we are a few edges short), we would have only as many directed triangles as the cube of that, or

	tri_max(1.5b) = 58,094,750,193,111

So, we are looking at at least 4x as many candidates as there could possibly be triangles in an adversarially arranged graph. We can probably do better than this (and we will, because others before us already have).

Let's write up some code to go and compute the number of triangles using a standard relational join. We are only going to run it on the `livejournal` graph, for obvious reasons. Rather than actually materialize the seven billion triples `(x0, x1, x2)`, we will enumerate them and check each to see if the edge `(x1, x2)` exists.

```rust
let mut triangles = 0;
for x0 in 0 .. graph.nodes() {
	for x1 in graph.edges(x0) {
		for x2 in graph.edges(x0)
			// check if edge (x1, x2) exists.
			if graph.edges(x1).contains(x2) {
				triangles += 1;
			}
		}
	}
}
```

The `contains` method could be written several ways. If the data were hashsets, we would just do a hash lookup. In my code, edges are stored as a sorted list of identifiers, so it would be binary search. If we were smarter, we could note that the `x2` are themselves sorted, and that means that we could probably do a bulk `intersect` a bit faster. Hold that thought for a moment.

	triangles: 946400853 in: Duration { secs: 41, nanos: 538378761 }

This is ... a time. Cool. It's actually not even a bad time, relative to the smarter approaches we are going to look at next.

If we were to run this computation on the `twitter_rv` graph, imagining we could enumerate and check edges at the same rate (say, 200m candidates / sec) it would take only 1219660.36s, roughly 338 hours or just over two weeks.

So, this vanilla approach using relational joins might work ok on smaller graphs, but probably isn't so great on some larger graphs.

### Being smarter about triangles

A certain smartness (who's lineage I don't know) has developed for computing triangles, which can substantially improve on the performance of the approach we discussed above. There are a few ways to think about the smartness, but the one I like most (for pedagogical reasons) starts by re-writing our code above to use `intersect` rather than `contains`:

```rust
let mut triangles = 0;
for x0 in 0 .. graph.nodes() {
	for x1 in graph.edges(x0) {
		// check all x2 at once ...
		let edges0 = graph.edges(x0);
		let edges1 = graph.edges(x1);
		triangles += intersect(edges0, edges1);
	}
}
```

How is this different from the version above? It depends on how we implement `intersect`. Here is one implementation that you might have though of, paralleling the code we wrote above.

```rust
// counts number of element of `a` also in `b`.
fn intersect1(a: &[u32], b: &[u32]) -> usize {
	let mut count = 0;
	for x in a {
		if b.contains(x) { count += 1; }
	}
	count
}
```

This `intersect1` corresponds to the query plan where we first join `edge(x0, x1)` with `edge(x0, x2)` and then with `edge(x1, x2)`, as the candidates `x2` come from their relationship to `x0`. It takes time linear in the size of `a`, and if `contains` is fast then roughly independent of the size of `b`.

Here is another pretty similar implementation, nonetheless different:

```rust
// counts number of element of `b` also in `a`.
fn intersect2(a: &[u32], b: &[u32]) -> usize {
	let mut count = 0;
	for x in b {
		if a.contains(x) { count += 1; }
	}
	count
}
```

This `intersect2` corresponds to the query plan where we first join `edge(x0, x1)` with `edge(x1, x2)` and then with `edge(x0, x2)`, as the candidates `x2` come from their relationship to `x1`. It takes time linear in the size of `b`, and if `contains` is fast then roughly independent of the size of `a`.

These two implementations vary only in which argument supplies the candidates and which filters the candidates, just like the two join orders we discussed. Each implementation takes an amount of time that depends mainly on how long their first (`intersect1`) or second (`intersect2`) arguments are. So which one should we use?

I mean, we don't really have to choose do we? We can just use the right one each time:

```rust
fn intersect(a: &[u32], b: &[u32]) -> usize {
	if a.len() < b.len() {
		intersect1(a, b);
	}
	else {
		intersect2(a, b);
	}
}
```

It turns out that this obviously hack-y change does something amazing. Using this "optimized" `intersect` takes the worst-case running time of the triangle finding computation down from `O(edges^2)` to `O(edges^{3/2})`. Boom, theory happened.

Ok slow down. What actually happened here? 

Our new implementation no longer corresponds to a standard implementation of a relational join, in which we would have to pick two relations to pair up. Rather, we are choosing which relation proposes candidates and which validates them *on a record-by-record basis*. That is a pretty big departure from the standard relational approach, and it makes all the difference.

Let's run our modified implementation on `livejournal`

	triangles: 946400853 in: Duration { secs: 40, nanos: 173807791 }

Compared to the run above this is ... one second faster. Not exactly the revolution we were promised. Let's dive a bit deeper and see what happened.

The first implementation considers each pair `(x0, x1)` and for each proposes `degree(x0)` candidates, which we computed up above as the sum of squared degrees. Our second implementation considers each pair `(x0, x1)` and for each proposes `min(degree(x0), degree(x1))` candidates, which we can also compute:

	candidates_1(livejournal) = 7,292,467,251
	candidates_2(livejournal) = 3,251,016,553

This doesn't seem to be a massive reduction, and we've made the code a bit more complicated so perhaps .. sucks. Let's look at the numbers for `twitter_rv`

	candidates_1(twitter_rv) = 243,932,072,881,466 
	candidates_2(twitter_rv) = 754,462,139,483

That is a much more substantial difference. Indeed, our estimated time of two weeks for the first approach actually runs in just about three and a half hours using the second approach. 

	triangles: 143011093363 in: Duration { secs: 12977, nanos: 592176810 }

The out-degrees of the `livejournal` graph just aren't as skewed as the `twitter_rv` graph, so smartly choosing between proposers doesn't help much (or at all, really). On the other hand, smartly choosing is pretty much mandatory for the `twitter_rv` graph.

Ok, cute hack. But does it apply to anything other than triangles? It does.

## 2. Worst-case optimal relational joins.

## 3. Streaming, data-parallel on static data.

## 4. Streaming, data-parallel on dynamic data.

## 5. Evaluating this.