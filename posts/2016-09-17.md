# Tracking motifs in changing graphs

Imagine you are the sort of person who gets to sit and watch high-throughput streams of social signals fly past. There is probably lots of really interesting information here, but how can you find it? Maybe you've realized that PageRank is kinda silly, and what you want to find are instances of surprising, non-trivial structure in your graphs.

**In this post we are going to look at the problem of finding and tracking *[motifs](https://en.wikipedia.org/wiki/Network_motif)* in graphs that change.**

A "motif" is a small connected subgraph whose nodes are variables. For example, the "triangle motif" is a graph that looks like so:

	(x0, x1), (x1, x2), (x0, x2)

A motif "exists" in a larger graph whenever we can assign actual node identifiers to the node variables so that all edges identified by the motif exist in the graph. There is some debate about whether the non-edges should non-exist; we'll talk about both. Each assignment of these variables is an instance of the motif.

Returning to the example of a triangle, we will write it the way we might write a Datalog rule, announcing some variables on one side and what needs to be true about the relationships between them on the other.

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

A triangle is a small graph, but we are interested in the sets of assignments to `x0`, `x1`, and `x2` so that all three edges exist. There could be *lots* of these in a larger graph, and they chould also change substantially as we add and remove edges. It would be great to have all of them brought to our attention. 

Perhaps you aren't as interested in abstract "triangles", because seriously who cares? In which case, the motif above applied to a directed graph is an instance of a [feed-forward loop](https://en.wikipedia.org/wiki/Network_motif#Feed-forward_loops_.28FFL.29), where signal from `x0` gets to `x2` both directly and through `x1`. There are lots of other motifs that are plausibly interesting, a subject on which I have only limited credibility (determining motifs which are meaningful, if any, seems to be open research).

For the busy people among you, we are going to end up describing **a data-parallel system that tracks all occurrences of arbitrary motifs in arbitrarily changing graphs, with (i) low latency, (ii) worst-case optimal throughput, (iii) a memory footprint linear in the size of the graph.** I'm also experimenting with bolding important things, rather than writing paragraphs explaining them.

This is all joint work with [Khaled Ammar](https://khaledammar.com) and [Semih Salihoglu](https://cs.uwaterloo.ca/~ssalihog/), both at Waterloo. It is an instance of a more general approach to efficiently maintaining relational algebra queries over continually updating relations, if you prefer to think of things that way. We are writing it up now, and experimenting with the question of "what happens to academics when they tell people what they are working on before they publish it". 

## An outline

There are some steps to follow to get where we need to go, so we will do this in parts. 

1. [Motifs in graphs. What are they, and how might we compute them?](https://github.com/frankmcsherry/blog/blob/master/posts/2016-09-17.md#1-motifs-in-graphs)
2. [Worst-case optimal join computation; the coolest thing to happen to joins in 40 years.](https://github.com/frankmcsherry/blog/blob/master/posts/2016-09-17.md#2-worst-case-optimal-relational-joins)
3. [Streaming implementations of worst-case optimal join processing on static data.]()
4. [Streaming implementations of worst-case optimal join processing on dynamic data.]()
5. [Evaluating this; show me some numbers!]()

I'm going to use the [`livejournal`](http://snap.stanford.edu/data/soc-LiveJournal1.html) graph dataset for most of the experiments, so if you want to grab that and follow along, sweet. I'll also report some statistics for the [`twitter_rv`](https://an.kaist.ac.kr/~hosung/papers/2010-www-twitter.pdf) graph dataset, though mainly to demonstrate the asymptotic difference between various algorithms, as it is just too painful to run the slower algorithms on this larger (and more skewed) dataset.

I'm going to use the directed version of these graph, and all of the work we will develop apply to motifs in directed graphs. This means that "triangles"

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

are really the feed-forward loops, because the edges have directions and we need to respect them. In this setting, a feed-forward loop is totally different from a directed 3-cycle (here written oddly),

	3cycle(x0, x1, x2) := edge(x0, x1), edge(x2, x0), edge(x1, x2)

Independently, there are a parade of tricks one can apply to motifs in undirected graphs, from only representing the edges once to observing symmetries in the motif to re-labelling graph identifiers to smooth out some issue we will run into with directed motifs. Our take is that you can and should (i) develop general infrastructure for directed graphs and relations generally, and (ii) then apply your collection of cute tricks. This isn't going to be the place to learn about the absolute best way to determine triangles in an undirected graph, but it will be pretty good nonetheless.

## 1. Motifs in graphs.

One way to look for meaning in graphs is to look for interesting repeated structural patterns.

Triangles are an interesting subgraph because they appear relatively infrequently by chance. In a graph with *m* edges placed randomly among *n* nodes, there are *n^3* possible directed triangles each of which has a (roughly) *(m/n^2)^3* chance of existing. This gives us roughly *(m/n)^3* directed triangles, in expectation. In real graphs triangles appear much more often than this and act as signals of structure, places where things are not completely random.

More generally, specific subgraphs on *k* vertices with at least *k* edges are unlikely to appear by chance. When they do occur, it is usually a sign that something interesting is going on in the graph. One can project various semantic interpretations of each of these subgraphs, but several lines of work are interested in tracking the arrival and departure of these subgraphs, or "motifs" as we will call them here, as graph edges arrive and depart.

There are several ways to go about finding motifs in a graph, so let's start with the simplest and get progressively smarter.

### Enumerate everything

From a graph on *n* nodes, we could just enumerate all *n* choose *k* possible assignments of nodes identifiers to the *k* variables, and see if the corresponding edges exist. Even for small *n* this could be an enormous amount of work. We want to compute the answer that this would compute, we just don't want to compute it this way.

Concretely, the `livejournal` graph has some 4.8 million vertices, in which the number of possible directed triangles is

	4.8m^3 = 1.10592x10^20

The `twitter_rv` graph has some 42 million vertices, and so has about 670x as many possible directed triangles as does `livejournal`.

Explicitly enumerating these possibilities and checking them is not reasonable, and we aren't going to implement and evaluate that for obvious reasons. It is nonetheless good to understand that what we want *can* be computed, and we just need to optimize the implementation.

Tracking changes explicitly might be easier: if an edge `(x0, x1)` changes (i.e. arrives or departs), two of the variables are already bound and we only need to look at all of the `x2`. There are as many settings of `x2` as nodes in the graph, which is still a lot but less crazy than when we had to cube that number. However, if we go to motifs on more variables the craziness returns. Let's hold off on this for now.

### Relational joins

One way to represent the `triangle` motif is by a relational query, representing the set of edges by a relation `edge(x,y)` and joining three instances of the `edge` relation. We've actually been writing things this way already.

	triangle(x0, x1, x2) := edge(x0, x1), edge(x0, x2), edge(x1, x2)

This notation says that if we have a relation (set) of pairs `edge(x0, x1)` then we can define `triangles` by the triples `(x0, x1, x2)` so that each of the corresponding pairs exists in `edge`.  

More generally, if we have a motif on *k* variables with edges `(xi, xj)` we can reframe the motif as a relational query (like, SQL) where we use the `edge` relation once for each edge we require between variables. 

	motif(x0, x1, ..) := edge(xi0, xj0), edge(xi1, xj0), ..

This may seem like we've just re-though how we write things down, but once we write the motifs as relational queries over an `edge` relation, we can evaluate each query as if we were a vanilla database system. 

The typical approach to evaluating relational joins is to cerate a tree of binary joins, where the leaves are relations in the join and each internal node corresponds to the relational join of whatever relations are produced by its two children. This approach has a certain simplicity, and has been the standard way that databases have resolved joins for quite some time. If your motif doesn't have any cycles (i.e. has *k-1* edges on *k* variables), you can even [implement this optimally](https://en.wikipedia.org/wiki/Mihalis_Yannakakis#cite_note-8), in the sense that the amount of work done is at worst linear in the size of your input (number of edges) plus the output (number of motif instances found). 

At the same time, if your motif has cycles (and the most non-trivial ones will) this approach can be much worse. Let's look at the triangles example, which uses the `edge` relation three times, and choose as the first join to perform:

	candidates(x0, x1, x2) := edge(x0, x1), edge(x0, x2)

Our next step will join `candidates(x0, x1, x2)` with `edge(x1, x2)`, an intersection, but as part of doing this we end up enumerating at least the contents of `candidates`; how big could it possibly be?

Each graph node `x0` contributes each pair of its out-neighbors to `candidates`. The maximum degrees in `livejournal` and `twitter_rv` are 

	max_deg(livejournal) = 20,293
	max_deg(twitter_rv)  = 2,997,469

which mean that they could produce lots of candidates, especially `twitter_rv`. In fact we can just evaluate the size of `candidates` for each by summing the squares of out-degrees, giving

	candidates(livejournal) = 7,292,467,251
	candidates(twitter_rv)  = 243,932,072,881,466

The `livejournal` graph has quite a few candidates, but I can't even remember how to pronounce how many candidates `twitter_rv` has.

Of pedagogical import, there are more candidates in `twitter_rv` than could possibly exist with its 1,468,365,182 edges. If we used all of those edges to make a complete directed graph on `sqrt(1.5b)` nodes (we are a few edges short), we would have only as many directed triangles as the cube of that, or

	tri_max(1.5b) = 58,094,750,193,111

So, we are looking at at least 4x as many candidates as there could possibly be triangles in an adversarially arranged graph. We can probably do better than this (and we will, because others before us already have).

Let's write up some code to go and compute the number of triangles using a standard relational join. We are only going to run it on the `livejournal` graph, for obvious reasons. Rather than actually materialize the seven billion triples `(x0, x1, x2)`, we will enumerate them and check each to see if the edge `(x1, x2)` exists.

```rust
let mut triangles = 0;
for x0 in 0 .. graph.nodes() {
	for x1 in graph.edges(x0) {
		for x2 in graph.edges(x0)
			// check if edge (x1, x2) exists.
			if graph.edges(x1).contains(x2) {
				triangles += 1;
			}
		}
	}
}
```

The `contains` method could be written several ways. If the data were hashsets, we would just do a hash lookup. In my code, edges are stored as a sorted list of identifiers, so it would be binary search. If we were smarter, we could note that the `x2` are themselves sorted, and that means that we could probably do a bulk `intersect` a bit faster. Hold that thought for a moment.

	triangles: 946400853 in: Duration { secs: 41, nanos: 538378761 }

This is ... a time. Cool. It's actually not even a bad time, relative to the smarter approaches we are going to look at next.

If we were to run this computation on the `twitter_rv` graph, imagining we could enumerate and check edges at the same rate (say, 200m candidates / sec) it would take only 1219660.36s, roughly 338 hours or just over two weeks.

So, this vanilla approach using relational joins might work ok on smaller graphs, but probably isn't so great on some larger graphs.

### Being smarter about triangles

A certain smartness (who's lineage I don't know) has developed for computing triangles, which can substantially improve on the performance of the approach we discussed above. There are a few ways to think about the smartness, but the one I like most (for pedagogical reasons) starts by re-writing our code above to use `intersect` rather than `contains`:

```rust
let mut triangles = 0;
for x0 in 0 .. graph.nodes() {
	for x1 in graph.edges(x0) {
		// check all x2 at once ...
		let edges0 = graph.edges(x0);
		let edges1 = graph.edges(x1);
		triangles += intersect(edges0, edges1);
	}
}
```

How is this different from the version above? It depends on how we implement `intersect`. Here is one implementation that you might have though of, paralleling the code we wrote above.

```rust
// counts number of element of `a` also in `b`.
fn intersect1(a: &[u32], b: &[u32]) -> usize {
	let mut count = 0;
	for x in a {
		if b.contains(x) { count += 1; }
	}
	count
}
```

This `intersect1` corresponds to the query plan where we first join `edge(x0, x1)` with `edge(x0, x2)` and then with `edge(x1, x2)`, as the candidates `x2` come from their relationship to `x0`. It takes time linear in the size of `a`, and if `contains` is fast then roughly independent of the size of `b`.

Here is another pretty similar implementation, nonetheless different:

```rust
// counts number of element of `b` also in `a`.
fn intersect2(a: &[u32], b: &[u32]) -> usize {
	let mut count = 0;
	for x in b {
		if a.contains(x) { count += 1; }
	}
	count
}
```

This `intersect2` corresponds to the query plan where we first join `edge(x0, x1)` with `edge(x1, x2)` and then with `edge(x0, x2)`, as the candidates `x2` come from their relationship to `x1`. It takes time linear in the size of `b`, and if `contains` is fast then roughly independent of the size of `a`.

These two implementations vary only in which argument supplies the candidates and which filters the candidates, just like the two join orders we discussed. Each implementation takes an amount of time that depends mainly on how long their first (`intersect1`) or second (`intersect2`) arguments are. So which one should we use?

I mean, we don't really have to choose do we? We can just use the right one each time:

```rust
fn intersect(a: &[u32], b: &[u32]) -> usize {
	if a.len() < b.len() {
		intersect1(a, b);
	}
	else {
		intersect2(a, b);
	}
}
```

It turns out that this obviously hack-y change does something amazing. Using this "optimized" `intersect` takes the worst-case running time of the triangle finding computation down from `O(edges^2)` to `O(edges^{3/2})`. Boom, theory happened.

Ok slow down. What actually happened here? 

Our new implementation no longer corresponds to a standard implementation of a relational join, in which we would have to pick two relations to pair up. Rather, we are choosing which relation proposes candidates and which validates them *on a record-by-record basis*. That is a pretty big departure from the standard relational approach, and it makes all the difference.

Let's run our modified implementation on `livejournal`

	triangles: 946400853 in: Duration { secs: 40, nanos: 173807791 }

Compared to the run above this is ... one second faster. Not exactly the revolution we were promised. Let's dive a bit deeper and see what happened.

The first implementation considers each pair `(x0, x1)` and for each proposes `degree(x0)` candidates, which we computed up above as the sum of squared degrees. Our second implementation considers each pair `(x0, x1)` and for each proposes `min(degree(x0), degree(x1))` candidates, which we can also compute:

	candidates_1(livejournal) = 7,292,467,251
	candidates_2(livejournal) = 3,251,016,553

This doesn't seem to be a massive reduction, and we've made the code a bit more complicated so perhaps .. sucks. Let's look at the numbers for `twitter_rv`

	candidates_1(twitter_rv) = 243,932,072,881,466 
	candidates_2(twitter_rv) = 754,462,139,483

That is a much more substantial difference. Indeed, our estimated time of two weeks for the first approach actually runs in just about three and a half hours using the second approach. 

	triangles: 143011093363 in: Duration { secs: 12977, nanos: 592176810 }

The out-degrees of the `livejournal` graph just aren't as skewed as the `twitter_rv` graph, so smartly choosing between proposers doesn't help much (or at all, really). On the other hand, smartly choosing is pretty much mandatory for the `twitter_rv` graph.

Ok, cute hack. But does it apply to anything other than triangles? It does.

## 2. Worst-case optimal relational joins.

Some recent very nice research by [Ngo et al](http://arxiv.org/abs/1310.3314) shook up the world of relational joins, or at least my view of them. There is a lot of crazy math and stuff, but the short version is that the neat trick up above with triangles applies to any old relational join you might like.

**When evaluating a join, one can repeatedly introduce *attributes* rather than repeatedly introducing relations, and thereby substantially improve the worst-case work required.**

What does this mean? In the triangles example we started with `x0` and `x1` and introduced `x2`. We did this by thinking holistically about `x2` rather than just by picking some relation that had an `x2` in it. We acted using both `edges(x0, x2)` *and* `edges(x1, x2)` at the same time, rather than one after the other.

This idea generalizes to any relational query where we order the attributes, introduce values for each attribute by considering the candidates from each relation, and start from the relation proposing the smallest set of candidates.

To demonstrate the ideas more clearly, let's think about directed four-cliques: sets of four nodes where all six edges exist.

    four(x0, x1, x2, x3) := edge(x0, x1), edge(x0, x2), edge(x0, x3), 
                            edge(x1, x2), edge(x1, x3), edge(x2, x3)

This query is the same as the `triangles` query on the first three attributes: `x0`, `x1`, and `x2`. 

The fourth attribute `x3` is different and has *three* relations that propose candidates based on the current values of `x0`, `x1`, and `x2`. This isn't fundamentally more complicated than with triangles, which has two sets of candidates, we just need a new `intersect` method that takes any number of lists of proposals (rather than just two). Here is a hypothetical implementation:

```rust
// returns the intersection of the supplied relations.
fn intersect(mut candidates: &[&[u32]]) -> Vec<u32> {

	// order `candidates` by number of candidates.
	candidates.sort_by(|x,y| x.len().cmp(&y.len()));

	// start with smallest number of candidates, intersect up.
	let mut result = candidates[0].to_vec();
	for other in 1 .. candidates.len() {
		result.retain(|x| candidates[other].contains(x));
	}

	result
}
```

The important property Ngo et al need `intersect` to have is that it should take time proportional to the smallest of the candidate sets. We do this by finding the smallest set, and then using it to drive intersection tests with the other sets of candidates. It *is* important that `contains` runs efficiently; in the Ngo et al paper they were cool with logarithmic, which works for me too. 

There are even smarter ways of doing this, like what the LogicBlox folks did in ["leapfrog join"](https://developer.logicblox.com/2012/10/leapfrog-triejoin-a-worst-case-optimal-join-algorithm/), where you repeatedly advance the smallest of the heads of each of the lists, as long as some head is larger. 

Now with this `intersect` at hand, we can write logic to count the number of directed four-cliques, which looks a lot like the code we used for directed triangles just with another nested loop, as:

```rust
let mut cliques = 0;
for x0 in 0 .. graph.nodes {
	for x1 in graph.edges(x0) {
		let edges0 = graph.edges(x0);
		let edges1 = graph.edges(x1);
		for x2 in intersect(&[edges0, edges1]) {
			let edges2 = graph.edges(x2);
			for x3 in intersect(&[edges0, edges1, edges2]) {
				cliques += 1;
			}
		}
	}
}
```

Each time we introduced a new attribute, here just `x2` and `x3`, we took each of the relations which constrain the new attribute based on existing attributes, looked up their candidate sets, and passed them as arguments to `intersect`. As long as `intersect` is smart and uses the smallest candidate set first, we get Ngo et al's sweet mathematical guarantees (time bounded by the most four-cliques there could be with as many edges as we have).

### General graph motifs

Up until this point we've been using motifs with some pleasant properties that made our life easy, without bringing these properties to light. Let's change the `four` query slightly and see how we might need to do a little more work for general queries:

    four(x0, x1, x2, x3) :=               edge(x0, x2), edge(x0, x3), 
                            edge(x1, x2), edge(x1, x3), edge(x2, x3)

This is nearly identical to the four-cliques query, but we have removed the `edge(x0, x1)` edge. This .. complicates our "algorithm" as written above, because we can no longer just say "for each `x0`, for each `x1`" unless we want to consider all `x1` for each `x0`, and we don't. At the same time this is a totally legit graph motif / relational join, so how can we sort this out?

There are two things we need to be able to do to deal with general motifs:

1.	**We may need to re-order the variables.** 

	The variable `x1` isn't constrained by `x0`, so we can't just add it after `x0` despite its name suggesting it should come next. Only the variables `x2` or `x3` are constrained by `x0`, and we will want to add one of them next, after which we could add either of the others. 

	If the motif is *connected* then we can always order the variables so that when each is introduced it is constrained by at least one prior variable, using any graph traversal (e.g. DFS or BFS). If the motif isn't connected, we should instead find instances of its connected components and take their cross products.

2.	**We may need to index `edges` by destination.** 

	When we add `x1` it is constrained by `x2` and `x3`, but the constraint is not that both of them have an edge *to* `x1`, but rather that there is an edge *from* `x1` to each of them. This means that to get a list of candidates from `x2` and `x3`, we will need an index on the second element of the `edge` relation. The constraint on a prior variable may be in either the source or target position, and we need to index the relation in both directions (unless we know we don't need one direction).

	Each relation may need to be indexed by any proper non-empty subset of its attributes. In the case of graphs this is pretty easy, because there are just two coordinates, and so two ways to index the graph. More generally we may need to build more indices.

What's a better way to do things than simply speculating about what we might need to do? Actually doing it. Then we can see: does it work? 

#### Re-ordering variables

Let's imagine that the query is presented to us as a list `description` of pairs of attribute identifiers `(i,j)` each indicating the constraint `edges(x_i, x_j)`, as well as the index `start` of the attribute we'd like to start from. To order the variables we just need to repeatedly look at pairs in `description` and add any un-added variables once they become constrained:

```rust
// orders attributes from `start` so that each is constrained by a prior attribute.
fn order_attributes(start: usize, description: &[(usize, usize)]) -> Vec<usize> {

    let mut order = vec![start];
    let mut done = false;
    while !done {
        done = true;
        for &(src, dst) in description {
            if order.contains(&src) && !order.contains(&dst) {
                order.push(dst);
                done = false;
            }
            if order.contains(&dst) && !order.contains(&src) {
                order.push(src);
                done = false;
            }
        }
    }

    order
}
```

This allows us to re-map the attributes to their position in this list, so that we may safely just add them in order of the identifiers, `0 ..`. For what follows, let's imagine we've done that.

#### Planning the query

Our next step is to take our list `description` and turn it in to a list of constraints on each attribute, so that we can just read out which prior attributes to use to propose candidates (and whether they should use the forward or reverse index to do so). We will return this as list (one entry for each attribute) of a list of pairs of (i) attribute identifiers and (ii) `bool`s which indicate forward index or not (else, the reverse index).

```rust
// indicates for each attribute which prior attributes constrain it, and how.
fn plan_query(description: &[(usize, usize)]) -> Vec<Vec<(usize, bool)>> {

	let mut plan = vec![];

	// for each attribute, determine relations constraining that attribute.
	for attribute in 1 .. {
		let mut constraints = vec![];
		for &(src, dst) in description {
			if src == attribute && dst < attribute {
				constraints.push((dst, false));
			}
			if dst == attribute && src < attribute {
				constraints.push((src, true));
			}
		}
		// add the constraints; return when done.
		if constraints.len() > 0 {
			plan.push(constraints);
		}
		else {
			return plan;
		}
	}
}
```

This is great! Now we have a "query plan", which tells us as we extend some binding of a prefix of the attributes, which of these bindings we need to use to produce candidates for the subsequent attributes. This is similar to a traditional relational query plan but different in some important ways, for example it will let us compute directed triangles in less than two weeks.

#### Suturing things together

Let's put these bits of code together into a general motif finding framework. We will start by planning out the query (step `1`). We then loop through each value for `x0` to initialize our working set `bindings` of solutions (step `2`). Finally, and the hardest part, for each attribute we extend each element of our working set of solutions by looking up candidate extension sets and intersecting them (step `3`).

```rust
// reports all bindings of node ids to variables in `description`.
fn motifs(forward: &[&[u32]], reverse: &[&[u32]], description: &[(usize, usize)]) -> Vec<Vec<u32>> {

	// 1. determine how to extend each attribute
	let plan = plan_query(description);

	// 2. start things out with `x0`
	let mut bindings = vec![];
	for x0 in 0 .. graph.len() {
		bindings.push(vec![x0]);
	}

	// 3. extend each attribute (starting at `x2`).
	for constraints in plan {
		// assemble extended bindings
		let mut new_bindings = vec![];
		for binding in &bindings {

			// a. assemble candidate extensions
		 	let mut candidates = vec![];
		 	for (idx, is_forward) in &constraints {
		 		if is_forward {
			 		candidates.push(forward[binding[idx]]);
			 	}
			 	else {
			 		candidates.push(reverse[binding[idx]]);
			 	}
		 	}

		 	// b. intersect candidates to find valid extensions
		 	let extensions = intersect(&candidates);

		 	// c. create an extended binding for each extension.
		 	for extension in extensions {
		 		let mut clone = binding.clone();
		 		clone.push(extension)
		 		new_bindings.push(clone);
		 	}
		}
	}
}
```

That is a bit of an eye-full, but I hope the first two steps at least should be clear. 

The third step proceeds through each of the attributes, and takes each of the proposed bindings to the attributes so far and tries to extend them. The extension process finds all of the candidates indicated by the `plan` (step `3a`) and uses our `intersect` method to find their intersection (step `3b`). For each extension, we create a totally new and independent binding and stash it in our working list (step `3c`).

### General relational joins

We also took advantage of the fact that graphs have only two directions to index: `forward` and `reverse`. More general relations may require more indices based on the order of attributes. For example:

	query(x0, x1, x2, x3) := edge(x0, x1), edge(x1, x2), edge(x2, x3), 
	                         other(x0, x1, x2), other(x3, x2, x1)

When we get around to extending `x2` the `other` relation needs to be indexed by its first two coordinates for the first use, and by the third coordinate for the second use. When we extend `x3` we need it indexed by its last two coordinates. If we had re-labeled some variables, we could switch these requirements around however we like. The `other` relation can be indexed six different ways (each attribute and pair of attributes), although we may not need so many. This isn't too different than relational queries, which also keep multiple indices of relations if you use multiple fields as keys, except that the traditional approach needs at most one index per use of a relation, whereas we might revisit the relation multiple times (e.g. our three indices for `other` while only using it twice).

### Measurements 

The code as written up above produces all of the extensions for each attribute in turn, so we don't actually see any results until we are done. Even for directed four-cliques, the most constrained motif on four attributes, the number of results are pretty epic and the time correspondingly large. That .. sucks for trying to figure out whether we are doing a good job or even computing the correct answer or anything like that. So we are going to put that off for the moment.

We should probably change the program somehow so that we produce results in a more continuous fashion...

## 3. Streaming, data-parallel on static data.

## 4. Streaming, data-parallel on dynamic data.

## 5. Evaluating this.