# Interpreted Datalog Check-in

I've been playing around building a column-oriented interpreted Datalog engine: [datatoad](https://github.com/frankmcsherry/datatoad).
It's linked only so you can follow along in the code if it comes to that, not because I want you (or anyone) to actually use it yet.
There may be bugs, it will certainly change .. there are certainly bugs.
I'm using it primarily to learn, and for me part of that is checking in periodically to record what I think I've learned.

First, I want to check in on some of load-bearing terms framing the work.

[**Datalog**](https://en.wikipedia.org/wiki/Datalog) is a declarative logic programming language that acts on collections of "facts": tuples of bounds values, and "rules" which produce new facts from existing facts that have some relationship to each other.
I'm not actually all that interested in logic programming itself, and if I were I'd take a very different approach to getting my work done.
What Datalog does do is provide a minimized relational language whose iteration requires one to handle both a few chonky relations and a long tail of increasingly teensy relations.

[**Columnar**](https://en.wikipedia.org/wiki/Data_orientation) data layouts reframe relations from many rows of few heterogenous types to relatively few homogenous columns.
There's a long literature on rows vs columns; I won't recount it here.
For the moment I'm taking it as a fundamental tenet that we will use columnar layouts and column-oriented computation, more to dial in my understanding than to be right about anything.
Column orientation brings an array (har) of advantages, from SIMD-like evaluation, to per-column specialization, to performant (tractable, even) interpreted execution.

[**Interpretation**](https://en.wikipedia.org/wiki/Interpreter_(computing)) is an approach to computation that avoids a translation of programs to machine code.
Without going deep on the essential difference from *compilation* (which seem to dissipate the closer you look), the tenet will be that we build one binary that should handle all data and programs, and we're interested in the time from pressing enter until we have the results.
Interpretation provides an appealing experience for the user, but often comes with a performance tax as the interpreter is continually improvising what to do next.

An *interpreted*, *columnar* *Datalog* is what we are doing.
These adjectives fit pretty well together, in that there is a lot of technical sympathy here.
They all want and contribute fairly synergistic components.

## Columnar Representation

Datalog works on "relations", each of which denote a collection of "facts", each of which is a tuple of typed columns.
All facts within a relation have the same number of columns, and the types of each are the same across all facts.

As an example, consider a relation containing first name, last name, age, and favorite animal.

| first |     last | age |  animal |
| :---: |    :---: | :-: |  :----: |
| frank | mcsherry |  49 | chicken |
| frank |       oz |  81 |     pig |
| frank |    zappa |  52 |  weasel |
|   jim |   henson |  53 |    frog |
|   ... |      ... | ... |     ... |

Some of these columns are text, one of them could be an integer, and some of them could be categorical ("animal", perhaps).
While it is natural to think of these as rows read left to right, it's equally valid to carve them up as four columns.
When we do this it's important that we keep the order within the column the same, so that like entries still line up, not unlike how we aren't meant to shuffle around the entries within a row (e.g. mixing up first and last names).

Let's take a more abstract example from [a previous post](https://github.com/frankmcsherry/blog/blob/master/posts/2025-08-24.md), of rows over three columns of likely different types:
```
(A, 1, x),
(A, 1, y),
(A, 1, z),
(A, 2, x),
(A, 3, x),
(A, 3, y),
(B, 2, z),
(C, 1, x),
(C, 2, x).
```
A naive shift to a columnar representation might produce these three lists of homogenous types.
```
c0  c1  c2
--  --  --
A   1   x
A   1   y
A   1   z
A   2   x
A   3   x
A   3   y
B   2   z
C   1   x
C   2   x
```
One immediate benefit is that we can stash the contents of each column in a specialized container.
Perhaps we know that the first column is always a single character, and we could use a Rust `char` (four bytes).
The second column is clearly integers, and potentially even bytes; we could use a `Vec<u8>` in that case.
The third column might be arbitrary strings, and a `Vec<String>` would work, or we could pack their contents into contiguous memory by appending them, and recording demarcations on the side.

A related benefit is that it is relatively easy to drop a column, add a column, re-order the columns.
If we initially mistook the second column for text, once we see that it is all integers we can parse them and change the column type.
Once we see they all fit within a byte we can change the type again.
We never needed to specialize our program to the type `(char, u8, String)`, as a compiled row-oriented program would be expected to do.

You might have noticed that the data above are *sorted*, when viewed as rows.

We are going to rely heavily on introducing and maintaining sortedness, as it is a key part of *finding* data.
Many columnar systems are happy to just grind over *all* data, and while they are really good at that we are going to need to be able to quickly locate subsets of data by prefixes of columns.
For example, to find all records matching `(A, 3, _)` we would only need to sniff around in the first two columns, compactly represented in our story, to find the bounds in the third column (text) but without tripping over the text itself as we look.
Columns often provide this benefit of only working with (e.g. bringing in to cache) the relevant subset of each row.

Another benefit of sortedness is that it supports a natural form of prefix compression.
You might have noticed that the first column is also sorted, which should be true of any sorted data, and that it has runs of identical values.
We could compress the data by recording each distinct value and the number of times it appears, called "run length encoding".
We'll do something similar in spirit, but slightly and importantly different.

Let's think instead about peeling off the last column.
It has all distinct values (if the rows are initially sorted and deduplicated), and is great just how it is.
Were we to then strip it off of our data, we would still have sorted data but now with a bunch of repetition.
```
c0  c1
--  --
A   1
A   1
A   1
A   2
A   3
A   3
B   2
C   1
C   2
```
Each distinct row repeats some number of times, corresponding to an interval in the third column.
For example `(A, 3)` repeats twice, and corresponds to the interval `[4, 6)` in the third column.
Let's deduplicate, but record the bounds and associate them with that third column.
```
c0  c1  bounds  c2
--  --  ------  --
A   1   [0, 3)  x
A   2   [3, 4)  y
A   3   [4 ,6)  z
B   2   [6, 7)  x
C   1   [7, 8)  x
C   2   [8, 9)  y
                z
                x
                x
```
The pair `(bounds, c2)` describes a sequence of lists, with as many lists as there are elements in `c1`, and with as many total list items as there are elements in `c2`.
The bounds essentially act as a translation layer from `c1` to `c2`.
You might notice that we always start with zero, and the upper bound always matches the next lower bound, so we can save a bit on representation (and formatting) by just recording the upper bound.
```
c0  c1  b2  c2
--  --  --  --
A   1   3   x
A   2   4   y
A   3   6   z
B   2   7   x
C   1   8   x
C   2   9   y
            z
            x
            x
```
We can repeat this with the first two columns, yielding
```
c0  b1  c1  b2  c2
--  --  --  --  --
A   3   1   3   x
B   4   2   4   y
C   6   3   6   z
        2   7   x
        1   8   x
        2   9   y
                z
                x
                x
```
You might now strongly feel that we should group `(c0, b1)` and `(c1, b2)`, based on their lengths, but you'll need to resist it because we aren't going to do that.
Each of the bounds translate from the previous column to the next, but otherwise have no meaning to that previous column.
It also makes a lot more sense if you want to accommodate [forward-looking patterns](https://www.cs.ox.ac.uk/dan.olteanu/papers/os-sigrec16.pdf) that might want to have multiple independent extension columns stapled on to the same `c1`.

This brings us to our core data representation: a sequence of "layers", each corresponding to a column, where each layer is a sequence of lists of the column type.
In our case above, that sequence would be the three elements:
```
([3], c0), (b1, c1), (b2, c2)
```
I've had to improvise a `b0` term, which will always be a single element for sorted data, but could be longer for unsorted data (which we may transiently have while processing data).

This representation has advantages and disadvantages relative to row-oriented and flat column-oriented representations.
I happen to like it, and the downsides don't stress me out as much as the upsides appeal to me.

Finally, rather than represent relations by a single sequence of layers, we'll use the idiom of a [log-structured merge tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree).
These are data-structures that maintain a mutable sequence of immutable collections, where modifications happen by appending new collections, and periodic "maintenance" merges collections to keep the number logarithmic (by maintaining the invariant that the sizes of collections should at least halve as you move through them).
This gets us out of the business of sneaky mutable datastructures and into a more predictable regime suitable for big blocks of contiguous data.

## Columnar Operation

Datalog is elegant, if simplistic, and has only a few inherent computational primitives.
You can build a fully functional Datalog with only:

1.  **Union(X, Y)**: facts from X or Y, deduplicated.

    This is used to collect facts that can be produced through different rules.
    It is also pervasively in the LSM implementation, which must merge facts collections of similar sizes for maintenance, and to reduce a list of collections to one for further operation.

2. **Antijoin(X, Y)**: facts from X, but whose prefix is not in Y.

    Datalog's iterative "semi-naive" derivation of facts proceeds in rounds starting from "novel" facts: those not previously derived.
    A critical part of this is determining which recently derived facts are indeed novel, rather than just redundantly derived.
    Some Datalogs also support an antijoin operator, but almost all Datalogs have antijoin somewhere in their innards just for baseline operation.

3. **Join(X, Y, arity)**: fact pairs from X and Y whose first arity values match.

    Datalog rules involving multiple atoms can be seen as relational equijoins, where the variable terms common to multiple atoms introduce an equality constraint.
    A common simplification is to binary equijoins, between two atoms, keyed by the equated columns.

4.  **Project(X, cols)**: facts from X but with a specified column subset and order.

    A bit of a leaky implementation detail, but we'll need to be able to re-key our collections of data by various columns.
    Although we might have a single meaningful set of facts, if we need to look its facts up by different columns we'll want different layouts.

These operations provide the backbone of work that needs to happen in a Datalog engine.
There are other operations to consider, for bonus points, that support cleverer engines; we won't get in to those in this post.

### Union

We've [previously discussed](https://github.com/frankmcsherry/blog/blob/master/posts/2025-08-24.md) a columnar implementation of union, but let's revisit it as a warm-up.

A standard way to merge two sorted lists is to go row-by-row from each input, recording the lesser item and advancing its list (or advancing both if they are equal).
Clearly not very columnar.

This approach is very reasonable when you have rows, but you can see that it invites one of the problems of row-oriented computation: it continually needs to shift what it is thinking about as it compares rows.
A row might have three different column types, and as each column is compared the program needs to be ready to choose left or right, or go deeper if they are equal.
It's usually a surprise what will happen next, both the data required and operations to perform, and modern processors don't care much for surprises.

Let's imagine instead I show up with two inputs to merge, each represented as their three columns `(x0, x1, x2)` and `(y0, y1, y2)`.
We need to produce the three columns that correspond to their merged results.

1.  Merging the first column is really easy: it will be a merge of the values in `x0` and `y0`.

    We are deduplicating, so we'll need to deduplicate as we merge, but there's no mystery about the values we'll need to produce.
    They are exactly the values in `x0` and `y0`, merged.

2.  Merging the second column is less clear.

    The second columns will have runs that correspond to one value in their first column.
    If that value didn't match a value in the other first column, we just need to splice that run in, as is.
    If the value in the first column did match the same value in the other first column, we need to merge the two runs and record them.

3.  Merging the third column isn't any harder.

    Runs in the third column also need to know if they correspond to a distinct prefix in the first two columns.
    If so they should just be copied in.
    If their prefix is also present in the other input, they need to be merged with their corresponding run.

For each column, we'll want to transcribe the sequence of comparison outcomes, less, equal, or greater, that inform the next layer about whether their prefixes match or not.
More actionably, we'll have each layer produce a "report" that is a list of three possible outcomes for the prefixes up through the list items of the
```rust
enum Report {
    This(usize, usize), // [lower, upper) are exclusive to the first input.
    That(usize, usize), // [lower, upper) are exclusive to the second input.
    Both(usize, usize), // index0 in the first input equals index1 in the second.
}
```
A list of reports should account for each prefix of each input:
The `This` intervals and first coordinates of `Both` should span exactly the list items in the first input.
The `That` intervals and second coordinates of `Both` should span exactly the list items in the second input.
The order of the reports in the sequence spells out to the next layers how they should interleave their *lists* (which have the same indexes as the items of the previous layer).

All told, you end up with a layer-by-layer union algorithm, with the benefit that these identified intervals allow for high-throughput `memcpy` to populate the output column.
As reported in the previous post, switching from a row-oriented implementation to a column-oriented implementation resulted in some speed-ups for a reference workload:

|            | in union | in total |
|------------|---------:|---------:|
| row-based  |    5,084 |   20,159 |
| col-based  |    3,279 |   18,561 |

### Antijoin

Antijoins find the subset of their first input that do not appear in the second input, or if they have more columns those whose prefix doesn't appear in the second input.

The easiest way I can think about antijoins are to start instead with intersection: finding the rows present in both inputs.
We'll see once we have that information, it is a relatively quick solve to flip the bits and keep those facts that are absent from the intersection.

In fact, we already have a great mechanism for intersection, hinted at just above.
The `Both` report variant tells us when prefixes match, and .. we can just ignore the `This` and `That` variants, which correspond to non-intersection.
In fact, we can ignore the entire intervals of `This` and `That`, all of which will not possibly intersect even as we explore further columns.

Having worked through all of the layers in common between X and Y, we'll have a list of `Both(idx0, idx1)` reports.
These indicate the prefixes in common, and are what we'll use to find the antijoin (alternately, the semijoin if we wanted that).
Informally, all we need to do is crawl back up the layers, at each layer retaining items on retained paths.
If X has more columns than Y we'll also need to project forward through the additional layers to retain all extensions of each retained prefix.

As with union, we now have a column-by-column intersection and antijoin/semijoin operator.
Though, hold off on using it for semijoin until you read the next section.

As before, we have some numbers that show how doing intersection column-by-column can improve performance.

|            | in except | in total |
|------------|----------:|---------:|
| row-based  |     3,451 |   19,417 |
| col-based  |     2,528 |   18,505 |

These numbers don't line up with above, but they are independent improvements that compose.

### Join

Join is really close to intersection, and our columnar join starts by immediately invoking it to determine the paths that match between the first arity layers of X and Y.
Having listed the `Both` variants we have produced what is called a "constant time enumerator": each `Both(idx0, idx1)` match results in at least one output, though potentially more, and all we need to do is just emit them.
The indexes tell us exactly where to go in the post-arity layers, and at this point it really is just a matter of recording the results.
Which .. turns out to be the hard part.

In fact, **Join(X, Y, arity)** is pretty easy if you are just expected to produce the X followed by Y.
It's a semijoin (X with the first arity columns of Y), followed by new layers that copy in the matched elements of Y (with some repetition, to effect the Cartesian product).
The giant pain in the ass is that no one ever wants that; they want the results projected onto a subset of columns in a different order.
That eventually leads us to the hardest of operations: **Project**.

It is with great shame, or at least great "I'm not finished yet; it's not quite done" that I can report that datatoad's columnar join produces the `Both` matches in a fraction of a second, and then spends about 20x as long translating the output into the requested representation.

|            | in join | in total |
|------------|--------:|---------:|
| row-based  |   9,431 |   15,870 |
| col-based  |   7,521 |   14,001 |

The row-based implementation does a row-by-row intersection and it's hard to extract, but the column implementation spends ~350ms determining the full set of `Both` reports across the computation.
It's pretty fast!

The slight misrepresentation here is that joins are fast when you have data in the right representation.
Putting the data into that representation is not free, and that's the thing I'm complaining about.
Of the 7521ms, the breakdown is roughly:

|              |   ms |    % |
|-------------:|------|-----:|
|        total | 7521 | 100% |
|   radix sort | 4471 |  59% |
|  layer build |  796 |  11% |
| intersection |  349 | 4.6% |
| faffing about| rest | rest |

"Faffing about" is what you get with row-oriented computation, where there's just unattributable time spend in undifferentiated computation.
It was doing something, perhaps allocating memory or freeing memory, or bouncing around in some control flow.
In this case, I think it's mostly blatting down bytes to feed to the radix sorter, which starts to reveal the larger misrepresentation that this is entirely columnar.

We're not fully columnar yet, unfortunately.
While the joined records are identified columnarly, in ~350ms, the work to translate the output to the target format and reform the columns is still done in a row-oriented form: lay out the rows, sort them, build the columns.
I'll be delighted to get there (full columnar) eventually, and there is a clear plan, but to be totally honest I'm mostly delighted that the ham-fisted columnar join I implemented is faster than the row-oriented join.

What's in the way you ask?

### Project

The project operator is the tax we pay for our pleasant compressed columnar representation.
Say we have three columns `c0`, `c1`, and `c2`, and you decide you'd really rather have them in the order `c1`, `c2`, `c0`, there is a non-trivial work to do.

By far the *easiest* way to make the change is to re-form all of the rows, sort them, and then build the prefix-compressed columnar representation.
We do that at the moment in datatoad, because it has a common fallback datatype `[u8]` that we can form rows out of.
But there really is a lot of time spent transposing the data (to rows) and then back again (to columns).
The radix sorting that happens along the way is pretty fast, and I don't want to point any fingers at it, but I would like to lean that out a bit.

Among the challenges of a fully columnar implementation is that when you rotate `c0` to the end, the sequence `c1`, `c2`, `c0` doesn't make any sense.
The first column is not a single sorted list, but rather a number of sorted runs, each corresponding to one element of `c0`.
Also `c0` is a bit weird at the back end, as it is way too short.

There are fixes to each of these problems, but they involve some thinking, some typing, and some refinement.
For example:

1.  Rotating `c0` past a layer can be accomplished by repeating each value a number of times that is determined by the length of lists the value passes through.
    You can almost just keep the same items and for each an integer indicating its number of repetitions.

2.  We'll want to convert `c1` to one sorted list, containing the same values, which is super easy to do.
    Just sort and deduplicate it!
    However, we'll need to pay attention to where each sorted item ends up, because the next layer is going to need to group and then sort its lists using this information.

This leads to a column-oriented permutation and sorting algorithm, which first permutes columns into place, and then sorts each column using some auxiliary grouping indexes provided from the previous column.
There is no reason a columnar sort should go faster than a row-by-row sort, but there are some fun reasons it might.
At least, in the limit, if I get to pick the data.

1.  Radix sort is mostly throughput limited, and sorting each column once with auxiliary grouping information can move less data than the same row-by-row sort, which needs to move the whole row for each sorting pass (which may grow in proportion to the number of columns).
2.  Prefix-compressed column sizes often grow geometrically, with `|c0| << |c1| << |c2|`.
    Row-by-row sorting would do work proportional to `|c2|` for everything, but e.g. when we need to sort `c1` it's much smaller, and easier.
    When we need to sort `c0` its values are already sorted, meaning we can skip those waves of bottom-up radix sort.

So that's where I'm headed next: columnar sorting.
Though to be honest I'll probably still be a bit noodling on the join implementation and trying to cut out various bits of fat there.
The code is quite long, about 5 lines for intersection and 120 lines to bounce into and out of the row-oriented representation.

## Related Reading

I have been a bad "researcher" and have just been designing and implementing, more than reading.
One paper I figure I should have read is [Column-Oriented Datalog on the GPU](https://thomas.gilray.org/pdf/column-datalog-gpu.pdf).
It seems like other folks have already been looking at columnar Datalog, though I worry I don't fully understand things as most of the other results seem not entirely great.

For example, here are their reports of elapsed seconds to perform the "same generation" Datalog fragment, an imo very silly fragment, on several systems.
```
sg(?x,?y) :- p(?z,?x), p(?z,?y) .
sg(?x,?y) :- sg(?a,?b), p(?a,?x), p(?b,?y) .
```
I think you are meant to do `?x != ?y` but datatoad doesn't support that yet, so I ignored it (pretty sure you can fix it in post, by filtering `sg`).
I say "very silly" because it's mostly a deduplication benchmark; the explicit

These numbers are for compiled (I think?) code on 64 cores (for the CPU) and 16,896 CUDA cores (for the GPU).
I add a column for interpreted datatoad on one core.

|   dataset | FVlog |  Vlog |  Nemo | Soufflé | RDFox | datatoad |
| ----------|-------|-------|-------|---------|-------|----------|
| vsp_finan | 7.52  |  4403 |  2172 |   151.5 |   257 |   205.39 |
| CA-HepTH  | 0.55  | 313.7 | 147.7 |   20.12 |  20.1 |    20.92 |

There are other datasets reported that I did pull down (I stalled out when it took three attempts to find `vsp_finan` in a non-binary format).

But like, what's going on here?
Props to FVlog, naturally, but .. I can't really figure out what is going on with these other systems.
There are some publications for me to read ([Vlog](https://arxiv.org/pdf/1511.08915) and [Nemo](https://arxiv.org/pdf/2308.15897) are both column-oriented), and Soufflé and RDFox are I think known not to be top performers in benchmarks (though presumably much less rough around the edges).
Tt feels like there is some room for improvement here (perhaps not over FVlog, unless we aim it at some weirder problems maybe).

So I'll also be looking into what's doing in these systems.
Again, Datalog itself isn't what I'm actually interested in, so much as the task of high-throughput data alignment, and it just happens that Datalog insists that you be good at this.
