## Materialize and Memory

If you've used Materialize, you've probably noticed that it uses memory.
It remembers things you've done before, to give you a head start when the world changes slightly.
At the same time, it sure seems to being remembering quite a lot; what's going on in there?

We've had a bit of a sprint recently reducing Materialize's memory footprint, and consequently its cost.
Materialize remembers just as much as before, but it is a fair bit smarter about how it remembers it.
The story of the smartness is interesting, and we invite you to join us for a guided tour!

### The Fundemantals of Remembered Things

Materialize models all data as relational rows, each of which has some number of columns.
However, it helps to remember these data as pairs `(key, val)`, where `key` are some of the columns, and `val` are the rest of the columns.
When you create an index on data, you specify some `key` columns, and expect to get access to the whole row by providing the values of the `key` columns.

Of course, Materialize doesn't just remember `(key, val)` pairs, but for each pair a *history*: a list of `(time, diff)` updates.
Each `(time, diff)` pair says that at some `time` the count of the `(key, val)` pair changed by `diff`.
When you introduce `(key, val)` the `(time, diff)` pair will be .. something where `diff` equals one.
If you ever delete the pair (perhaps as part of updating `val` to something new) the `diff` entry will be negative one.

The abstract data type we want to access is a map from `key` to `val` to a list of `(time, diff)` pairs.
In Rust you might use
```rust
/// Map from key, to val, to a list of times and differences.
type Indexed<K, V, T, D> = HashMap<K, HashMap<V, Vec<(T, D)>>>;
```

For a large number of reasons we don't actually want to use `HashMap`, but instead other types that provide similar access with more robust performance and a leaner memory footprint.
Not to ding `HashMap`, we just won't need a great deal of its flexibility, and we will need to be very careful about issues like insertion throughput and contiguous memory layouts.

### A First Evolution, circa many years ago

Differential dataflow's fundamental data structures are based on sorted lists.
You probably thought we were going to try to impress you with exotic improvements on Rust's `HashMap` implementation, but we are going to start with sorted lists.
All of differential dataflow's performance, which has been pretty solid, has been based on sorted list technology.

Sorted lists have a very compelling property that Rust's `HashMap` does not have: you can append many sorted lists into one larger list, at roughly no overhead. You need to stash some integer offsets to clarify the bounds of the sorted runs, but you don't need as many instances of `SortedList` as you have keys, in the way that you would with our hypothetical `HashMap` implementation above.

To store the map from `key` to `val` to list of `(time, diff)` updates, differential dataflow uses three vectors:
```rust
struct Indexed<K, V, T, D> {
    // key, and the start of its sorted run in `self.vals`.
    keys: Vec<(K, usize)>,
    // val, and the start of its sorted run in `self.upds`.
    vals: Vec<(V, usize)>,
    // lots and lots of updates.
    upds: Vec<(T, D)>,
}
```

Each key is present once, in sorted order. 
The `usize` offset for each key tells you where to start in the `vals` vector, and you continue until the offset of the next key or the end of the vector.
The `usize` offset for each value tells you where to start in the `upds` vector, and you continue until the offset of the next value or the end of the vector.

The overhead is a `usize` for each key, and another `usize` for each distinct `(key, val)` pair.
You have three allocations, rather than a number proportional to the number of keys or key-value pairs.

### Shenanigans

Although told a bit out of order, historically, there are several shenanigans you can now do to thin out your memory footprint.
We've done each of these, and they did a great number on the memory requirements of our users.

#### Key specialization

There are many cases where we know statically that `val` is empty.
This happens when we need an index on all columns of your row, or only need as many columns as we happen to index (e.g. when applying `DISTINCT` to a set of values).

In these cases, we can elide the whole `vals: Vec<(V, usize)>` field, and have the offsets in `keys` directly reference into `upds`.
This shaves off a large pile of pairs of unhelpful `V` values, and promotes their offsets into the largely unhelpful offsets that were being maintained in `keys`.

In our reckoning, this saves about 32 bytes per key-value pair, when applicable, at the cost of some code complexity (it's a different type, and we don't want to cross dynamic dispatch boundaries to use it).

#### Singleton compression

It turns out that a lot of those `(time, diff)` pairs look a lot like `(0, 1)`.
That is, these collections have a lot of records that exist once, and came in to existence far enough back that we might as well treat it as the beginning of time.

There's a cute trick to apply in this case, where the offsets that are meant to index into `upds` communicate something different when they equal the next offset.
Normally this wouldn't be allowed: if you have an empty range of updates, you should just be removed, rather than present with no updates.
However, we'll steal that configuration to mean "actually, grab the one update at that position".
Now, long runs of folks whose updates are `(0, 1)` can use the same offset, and we'll only have a single instance of `(0, 1)` for the whole run.

This saves about 16 bytes for each key-value pair where this happens.
As it turns out this happens quite a lot, because folks do seem to have a large backstop of unchanging data, amidst smaller eddies of churning updates.

#### Offset specialization

We wrote `usize` above for the offsets, and it is totally possible we'll need all eight bytes if we happen to have more than 4 billion updates.
Well, four billion updates in one of these indexes, on a single worker, in a single layer of the log-structured merge list.
Realistically, we could probably get away with four bytes rather than eight bytes.

In fact, we can use a cute data structure that presents as a `Vec<usize>` even though internally mostly using `u32` values:
```rust
struct OffsetList {
    smol: Vec<u32>,
    chonk: Vec<u64>,
}
```
Here we push values into `smol` as long as they fit in a `u32`, and spill to `chonk` as soon as they do not.
The logic to look up is essentially the same as for a `Vec<u32>` in the common case, and we reap a 2x memory saving at roughly no computational cost.

This saves roughly four bytes for each key and key-value pair.

### Containers, Columnation, and 

At this point we've thinned out quite a bit of our initial proposal.
We have something that maintains all the keys, and values for each key-value pair, and in addition to that roughly four bytes for each of them.
You may be wondering what we have left to squeeze.

Materialize's keys and values are a `Row` type that is morally equivalent to `Vec<u8>`.
It takes up 24 bytes and except for [some inline storage tricks]() we use it to point at a bunch of bytes that we then interpret as columns of data.

Ignoring the inline storage for the moment (you shouldn't; it is smart!), the `Row` type comes with a 24 byte overhead beyond the binary data it stores.
Worse, potentially, each also comes with its own allocation. 
We had a nice story about how there were only three allocations, and now you are learning that each key and key-value pair have an allocation also.
Disappointing!

In fact, we have a smarter way that we can wrangle the data than `Vec<Row>`.
A `Vec<Row>` has the useful interface of methods
```rust
/// Inserts `row` at position `index`.
set(&mut self, index: usize, row: Row);
/// Returns a reference to the row at `index`.
get(&self, index: usize) -> Option<&Row>;
```
However, these methods are way more than we need.
To hint at where we are going, we'd be just as happy-ish with these slightly different methods:
```rust
/// Inserts `row` at the next position.
push(&mut self, row: &Row);
/// Returns a reference to the row at `index`.
get(&self, index: usize) -> Option<&[u8]>;
```
What changed here?
We don't have to have an *owned* `Row` when we insert; that's interesting.
Also when we read out we don't receive a `Row` but rather a `&[u8]` byte slice.

If you scan up to how we represented keys, values, and updates, you might see where we are going here.
We can create a different *container*, not a `Vec<Row>` but similar enough to suit our needs:
```rust
struct ByteContainer {
    /// Offsets into `self.bytes` indicating slice bounds.
    offsets: OffsetList,
    /// Concatenated byte slices.
    bytes: Vec<u8>,
}
```
As above, we use access into `self.offsets` to determine the slice bounds in `self.bytes`.
When you `push` a new `Row` we just concatenate in the bytes and push into the offset list.
When you `get` an index we form the byte slice from the offsets and return it.

This representation has four bytes of overhead for each `Row` we maintain, rather than twenty four bytes.
It does miss out on the inline storage, and that can be a big deal when data fit nicely in that inline storage.
However, shearing off another 20 bytes for each key and for each key-value pair is a massive opportunity.

### Generic Associated Types (GATs)

If you are familiar with Rust's "generic associated types" you probably just sensed how the room got quiet, and some folks started moving towards the emergency exits.
They are, for me at least, a pretty intimidating concept that I'm not certain leads to things like memory reduction so much as complicated error messages and PhD theses.

However, just above we did something that is very much like baby steps towards GATs.
The container modification we proposed said "instead of a `&Row`, howsabout I give you a `&[u8]` instead?"

That seems pretty innocent, but "howsabout I give you `<something else>` instead?" is an instance of generic associated types.

For example, where a `&[u8]` needs to be a reference to contiguous bytes, what if instead we provided something that can produce the bytes, but are not the bytes themselves?
Let's talk through some examples.

1. **Entropy compression**: [Huffman coding](), and more recently [asymmetric numeral systems](), take sequences of symbols, for exmaple bytes, and reduce them to a size that reflects their entropy.
We can (and have) encode the binary `Row` data into a compressed form, and rather than return a `&Row` or a `&[u8]` we can return a `(&Codebook, &[u8])` from which the data can be decoded.
2. **Dictionary compression**: Entropy coders are ofen the second line, after techniques that find larger scale recurring structure. 
In LZ77 this is some amount of look-back, and in LZ78 it is a dictionary of common sequences.
In relational data especially, columns often have recurring identical values: order statuses of `'ordered'` or `'pending'` or `'shipped'`, to say nothing of the heart-breaking lack of normalization.
Whole sequences can be replaced by short surrogates, who are then excellent subjects of entropy compression.
3. **Columnar compression**: The techniques above could apply to whole `Row` or `&[u8]` sequences, but we have further information about the column structure of our data. 
Individual columns exhibit different statistical properties, have different frequent elements, and benefit from independent compression.
In many realistic cases, all values in a column are the same and after dictionary and Huffman coding take literally zero bytes to record (other than once, in the codebook).

Each of these techniques derive from our ability to say "howsabout instead of a `&[u8]` I give you something else?".

The implications of these techniques depend tremendously on the specifics of the data.
If you are storing compressed data as part of good operational security, we may not find entropy coders doing a good job, but we might find dictionary techniques still work.
If you have a relation with a primary key column, we shouldn't expect dictionary coding to help as every row *should* be distinct.
If your data have a single `jsonb` column then columnar compression isn't going to be very much help.

All that to say, we have no idea how impactful these techniques will be. 
They do throw the doors wide open for memory reduction, though.

### 