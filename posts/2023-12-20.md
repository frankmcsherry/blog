## Materialize and Memory

Materialize keeps your SQL views up to date as the underlying data change.
To do this, it retains valuable and relevant intermediate state, and requires random access to this data.
The most natural way to maintain random access to information is using memory, and memory can be expensive.

The value Materialize provides comes from how promptly it reflects new data, but its *cost* comes from the computer resources needed to achieve this.
While we talk about the value Materialize provides, and work continually to improve it, we are also hard at work continually reducing the cost.
This work has had marked impact recently, and it felt like a great time to tell you about it, and the reductions in cost. 

To be clear, Materialize maintains your source data, and much derived data, durably in economical cloud storage.
However, to promptly maintain and serve the data we want to use much more immediately accessible storage.
This storage, memory or as we'll see here local disk, acts as a local cache that must be fast, but needn't be durable.
And of course, we would all like it to be as economical as possible.

We've been dialing down the amount of "overhead" associated with each maintained record in Materialize.
It started some months ago at roughly 96 bytes of overhead (we will explain why), and is now closing in on between 4 and 16 bytes of overhead, depending.
This first wave of results have already seen many users memory requirements reduced by nearly 2x, in cases where the maintained records are many but narrow.
Moreover, we've laid the groundwork for further improvements, through techniques like spill-to-disk and columnar compression.
The work comes at the cost of CPU cycles, but for the moment CPU cycles are abundant (and elastic) in a way that bytes are not.

### The Fundemantals of Remembered Things

Materialize models all data as relational rows, each of which has some number of columns.
Over time the rows come and go, changing their multiplicity through what we call "updates": triple `(row, time, diff)`.
Each update indicates a `row` that at some `time` experiences a change `diff` in its multiplicity.
These changes are often `+1` (insertion) or `-1` (deletion) or a mix of two or more (updates).

However, Materialize maintains *indexed state* by viewing each `row` as a pair `(key, val)`, where `key` are some signified columns and `val` the remainder.
When you create an index on a collection of data, you specify columns by which you hope to access the data; those columns define `key` for each `row`.

The abstract data type we use maps from `key` to `val` to a list of `(time, diff)` pairs.
In Rust you might use the `HashMap` type to support this abstraction:
```rust
/// Map from key, to val, to a list of times and differences.
type Indexed<K, V, T, D> = HashMap<K, HashMap<V, Vec<(T, D)>>>;
```

For various reasons we won't actually want to use `HashMap` itself, and instead prefer other data structures that provide different performance characteristics.
For example, we are interested in minimizing the number and size of allocations, and optimizing for both random and sequential read and write throughput.
Not to ding Rust's `HashMap` at all, but its flexibility comes at a cost we won't want to pay.

### A First Evolution, circa many years ago

Differential dataflow's fundamental data structures are thusfar based on sorted lists.
You may have thought we were going to impress you with exotic improvements on Rust's `HashMap` implementation, but we are going to start with sorted lists.
All of differential dataflow's performance, which has been pretty solid, has been based on sorted list technology, which is nothing to write home about.

Sorted lists do have one compelling property that Rust's `HashMap` does not have: you can append many sorted lists into one larger list, and only need to record the boundaries between them.
This reduces the per-key, and per-value overhead to something as small as an integer.

To store the map from `key` to `val` to list of `(time, diff)` updates, differential dataflow uses roughly three vectors:
```rust
/// Simplification, for clarity.
struct Indexed<K, V, T, D> {
    /// key, and the start of its sorted run in `self.vals`.
    keys: Vec<(K, usize)>,
    /// val, and the start of its sorted run in `self.upds`.
    vals: Vec<(V, usize)>,
    /// lots and lots of updates.
    upds: Vec<(T, D)>,
}
```

Each key is present once, in sorted order. 
The `usize` offset for each key tells you where to start in the `vals` vector, and you continue until the offset of the next key or the end of the vector.
The `usize` offset for each value tells you where to start in the `upds` vector, and you continue until the offset of the next value or the end of the vector.

The data structure supports random access through binary search on keys, high throughput sequential reads and writes, and random access writes through a [log-structure merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree) .. or merge-list idiom.

The overhead is one `usize` for each key, and another `usize` for each distinct `(key, val)` pair.
You have three allocations, rather than a number proportional to the number of keys or key-value pairs.
The overhead is seemingly not tremendous, until we perform a more thorough accounting.

### A More Thorough Accounting

Although Materialize maintains only two `usize` (each 8 bytes) beyond the `K`, `V`, `T`, and `D` information it needs for updates, there is more overhead behind the scenes.

Both `K` and `V` are `Row` types, which are variable-length byte sequences encoding column data.
In Rust a `Vec<u8>` provides a vector of bytes, and takes 24 bytes in addition to the binary data itself.
In fact, we used a 32 byte version that allowed some amount of in-line allocation, but meant that the minimum sizes of `K` plus `V` is 64 bytes, potentially in addition to your binary row data.

Both `T` and `D` are each 8 byte integers, because there are many possible times, and many possible copies of the same record.
Adding these to the two 8 byte `usize` offsets, and the 32 + 32 bytes for `Row`, we arrive at 96 bytes of minimum buy-in for each update.
These 96 bytes may cover no actual row data, and can just be pure overhead.

### Optimization

Fortunately, the more thorough accounting leads us to a clearer understanding of opportunities.
Every byte that is not actual binary payload is in play as optimization potential.
Let's discuss a few of the immediate opportunities.

#### Optimizing `(Time, Diff)` for Snapshots

Materialize both computes and then maintains SQL views over your data.
A substantial amount of updates describe the data as it initially exists, an initial "snapshot", before changes start to happen.
As changes happen, we continually roll them up into the snapshot, maintaining a collection of data from which updates describe changes.

The snapshot data commonly have updates where `(time, diff)` are, roughly, `(now, 1)`.
That is, the `(key, val)` pair exists "now", once. 
This provides an opportunity for bespoke compression: if a `(time, diff)` pair repeats, as it often does for these snapshots, we are able to avoid writing it down repeatedly.
We can sneak this in at zero overhead by taking advantage of a quirk in our `usize` offsets: they *should* always strictly increase to indicate ranges of updates, but we can use a repetition (a non-increase) to indicate that the preceding updates should be reused as well.

This saves 16 bytes per update for the snapshot, which is often the lion's share of data.

#### Optimizing `Row` representation

Although we have a 32 byte `Row` we could get by with much less.
Just like we appended lists and stored offsets to track the bounds, we could append lists of bytes and maintain only the `usize` offsets that tell us where each sequence starts and stops.

This takes us from 32 bytes with the potential for in-line allocation, to 8 bytes without that potential.
This applies twice, once each to `key` and `val`.
We now have four offsets, two for each of `key` and `val`, which will be important next.

#### Optimizing `usize` Offsets

Our `usize` offsets take 8 bytes, but rarely get large enough to need more than 4 bytes.
Rather than use a `Vec<usize>` to store them, we can use a `Vec<u32>` until we need to represent 4 billion-ish, at which point we can spill into a `Vec<u64>`.

This shaves the four `usize` offsets down from 32 bytes to 16 bytes, in most cases.

Going further, when there is exactly one value for each key (e.g. as in a primary key relationship) the offsets from key to value will be the sequence 0, 1, 2 ..., and can be recorded as such (and converted to an explicit list should the property be violated). When the binary slices have the same length (e.g. contain only fixed-width columns) the corresponding length offsets are simply multiples of this length, as above but with a multiple other than one. For the snapshot, when all updates are `(now, 1)` the offsets are almost all the same value, and can similarly be optimized.

These further optimizations bring the 16 bytes of overhead closer to 4 bytes, with the potential to dip close to zero when stars align.

### Further Optimization and Future Work

With nearly zero overhead, you may be surprised to learn that we are not yet done.
But in fact, there is still opportunity to further reduce cost!

#### Paging Binary Data to Disk

Materialize, by way of differential dataflow, performs its random accesses in a way that resembles sequential scans (essentially: batching and sorting accesses).
This means that putting binary payload data on disk is not nearly as problematic as it would be were we to access it randomly, as in a hash map.
Disk is obviously substantially cheaper than memory, and it provides the opportunity to trade away peak responsiveness for some cost reduction.

Our experience so far is that initial snapshot computation experiences no degradation (the disk accesses are sequential scans), and once up and running updates are often low enough volume that SSD accesses do not hold up timely results.

#### Columnar Compression

Rust has some [handy mechanisms](https://blog.rust-lang.org/2022/10/28/gats-stabilization.html) that allow us to interpose code between the binary data for each row and the SQL logic that will respond to the row data.
In particular, we are able to sneak in various compression techniques, ranging from [entropy coding](https://en.wikipedia.org/wiki/Entropy_coding) like Huffman and ANS, to [dictionary coding](https://en.wikipedia.org/wiki/Dictionary_coder) which often works well on denormalized relational data.
Moreover, we can apply these techniques column-by-column, using statistics for the whole collection, and achieve substantial reduction in the recorded binary data.

The benefits of compression depend greatly on the nature of the data itself, and come at a non-trivial CPU overhead, but can unlock significant savings and opportunities.

#### Query Optimization

Of course we could also just need to store less information, through query optimization.
This is an evergreen source of improvements that we regularly dip into.
As some algebraic insight allows us to restructure how we derive results, we can simply store less *information*, and consequently store fewer bytes.

### Wrapping Up