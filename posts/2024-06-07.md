
# Understanding Materialize's resource usage

Materialize translates your SQL into dataflows that compute and then incrementally maintain their results.
It does this transparently, from your point of view, except that it consumes (and retains) resources that you need to supply.

To update your query results Materialize needs to remember things for you, and those things need to be readily accessible in order for the updates to be prompt.
We often think of this as "using memory", but it could equally be local disk, or paged to and cached from cloud storage. The location isn't as important as what what we need out of this state: random access.
Materialize's principal ongoing cost is denominated in **indexed bytes**.

Materialize has (to a first approximation) just one stateful construct, which it re-uses throughout: 
the [**arrangement**](http://www.vldb.org/pvldb/vol13/p1793-mcsherry.pdf). An arrangement is an indexed collection of evolving data, maintained in a form that allows random access by key, but also presents as a streaming changelog.
Arrangements are the backbone of both Materialize's indexes and the dataflows that compute and maintain query results.
The costs in indexed bytes for a workload can be accounted to the arrangements that Materialize needs to create to support the views you need to maintain.

This post is an explanation of when (and why) Materialize creates arrangements on your behalf.
By the end, you should understand the `CREATE INDEX` command, how Materialize uses arrangements to maintain indexes for your views, and how you can read out and diagnose extensive memory use.

1. [SQL in Materialize]()
1. [Introducing: arrangements]()
1. [Introducing: dataflows]()

## An introduction to SQL in Materialize

Materialize supports a mostly vanilla flavor of SQL, but some of the concepts come with new implications.
Let's take a quick tour through things that you can do in Materialize, to get us all on the same page.

Materialize has several ways you can introduce data; the most common are [**sources**]() and [**tables**]().
We'll use tables in our examples, as we can turn them on without any external dependencies, but sources should be your go-to answer for high volume CDC replication.
```sql
-- Some facts, and the corresponding dimension data.
CREATE TABLE facts (key1 INT, key2, INT, data0 TEXT);
CREATE TABLE dims1 (key1 INT, data1 TEXT);
CREATE TABLE dims2 (key2 INT, data2 TEXT);
```
Both sources and tables are backed by cloud storage, and even when filled up with data they are generally inexpensive until they are used.
We will ignore their low ongoing cost, and just study the cost of the commands that use them.

Materialize encourages you to express your SQL logic as [**views**](): names given to `SELECT` statements.
Views themselves are not yet live computation, but rather the description of what you would like to see were you to query them.
```sql
-- Enrich facts by the corresponding dimensions.
CREATE VIEW enriched AS
SELECT data0, data1, data2
FROM facts, dims1, dims2
WHERE facts.key1 = dims1.key1
  AND facts.key2 = dims2.key2
```
At this point you could SELECT from `enriched` and Materialize would leap into action to determine the result.
Until you SELECT from it, or otherwise instruct Materialize to act, the view imposes no more cost than writing down the association between the name and the query.

The creation of [indexes]() causes Materialize to leap into action.
Many of you may be familiar with indexes from databases as things you have on tables: they allow you to look up rows by some key columns or expressions.
In Materialize you can also create indexes on *views*: like with tables, these indexes also allow you to look up rows in the result by some key columns.
However, to accomplish this Materialize must both compute and then incrementally maintain the results of the view, as its inputs change.

```sql
-- Compute and continually maintain enriched facts
CREATE INDEX enriched_data1 ON enriched (data1)
CREATE INDEX enriched_data2 ON enriched (data2)
```

These commands will cause Materialize to leap up, grab whatever compute resources you have made available, and get to work.
Moreover, once the work of computation is done, Materialize will *maintain* the results as the input data change.
To efficiently maintain the results it will likely *retain* several of the resources it acquired for the computation.
These retained resources will be arrangements, and will have a cost in terms of indexed bytes.

The `CREATE INDEX` command is the one that causes Materialize to spend resources on your behalf, and it is what we'll try to unpack in this post.

---

**IMPORTANT SIDEBAR**: **Arrangements in Materialize contain the data that they index**.

This is an important and substantial departure from most database *indexes*.
In many conventional databases the index would contain only enough information to *find* the data, which is stored elsewhere.
Given that the "elsewhere" is cloud storage for Materialize, rather than "right over here" as it might be for an OLTP database, our indexes will need to host the data itself to be effective.

The "narrow" indexes of conventional databases can be expressed as an idiom in Materialize.
You could for example use a `facts_key1key2` index as the *primary* index for `facts`, imagining that these two columns form a unique key, and then create views and indexes that provide random access by each of the colums of `facts` back to the unique key pair.
```sql
-- Views that contain the unique key and secondary key.
CREATE VIEW facts_by_key1 AS SELECT key1, key2 FROM facts;
CREATE VIEW facts_by_key2 AS SELECT key1, key2 FROM facts;
CREATE VIEW facts_by_data AS SELECT key1, key2, data0 FROM facts;

-- Indexes on the restrictions, by the non-key columns.
CREATE INDEX facts_by_key1_idx ON facts_by_key1 (key1);
CREATE INDEX facts_by_key2_idx ON facts_by_key2 (key2);
CREATE INDEX facts_by_data_idx ON facts_by_data (data0);
```

Ideally Materialize will reach a point where you don't need to think about this, and we handle the primary and secondary nature of indexes behind the scenes.
For the moment we do not, and it is better to be clear about it, so that you can understand exactly what happens.

---

The `CREATE INDEX` command does two things:
1. It first builds a **dataflow**, to compute and incrementally maintain the subject of the index, 
2. It then builds an **arrangement**, to present the results in an indexed form, randomly accessible by the key expressions of the index.

The first of these (dataflows) uses the second (arrangements) as a fundamental building block.
With that in mind, let's unpack these two actions, but in reverse order.


## Arrangements

The location of nearly all **indexed bytes** in Materialize are in arrangements.
These are data structures that allow random access to row-shaped data by expressions on those rows.
The simplest form of these is an index on a table: an arrangement that requires no dataflow.

Let's create an index on a raw table, `dims1`, by its primary key.

```sql
-- Make random access to dims1 by key fast.
CREATE INDEX dims1_key1 ON dims1 (key1);
```

This index makes the data present in `dims1` available in indexed form.
```sql
-- Goes fast, because `dims1_key1` exists.
SELECT * FROM dims1 WHERE key1 = 37;
```
Indexes will also be useful in making `JOIN` plans more resource efficient, which we'll get to in the dataflow section.

To a first approximation, an index in Materialize requires memory proportional to the current cumulative size of the rows it contains.
Each row takes some amount of space, depending on the type and contents of the columns of table.
Generally, this should track what you expect: `INT`s take at most four bytes, `BIGINT`s take at most eight bytes, `TEXT` takes as much space as you have bytes of text, plus some space to record the length.
In some cases we are able to use less memory, because your data are simpler than they could be.
Generally, the bulk of an arrangement is a snapshot reflecting the current (or nearly so) contents.

The full accounting of resources required by an index is complicated by the fact that the data are changing.
Materialize maintains the *history* of the changes, with enough historical detail to correctly detail the contents at an interval of recent times.
The size of that interval, and the number of changes that occur within the interval, increases the amount of data the arrangement maintainst.

The implementation underlying indexes uses a technique called a [log-structured merge-tree](https://en.wikipedia.org/wiki/Log-structured_merge-tree).
Rather than update data in place, they record a log of changes and roll-up that log into indexed state at various convenient moments.
The effect is that once a table is up and changing, there may be additional overhead to record the changes as they happen, and then to apply them later.
In particular, the application of the updates can require access to the old snapshot as it forms the new one, potentially doubling the indexed bytes requirements.

---

**SUMMARY**: The resource requirements of any arrangement can be understood from the data it has been asked to represent.
The bulk of the data is in a "snapshot" proportional to the size the collection at the beginning of the historical interval the arrangement maintaints.
In addition, any changes from that snapshot until "now" are maintained separately, and are rolled into the snapshot as the historical interval advances.

---

## Dataflows

Dataflows both compute and then maintain the contents of a view.
These dataflows will deposit results in an arrangement, and that arrangement does have a cost that we have just described, but here we will unpack the requirements of the computation and maintenance.

Concretely, here are two views that are differently hard to compute and maintain:
```sql
-- Count the raw facts.
CREATE VIEW foo AS SELECT COUNT(*) FROM facts;

-- Count the enriched facts.
CREATE VIEW foo AS SELECT COUNT(*) FROM enriched;
```

The first view just pulls data from `facts`, determines the number of records, and maintains it as `facts` changes.
As we'll unpack, this is a very easy and efficient thing to do: it requires almost no state itself, and only needs to land the resulting values into an arrangement.

The second view (recall) performs a three-way join between `facts`, `dims1`, and `dims2`.
We must perform the join, rather than just use `facts`, because we may both drop facts (if keys are absent) and repeat facts (if keys are present multiple times).
A join is more complicated to perform than a select, but it is also complicated to *maintain*.
If we introduce a new fact we must determine the dimensions it matches, but also if we modify a dimension table we must update the enriched facts.
Doing this efficiently requires indexed access to `facts`, `dim1`, and `dim2`, potentially as well as some intermediate results.

Materialize reports the total indexed bytes associated with each dataflow.
This information is available in the Console by clicking through `Clusters`, into the cluster hosting your dataflow, then clicking either `Materialized Views` or `Indexes` corresponding to the dataflow you want to investigate.
The information can also be read out of internal tables in the `mz_internal` schema, where there are broken down in greater detail.

### From SQL to Dataflows

As you can imagine, the costs associated with SQL fragments can vary substantially.
Moreover, it can be hard to determine from the source SQL what is actually going to happen, and what costs will be incurred.
Consider the query
```sql
SELECT *
FROM facts, dims1
-- ...
-- Lots of intervening SQL, let's say
-- ...
WHERE facts.key1 = dims1.key1
```
The `FROM` clause creates a join, but it is the `WHERE` clause that unlocks the efficient execution of the join.
These lines aren't necessarily close to each other, and in fact don't even need to be part of the same command when views are stacked on each other.
Additionally, the query optimizer may transform your query to be less expensive, in ways that may not be clear to you ahead of time.

Internally, Materialize uses a different representation of queries that directly corresponds to the work and indexed bytes.
Each join is a single operator, `Join`, and fully spells out the equalities it will use to efficiently implement a potentially complex multi-way join between several inputs.
Each grouping operation (`GROUP BY` combined with aggregate functions in the `SELECT` block) is a single operator, `Reduce`, that identifies the keys and aggregates it needs to use.
This representation will be the first step in understanding the costs of your plan.

The [`EXPLAIN`]() command reveals the operators that Materialize will use for your supplied SQL.
There are several arguments to the command, and increasing detail and fidelity to the implemented plan.
For the moment, we will start with the bare-bones `EXPLAIN` command, which gets us over the hurdle of converting SQL to Materialize operators.

Many of the operators are stateless.
There operators are important for understanding your query, and for example what amount of filtering and projection are applied to your data.
They do not maintain any indexed bytes themselves, though.

### The ArrangeBy operator

The `ArrangeBy` operator indicates that a collection of data must be arranged by some specified key expressions.
It does not alter the data, but only ensures that certain arrangements exist. 

This operator is introduced in support of other operations that require arranged data as inputs, primarily the `Join` operator.

### The Join operator

The `Join` operator has a variable number of input collections, and a number of "equalities" which are used to correlate the rows in each of the inputs.

There are multiple "plans" for a join, each of which indicates an implementation strategy.

*   **Linear**: 
    The linear plan will perform a sequence of binary joins, starting with the first two, then joining the third to the result, then joining the fourth to the result, and so on.
    Each binary join requires its inputs are arranged, and the implementation will create arrangements for each of the intermediate results.
    These arrangements have a cost in **indexed bytes** that depends on the amount of data within each intermediate result.

*   **Delta**: The delta plan will create no new arrangements.
    This plan is only available if sufficient input arrangements exist.
    These arrangements will be visible as `ArrangeBy` operators if they do not otherwise exist.

### The Reduce operator

The `Reduce` operator groups records by some key expression, and applies a number of reduction functions to each group.
The cost of the `Reduce` operator depends on the type of the reductions used, which have three flavors:

1.  **Accumulable**: 
    Reductions that sum things, including `COUNT`, `SUM`, `AVG`, and others, can be maintained in an arrangement that maintains the keys themselves and each of the aggregates.
    The state is independent of the number of records in each group.

2.  **Hierarchical**: 
    Reductions like `MIN` and `MAX` will be implemented using a sequence of reduction operators, which progressively reduce the set of values associated with each key.
    The sequence exists to trade off indexed bytes against update time, where the more stages the more bytes and more robust update time.
    Each operator in the stage will use memory proportional to the current keys and values.
    The trade off between indexed bytes and update time can be managed through [query hints](https://materialize.com/docs/transform-data/optimization/#query-hints).

    In case the input is append-only, a more efficient plan can be used which only maintains the aggregate for each key, rather than all values for all keys.

3.  **Fully general**: 
    Some reductions are an arbitrary function of all input records, and the only strategy we have is to maintain all key and value pairs.
    An example is `jsonb_agg`, which aggregates up all values into a JSON object.
    Not much can be done here other than to maintain all input keys and values, and any change to the input requires a full re-evaluation of the reduction for the affected key.

All SQL reductions can be prefixed with a `DISTINCT` modifier, which has the effect of introducing a stateful reduction before the aggregation to effect the distinct.
This reduction will use indexed bytes proportional to both the key and value, as it must distinguish between the first and second (or more) times each value is associated with a key.

### The TopK operator

The `TopK` operator groups records by some key, and applies an `OFFSET` and a `LIMIT` to each group, producing a subset of the input records for each key group.

The operator is implemented by a sequence of stateful dataflow operators, each performing some amount of the `LIMIT` work that needs to be accomplished.
This sequence exists to avoid biting off too much work at once, and it trades off more indexed bytes against more time to update results.
One can supply hints about the [expected group size](https://materialize.com/docs/transform-data/patterns/top-k/#group-size-hints) to guide Materialize in trading off indexed bytes against robust response times.

Each of the stateful dataflow operators maintains the data input to the `TopK` operator, minus some amount of "already removed" records.
The operators must maintain the *input*, not just the output, because it they must correctly update in the face of *retractions* of arbitrary input data.

If the input to the operator is certainly append-only, meaning they are certain to contain no retractions, Materialize is able to use a more efficient implementation that maintains indexed bytes proportional to the output rather than the input.

## Other commands