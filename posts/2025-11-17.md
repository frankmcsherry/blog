# Accumulating mid-Join

A quick note about an optimization to datatoad's join implementation that I think shows off a potentially non-obvious type of optimization.

Oversimplifying dramatically, imagine we have binary collections `A` and `B` represented as maps from the first column to the second column.
```
A: Key -> Val0
B: Key -> Val1
```
To produce the join of `A` and `B` via `Key`, it's a matter of enumerating all pairs `(Val0, Val1)` for each matching `Key`.

We often find ourselves wanting to join `A` and `Sum_i Bi`, as above using `Key`
This spills out of our use of log-structured merge-tree datastructures, which rather than accumulate `B` in place as it changes instead amortizes the aggregation using logarithmically many terms that decrease exponentially in size.
The `A` term is not a sum, as we end up needing to flatten it to judge whether there are novel facts (in Datalog) or non-zero accumulation (in differential dataflow) which would cause us to continue iteration / computation.

Looking at `A ⋈ Sub_i Bi` it is natural to think about where we might want to perform the summation.
We could perform it as written, on the `Bi` terms themselves.
Alternately, we could un-distribute the join into the sum, and accumulate the `A ⋈ Bi` output terms.
The first approach may ruin the amortization argument (as `A` may be much smaller than the `Bi`), and the second approach may perform substantial work after the join inflates the inputs to their product of values.

There is at least one other approach, which starts by viewing the join operator as two steps:
1. Find the keys in common to A and B.
2. For each key, enumerate matching pairs of values.

As it turns out, we'll be able to do the accumulation in between steps 1. and 2., and thereby avoid the perils of the two approaches above.

Let's unpack the two steps with a third between the two that we usually wouldn't perform explicitly.
1. Find the keys in common to A and B.
2. Restrict each of A and B to these common keys. **(NEW!)**
3. For each key, enumerate matching pairs of values.

Having found common keys, but before crossing their values, we can speak of the subset of the two inputs that will produce joined results.
Each are the "semijoins" of the one input with the key column of the other.
These semijoins are each subsets of the inputs, only on intersecting keys, and they are before we expand out the product of values.
They are excellent terms to accumulate.

Writing `Ak ⋊ Bi` (⋊ is right semijoin) for "entries in `Bi` whose key is in `A`", we'll write our summation as
```
A ⋈ Sum_i (Ak ⋊ Bi)
```
We identify the relevant subset of each `Bi`, accumulate those terms, and then complete the join with `A`.

Although it may look like we have more total joins and semijoins, when we break them down into the inner steps of key matching (1.) and then expansion (3.) we see that it's the same work as with just the one join.
The only modification from the original join implementation is the summation of the intermediate terms, which we can skip if there is only one term (making this a strict improvement on the previous implementation).

As an added benefit, by accumulating the semijoins we only need to make one pass through the values of `A`, rather than one for each `Bi` term.
If the `Bi` contain duplicates, or otherwise reduce down, the total amount of step (3.) work can also be reduced (never the case for datatoad, but potentially for differential dataflow).

The total performance benefits will depend on details of the problem at hand.
It will never be an asymptotic improvement because we still have to do the work of producing the join output, and our second approach (accumulate outputs) would do work proportionate to this size anyhow.
So it seems at most a constant factor, and likely not an enormous one (adding terms is a sequential scan, vs forming the output of the cross join, which involves sorting the way we do it).

It's about half a second improvement on the GALEN problem I've been looking at (from 14.25s to 13.75s, plus or minus).

---

Playing more with columnar representations, I'm finding more and more opportunities to reshape computation.

Breaking joins into the three steps we did makes less sense going row-at-a-time, or alternately doing so is tantamount to introducing columnar thinking (separating the actions on the key and val columns).
But also, these three steps are closer to what the computer actually needs to do, and the intermediate data it needs to identify, than is the single step "join".
While viewing a query as joins helps with its algebraic restructuring, decomposing the joins into finer grained operations helps with restructuring its implementation.

You can see this a bit in differential dataflow, where operators like `join` are broken into
1. Form an arrangement of each of the inputs (updates grouped and sorted by key),
2. Perform the join logic from the provided input arrangements.

It turns out that the same arrangements are used by multiple operators, and moreover are often their formation and maintenance are the expensive part of the task.
By peering closer at the hunks of work the computer will perform in support of complex operations, we can find reusable structure.
