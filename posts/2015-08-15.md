## Sorting out graph layout

We've seen a few papers recently looking at the problem of processing *very* large graphs; graphs that are so large, the cost of sorting their edges to fit the adjacency-list representation is prohibitive.

Specifically, I'm thinking of [X-Stream](http://infoscience.epfl.ch/record/188535), whose abstract indicates that its novelty lies at least partly in

> ... streaming completely unordered edge lists rather than performing random access.

This statement always weirded me out a bit, because I think of unordered edge lists as inducing a lot of random access: if you don't have any locality in the edge endpoints, you stab around in memory pretty much arbitrarily.

When X-Stream was presented at SOSP 2013, I asked author Amitabha Roy how come the time to process the unordered edges on their system was larger than the time to process ordered edges on my laptop (with all of its "random accesses").

Specifically, [I asked](https://www.google.com/moderator/#15/e=215b85&t=215b85.47)

> What overheads does X-Stream introduce, and at what scales does X-Stream outperform more specialized approaches? E.g., PageRank iterations on my laptop (40 lines single threaded C#) takes 64s, versus 75s for 16 cores in X-Stream

Clearly, even at this point I was already well on my way down the path of making friends and influencing people.

His answer, and the motivation for today's post, was:

> X-Stream trades pre-processing overheads for inefficiency at runtime. We estimate it would take about 600 seconds to sort the Twitter edge list [1.48B edges] into the format you need for your implementation. Your single core implementation therefore takes around 664 seconds as compared to our 75 seconds. This is a very good question though and underlines the need for proper baselines and a comparative study of graph processing systems.

This got read aloud, so the whole audience got to hear just how silly I was. Clearly sorting is *expensive*. Geez. *Six hundred seconds* to sort those edges. How awkward for me, right?

---

If you've read any of my earlier post on single-threaded graph processing, you might also be saying "64 seconds for PageRank?!? But, you know you can do that in about 5 seconds...".

As it turns out, I wasn't the only one off by an order of magnitude.

### Sorting things out

Let's take a tour through sorting algorithms, starting from the very easy, up to the slightly more intelligent, and on to what you find to be the state of the art when you actually look around at the work others are doing.

#### Calling `sort`

I'm using [Rust](http://rust-lang.org), and Rust has a `sort` method on mutable slices of types `T: Ord`. The implementation of `sort` uses merge sort and allocate twice the size of the original slice, so it's going to be our straw man.

Because it does all these allocations, I'm not really able to try it out with the 1.48B edges, because that would be 36GB or so. Instead, I'll sort some smaller things, and we will already be disappointed, which is the goal of this section anyhow.

We are going to sort `1 << 29` elements, because that is what fits in my  laptop's memory, and coincidentally that is the largest number of elements the X-Stream paper sorted when evaluating the cost of sorting (their Figure 18).

We will be able to compare out results with their results! SCIENCE!

I'm going to sort random `(u32, u32)` data, then flip the data around and do it again, ten times:

```rust
extern crate rand;

use rand::{Rng, SeedableRng, StdRng};

fn main() {
    let mut vector = vec![];
    let seed: &[_] = &[1, 2, 3, 4];
    let mut rng: StdRng = SeedableRng::from_seed(seed);
    for i in 0u32..(1 << 29) {
        vector.push((rng.next_u32(), rng.next_u32()));
    }
    for _ in 0..10 {
        for index in 0..vector.len() {
            let (x,y) = vector[index];
            vector[index] = (y,x);
        }
        vector.sort();
    }
}
```

Here is what I got:

```
Echidnatron% time cargo run
     Running `target/release/merge`
cargo run 1034.41s user 204.27s system 90% cpu 22:56.07 total
```

That is about 1376 seconds, 9 seconds of which are data generation, to do ten iterations, so 128.6 seconds per iteration.

By comparison, in X-Stream's Figure 18, at `1 << 29` edges `gcc`'s sort doesn't finish in 500 seconds (it looks like it is heading for 1000 seconds), and counting sort clocks in at a little over 250 seconds.

It is totally possible I don't understand what they mean by "RMAT scale factor 25". I do think I have the right number of nodes and edges, though I did not follow the "do some random shit ... power laws!" RMAT methodology). Also my laptop totally wasn't available back in 2013 when they did these measurements, but there seems to be a bit of a discrepancy here. Maybe my laptop is 8x faster than whatever they were running on, but...

On the other hand, this graph has about one-third of the edges of the graph I asked about in the question, and if you multiply this by three you get almost 400 seconds, plus some slop to account for a few years passing... Maybe 600 seconds is the right ballpark?

Rust's `sort` is quite primitive. Isn't there anything better?

#### Counting sort

I was going to tell you about counting sort, because that is what X-Stream has as their faster sort comparison, but it is dumb. I'm going to save you some time and show you what they should have used.

The main reason counting sort is dumb is that it has horrible memory locality. Each time you see an edge `(src, dst)` you go and do a random access based on `src`. Actually, you do that three times, first into an array as large as there are nodes, then again into that array in a second scan, and then into an array as large as there are edges.

These random accesses are exactly what you are trying to avoid by sorting the edges; taking them on the chin three times isn't helpful.

If you try hard, you can do `1 << 29` edges in under 200ns / edge, which would be about 100 seconds all told. That is a bit better than `sort` above, and while you can improve it with pre-fetching, we aren't going to go in that direction.

Instead, we are going to go in the direction of *awesome*.

### Radix sort

Radix sort is a very cool not-comparison-based sorting algorithm. It sorts records by treating each record as a sequence of bytes, and making sure the records end up sorted by their byte sequence. This is just great for sorting things like graph edges, which we think of as just `([u8; 4], u32)`.

Radix sorting works by repeatedly partitioning a sequence of elements into some number of disjoint subsequences. That number is usually 256, and the partitioning is usually based on looking at a byte in each element. After partitioning, you put the subsequences back together in order, and repeat with the next byte.

The resulting sequence is ordered by the byte you last looked at, and records with the same byte are ordered by the second to last byte you examined, etc. Once you've looked at all bytes, the sequence is ordered by the binary representation of each element, assuming you looked at the bytes from least significant to most significant.

Here is a quick sketch of what each pass of radix sort might look like, if we wanted to partition based on some `get_byte` method I just made up.

```rust
for element in input {
    output[get_byte(&element)].push(element);
}
```

Pretty easy, huh?

You do four of these sequential scans, and in each you write sequentially to one of 256 locations. Four passes, no random access. This is great. I don't even see a `log n` there, do you?

The great thing about radix sort is that in one scan through the data does the equivalent of 8 comparisons which would normally take a bunch of conditional logic. Instead, it just pulled out a byte and used it to look up an address.

The downside to radix sort is that is just looks at bytes. If you had some deep and semantically meaningful ordering defined over your type, that's great but were sorting by its bytes.

#### Improvements

Ok, maybe the above algorithm is a little too simple. The code above has the sketchy property that each of the `output[byte]` lists need to re-allocate as they grow, and that is a waste of everyone's time.

A different approach fills buffers of some small size (e.g. 1024 elements), and enqueues them instead. These buffers can be recycled each round, like so:

```rust
for list in input {
    for element in list.drain(..) {
        let byte = get_byte(&element);
        buffer[byte].push(element);
        if buffer[byte].len() == 1024 {
            let free = stash.pop().unwrap_or_else(|| Vec::with_capacity(1024));
            let full = mem::replace(&mut buffer[byte], free);
            output[byte].push(full);
        }
    }
    stash.push(list);
}
```

Ah, that looks better. We just need to put a loop around it, add an `unsafe { }`, erase a few bounds checks, and we are good to go!

```
Echidnatron% cargo run --release
     Running `target/release/radix`
data generated 6.605743866995908
sorted in 9.62945753801614
sorted in 7.738075986970216
sorted in 7.423567182035185
sorted in 7.664827859029174
sorted in 7.620034374995157
sorted in 7.483836957952008
sorted in 7.485334731056355
sorted in 7.414655723958276
sorted in 7.516964436043054
sorted in 7.4602396480040625
```

Oh my. Is that 7.5 seconds to sort `1 << 29` edges?

Well, that sure is faster than 250 seconds, or whatever was floated in the X-Stream paper as the faster way to sort edge data. I wonder if their conclusions still hold up given that sorting is now about 30x faster than they thought it was.

### About those conclusions ...

Hey let's test the main thesis of the X-Stream paper with a simple experiment.

Their claim is that for many graph processing tasks it is faster to use unordered edge data than it is to sort the edge data and then do the task. We are going to try and reproduce this for a simple task: computing the out-degrees.

Computing out-degrees is about as simple as it gets in graph processing land: we have a bunch of edges, and we want to know, for each source, how many edges are there from it to other nodes?

We first make some random graph data:

```rust
let mut nodes = vec![0; (1 << 25)];
let mut edges = vec![];
for _ in 0..(16 * nodes.len()) {
    vector.push((rng.next_u32() % (nodes.len() as u32),
                (rng.next_u32() % (nodes.len() as u32))));
}
```

We want to compute the degrees two ways: unordered, and after sorting. We'll do it ten times each, to warm up memory and stuff.

First, using random access:
```rust
let start = time::precise_time_s();
for i in 0..10 {
    for edge in edges.iter() {
        nodes[edge.0 as usize] += 1;
    }
}
println!("random {}", (time::precise_time_s() - start) / 10.0);
```

Next we'll do it by sorting the edges from scratch, each of the ten iterations.

To reiterate: each iteration, the data are sorted again, from the unsorted data.

```rust
let start = time::precise_time_s();
let mut sorter = RadixSorter::new();
for _ in 0..10 {
    for &edge in &edges {
        sorter.push(edge, &|x| x.0);
    }
    let output = sorter.finish(&|x| x.0);
    for edge in output.iter().flat_map(|x| x.iter()) {
        nodes[edge.0 as usize] += 1;
    }

    sorter.recycle(output);
}
println!("sorted {}", (time::precise_time_s() - start) / 10.0);
```

Any bets?

```
Echidnatron% cargo run --release
     Running `target/release/xstream`
random access 10.483409735094756
sorted 8.601547808106989
```

In less time than it takes to do the computation using unsorted data, we have not only finished the computation, but we actually sorted the damned edges for you.

You're welcome.

If you wanted to do an iterative graph computation like PageRank, it would literally be faster to sort the edges from scratch each and every iteration, than to do it using unsorted edges. If you want to do graph computation, please sort your edges.

Actually, you know what: if you want to do big data computation, please sort your records. Stop talking sass about how Hadoop sorts things it doesn't need to, read some papers, run some tests, and then sort your damned data. Or at least run faster than me when I sort your data for you.

(Aside: Who's the big data system that radix sorts *all* its records? ([Differential dataflow](http://github.com/frankmcsherry/differential-dataflow)!) You're damn right.)

### Wait, seriously?

Sorting is faster than blind random updates, because radix sorting specifically has great locality of reference, and just does four passes over the data. Each of those passes is much more than four times faster than a some random read out of cold, dark main memory.

This isn't a fluke, and awkwardly, it only gets *worse* the larger the data gets. For massive graphs this is an increasingly bad idea. Radix sorting, with its excellent locality, has a running time that grows linearly with the number of edges.

Here is a picture where I computed the average number of nanoseconds per edge to do degree counting using unsorted data, and sorting the data using radix sort and then doing degree counting. The number of nodes goes from 2^16 to 2^25, and the number of edges is 16x the number of nodes.

![Counting degrees, badly vs radix](https://github.com/frankmcsherry/blog/blob/master/assets/degrees.png)

I did this by putting a loop around the bits of code I wrote up above.

Then I sorted a random graph with 65M nodes and 1.5B edges, like the `twitter_rv` graph, in 50 seconds on one core, improving on prior estimates of 600 seconds (and, when you add 15 seconds for an ordered PageRank step, slightly edging X-Stream's 75s on 16 cores).

According to their web page, "X-Stream is a large and complex system, currently in the region of 15K lines of code and growing fast". I couldn't figure out how to make a pull request, but I have about 100 lines of radix sort in Rust that might help.

### Conclusions

Here is where you might expect that I would make some snarky, unkind comments about how not everyone is right. Whatever; I was just as wrong (relatively) about how long pagerank should take as the X-Stream folks seem to have been about sorting. Who knows, maybe I am the one who is wrong about sorting too.

Here is a bigger problem: in the big data processing session of one of the top computer systems conferences, no one (speaker, audience, me) seemed to know how long it should take to sort some numbers. I mean, we laugh at those database people who can't unstick their capslock key, but at least they have this figured out.
