# Columnar Worst-Case Optimal Joins

The [datatoad](https://github.com/frankmcsherry/datatoad/tree/main) project uses vanilla binary equijoins to drive its bottom-up derivations.
There's [a PR up](https://github.com/frankmcsherry/datatoad/pull/17) that implements an alternate approach to multi-way joins that follows some [prior work on complex joins](https://www.vldb.org/pvldb/vol11/p691-ammar.pdf) that implements [Generic Join](https://en.wikipedia.org/wiki/Worst-case_optimal_join_algorithm) in a column-by-column style.
Along the way, we'll seemingly introduce a column-oriented form of [Free Join](https://dl.acm.org/doi/pdf/10.1145/3589295), though we'll have to wait and see whether this is anything more than a change in perspective, perhaps just me coming to understand something everyone else already knew.

## Background: Multiway Joins in Datalog

Datalog is a language that makes it easy to specify complex joins that must be continually re-evaluated as new facts are discovered.
The two properties of **complex joins** and **continual re-evaluation** introduce challenges that make conventional implementations inappropriate.
We'll say a bit more about them once we have an example in hand.

Here's an example that I spend a bunch of time with, from [Liagouris et al](https://www.vldb.org/pvldb/vol7/p1993-liagouris.pdf), about fleshing out medical ontologies.
Don't worry too much about what the rules mean (I sure don't), but take a peek:
```
p(?x,?z)    :- p(?x,?y), p(?y,?z) .
q(?x,?r,?z) :- p(?x,?y), q(?y,?r,?z) .
q(?x,?q,?z) :- q(?x,?r,?z), s(?r,?q) .
p(?x,?z)    :- p(?y,?w), u(?w,?r,?z), q(?x,?r,?y) .
p(?x,?z)    :- c(?y,?w,?z), p(?x,?w), p(?x,?y) .
q(?x,?e,?o) :- q(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
```

Each of the lines is a "rule", each of which is divided by the `:-` into a "head" and a "body".
Here the heads are each a single "atom", and the bodies are lists of "atoms", where an atom names a relation (here tersely represented by individual letters) and lists some symbols for each of its colunms.
The symbols are "terms", and each is either a variable (e.g. `?foo`) or a literal (none in this example, but e.g. `"potato"`).
Terms can be re-used across multiple atoms, and each time this happens it introduces an equality constraint: the term can take on only one value, and it must be the same in both locations.


The meaning of a rule is that for any assignment of the terms that satisfy the body atoms, the head atoms are similarly satisfied with the same assignment of terms.
For example,
```
p(?x,?z)    :- p(?x,?y), p(?y,?z) .
```
means that for any `?y` that is present in both the first and second columns of `p`, we can add to `p` all pairs `(?x, ?z)` that are respectively the first and second attributes matched with `?y` in `p`.

The rules get harder to read aloud when they have more atoms and terms.
This rule, for example
```
q(?x,?e,?o) :- q(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
```
is made awkward by having more than two atoms in the body, the same relation named for multiple atoms, and more terms than I can keep track of (six).
Unfortunately, it ends up being the more complicated rules where we spend the most of our time, so we'll need to understand these better.

### Conventional Approaches

Starting from relations `p`, `q`, and the various others, we can certain evaluate these rules using conventional join planning techniques.
As an example, we can go left-to-right, finding terms in common between each pair of atoms, and performing a relational equijoin to find all assignments to their terms that satisfy the equivalence constraints.

In our example from above, we could extract the first pair of atoms into a new rule that finds satisfying values of terms they share.
We can then slot that in for their pair in the original rule, giving something like this:
```
t(?x,?z,?u,?e) :- q(?x,?y,?z), r(?y,?u,?e) .
q(?x,?e,?o) :- t(?x,?z,?u,?e), q(?z,?u,?o) .
```
Each of these rules now has two atoms in the body, and you can perhaps more easily see how a binary join between `q` and `r` (for the first rule) or `t` and `q` (for the second rule) could produce the right results.
We know how to efficiently binary join collections, and have been doing so for years, so why not just do that?
There are at least two reasons.

### Reason 1: Cyclic Joins

Our complicated rule has some pedagogically helpful structure.
The terms in the rule come in one of two flavors:
1. Some terms appear exactly once in the body, and also in the head.
2. Some terms appear twice in the body, and not in the head.

We could rewrite the rule into two rules: one that first looks for the terms that appear twice in the body, which are the "constraints", and a second that uses these bindings to "look up" the values of output terms.
```
t(?u,?y,?z) :- q(_,?y,?z), r(?y,?u,_), q(?z,?u,_) .
q(?x,?e,?o) :- t(?u,?y,?z), q(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
```

This probably looks more complicated, but we've broken the rule into two fundamentally distinct steps.
The first rule needs to resolve some non-trivial constraints to form `t`, and the second rule starts from `t` and simply "looks up" values of `?x`, `?e`, and `?o` for the output, with no complex constraints to resolve.
The second rule is indeed easy, and the first rule is the challenge.

The first rule is "cyclic" in that chasing the term constraints through atoms does not result in a tree.
It's more nuanced than this, but were it just a tree there are nice algorithms that spend time at most proportional to the size of the input plus the output.
That's would be good as it gets, so we'd be pretty happy.
But it's not the case, and in particular conventional joins (e.g. binary equijoins) can perform asymptotically more work than could appear in the output.

The good news is that there are better join algorithms, and we'll implement one of them, but we'll want to keep our eyes on these cyclic components of rules.

### Reason 2: Continual Re-evaluation

The second challenge for conventional join reasoning comes from Datalog's inherently iterative nature.
We will repeatedly derive new facts, for example new assignments to `q(?x,?e,?o)` in the problematic rule above, and need to then chase down any further consequences of these new facts.
Each round of derivation may produce a limited number of new facts, and we may need a large number of rounds to derive all facts, and any approach that proposes to re-consider the input collections in their entirety is at a serious disadvantage, if not off the table altogether.

The "nice algorithms" referenced above, for example, start with a pass over all of the input data to first clean it up to make the rest of the query more efficient.
It's a great thing to do if you just need this one join, and are starting from scratch, as you'll need to read the input anyhow.
If this is the 100th iteration and only seven facts have been added, it's not as acceptable to reprocess the entire input.

What this means for us is that we'll break apart our rule evaluation into two types of atoms:
1. base atoms: the current accumulated facts for an atom in the rule,
2. delta atoms: the facts derived in the most recent round for an atom.

We will rewrite our rules to be in terms of base and delta atoms, and restrict ourselves to only interacting with base atoms through indexes.
We will permit ourselves the ability to shuffle and otherwise revisit all of the facts in delta atoms, and we'll use this manipulation to match up to existing indexed forms of the base atoms.

If we denote delta atoms by putting a `d` at the beginning of their name, we can rewrite rules like
```
q(?x,?e,?o) :- q(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
```
into a wordier looking set of rules:
```
dq(?x,?e,?o) :- dq(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
dq(?x,?e,?o) :- q(?x,?y,?z), dr(?y,?u,?e), q(?z,?u,?o) .
dq(?x,?e,?o) :- q(?x,?y,?z), r(?y,?u,?e), dq(?z,?u,?o) .
```
These rules say that to find the changes to the head, `dq(?x,?e,?o)`, there are three potential derivations, each containing one delta term.
If a new instance of the head would be found, it must be due to some new fact in one of the atoms, and for each of the atoms its new facts interact with the full set of facts in other atoms (not just the new facts).

Some light but evocative rewriting gives us the equivalent rules
```
dq(?x,?e,?o) :- dq(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
dq(?x,?e,?o) :- dr(?y,?u,?e), q(?x,?y,?z), q(?z,?u,?o) .
dq(?x,?e,?o) :- dq(?z,?u,?o), q(?x,?y,?z), r(?y,?u,?e) .
```
The rules now all start with a `d` atom, and one can imagine how going from left to right one could continually shuffle delta atoms and join with appropriately indexed base atoms.
Taking just the first rule,
```
dq(?x,?e,?o) :- dq(?x,?y,?z), r(?y,?u,?e), q(?z,?u,?o) .
```
we can maintain an index on `r` by `?y` its first coordinate, which will allow us to efficiently join `dq(?x,?y,?z)` and find matches.
We can also maintain an index on `q` by `(?z,?u)` which would then allow us to join the resulting intermediate delta atom with the next base atom.
We can evaluate the rule directly manipulating only delta atoms, and using indexed access into base atoms.
The process should take time proportional to the size of the delta atoms derived, and be otherwise independent of the size of the base atoms.

Of course, doing the joins left-to-right with binary joins runs afoul of Reason 1, so we'll want to avoid that.

## A General Join Plan

Datatoad's rule plans include one inner plan for each body atom, to handle the updates to that atom as a delta atom.
The multiple inner plans unioned together update the head of the rule in response to each change that could occur.
The inner plans are what we are going to generalize.

Conventional "linear" binary join plans amount to repeatedly selecting a "next atom" and joining using all terms in scope, and introducing all terms that are new to the atom.
By the end of the process we've heard from every atom about every term it constrains, simultaneously.
This ensures that our collection of values for the terms satisfies all of the constraints imposed by the atoms.

One the other hand, one instance of worst-case optimal joins (not all of them) amounts to to repeatedly selecting a "next term" and inviting all atoms that reference this term to propose and constrain the values we might associate with each of the bindings to terms that are already in scope.
Exactly how this happens hasn't been described yet (it's in the paper linked above), but having done this for all terms it means we've heard from all atoms about all of their terms, simultaneously, and like the conventional plans this ensures that our bindings to terms satisfy all constraints.

To unify these two approaches, we just think about a sequence of steps each of which identify some set of atoms and some set of terms, and invites those atoms to propose and constrain those terms.
When we choose single atoms and all of their terms, we get the conventional approach.
When we choose single terms and all of their atoms, we get a worst-case optimal approach.
If we mix and match, we have the ability to better than either in isolation.
Let's do that.

Our main requirement is that by the end of the sequence, we have heard from each atom about all of its terms at the same time.

There are a few different strategies you could use here, and we follow a few simple rules for the moment.
Informally, starting from the terms of the delta atom, we repeatedly chose as a next term the one present in the most atoms that intersect the currently bound terms.
Having ordered the terms, we then collapse runs of stages that involve the same single atom.
There is certainly more advanced and robust thinking to do here, but this has been a good enough start.

## A Columnar WCO Join Implementation

The next thing to spell out is how we handle stages of these plans, and in particular what we do when there is more than one atom.
A stage with a single atom, recall, ends up being just a conventional binary join between the current delta facts and an index on the base atom.

Borrowing liberally from [prior work](https://www.vldb.org/pvldb/vol11/p691-ammar.pdf), the scheme is to develop the delta facts on an initial set of terms, into the set of delta facts on those terms extended by one, at which point the process can repeat.
For any number of base atoms, the algorithm does three steps:
1.  First, for each base atom determine for each delta fact the number of distinct values for the next term the base atom would propose if directly joined.
    Just the number, not the values themselves.
    For each fact, retain the least number and the identity of the corresponding atom.
2.  Next, partition the delta facts into one part for each base atom, so that each fact is associated with the atom that would propose the fewest distinct values for the next term.
    Actually do the joins, to propose terms from each of these base atoms, and merge the results.
3.  Last, semijoin (intersect) the delta proposals with each of the base atoms, so that the surviving delta proposals are only those present in all base atoms.
    Produce the results as the set of delta facts for the next round.

Fortunately, we have the tools in place in datatoad to make this manageable if not entirely pleasant.
The code is closer to array programming (in style, if not terseness) than relational programming, and it currently leaves quite a bit of performance on the floor.

At the moment, a run of the six rules at the top of the post on the GALEN dataset takes
```
17.648980625s
```
If you've been following along, this is slower than the best numbers we have been working with for non-WCO joins, though they have been very carefully planned.
The best performance is for this query that means to compute the same quantity, but with more explicit sequencing of operations:
```
p_10(?y, ?x)      :- p(?x,?y).
q_120(?b, ?c, ?a) :- q(?a,?b,?c).
p(?x,?z)          :- p_10(?ay,?x), p(?ay,?z).
q(?x,?r,?z)       :- p_10(?ay,?x), q(?ay,?r,?z).
q(?x,?q,?bz)      :- q_120(?ar,?bz,?x), s(?ar,?q).
ir4(?r,?y,?z)     :- p_10(?aw,?y), u(?aw,?r,?z).
p(?x,?z)          :- ir4(?ar,?ay,?z), q_120(?ar,?ay,?x).
ir5(?x,?y,?z)     :- c(?y,?aw,?z), p_10(?aw,?x).
p(?x,?z)          :- p(?x,?y), ir5(?x,?y,?z).
ir6(?bz,?u,?x,?e) :- q_120(?ay,?bz,?x), r(?ay,?u,?e).
q(?cx,?e,?o)      :- ir6(?az,?bu,?cx,?e), q(?az,?bu,?o).
```
This plan takes `14.730949167s` though it dips down closer to 13.75s with full attention paid to it (e.g. some diagnostic work that is currently live instead disabled).

However, this plan was chosen with very specific knowledge of the data distribution.
If you choose an arbitrary plan, for example disabling the multi-atom stages and just performing single binary joins for each delta atom, you end up with a much larger running time.
At least I assume, on account of it is still running.
The first round of derivation took `160.180578125s`, the second round `57.633599792s`, and the third round is still going at about a 40GB footprint. (*Ed*: I terminated it after half an hour; the third round never completed.)

The binary join plans are very fragile in this sense.
The worst-case optimal algorithms may be slower, or potentially faster once I tidy things up, but the main thing they are from my point of view is *robust*.
You don't have to worry as much about them breaking catastrophically when you don't hold them right.
Which is great news because users are rarely as careful as you are when trying out your systems.
It's much easier for an author to hold something correctly, at least for as long as it takes to write the implementation section of a paper, than for a user whose goals are to get in and out as quickly as possible.

### Next Steps

At the moment the WCO joins are slower than the best binary joins.
There's no fundamental reason that they should be faster: they do more work as part of an insurance policy, and if you don't need it then it's just dead weight.
At the same time, their optimizations can be helpful and do lead to strict performance improvements in some cases.

My next steps are to tighten up the implementation, to bring the WCO time more into line with the best hand-planned conventional approach.

If you look at profiles of the two implementations on corresponding plans, most of the difference seems to be attributable to waste, but waste that is newly introduce by the more general approach.
For example, when manually planning everything as binary joins with baked-in attribute orders, the planner never needs to think about which term order to use as a layout as it moves delta facts through a pipeline.
Consequently, it .. does a bad job, because I haven't implemented careful thought there. About 3s of the time is spent shuffling the initial delta terms before their first join stage, to put them in an order that matches the atoms they'll join, which is a cost that does not occur in the manually planned version.
But it's also totally avoidable, by having the planner think more intentionally about this and choose an order that should work out well (and .. extend the plans to make sure this can be expressed).

There is some amount of overhead that does feel bad; the `(count, index)` pair we use for deciding which atom to use for each fact gets treated as its own column, even though it is 1:1 with the last column of our facts, and an ideal implementation would stitch its data into that last column somehow to be laid out with it.
There is specialized sorting for `[u8; 4]` data, but not yet for the `[u8; 2]` data that the count and index use (we use the log of the count, so a byte ends up fine).
I figure no need to get stressed about this until the rest of the fat gets trimmed off, and then if it is a large enough fraction of what remains it would be worth looking at again.

Broad strokes, it does *feel* like the potential overhead of WCO joins show up in the pre-work for a large join, where you are resolving a dense core of constraints before expanding outwards.
Additional constant factors on these reduced problems, in order to avoid asymptotic blow-ups in later stages, feels like it is often a win without much of a cost.
I'll need to tidy things up more to make that point with datatoad, though.
