## Consistency, Correctness, and Completeness in Materialize

Consistency is one facet of Materialize's "Trust" pillar, the others being responsiveness and freshness.
It turns out that being super responsive and ultra fresh doesn't amount to much if the results don't make any sense.
Consistency speaks to Materialize *appearing* to simply process commands and events in the order they happen in the real world.
The reality is that no scalable data processor does anything of the sort, but responsible ones don't make that your problem.

Many popular systems ultimately have weak consistency properties.
We've discussed in [our product principles post]() how caches and bespoke microservices are one way to get both responsiveness and freshness, but at the expense of consistency.
Even consistent platforms, like some stream processors and data warehouses, often end up wrapped in caches and serving layers for operational work, and their consistency properties largely go out the window at this point.
Consistency is fundamentally a property of your entire stack, and it does not result only from using consistent components.

Materialize is a responsible platform, and opts you in to the strongest consistency guarantees we know of: [strict serializability](https://jepsen.io/consistency/models/strict-serializable).
Although powerful, these guarantees needs to be extended from command-response operation (pull) to streaming operation (push), as Materialize provides both.
In this post we will unpack Materialize's consistency guarantees, show them happening in a [playground environment](https://materialize.com/register/), and help you exercise the consistency properties of your existing platforms.


### Consistency a la Databases

Ironically perhaps, the term "consistency" means many different things to folks in the databases, distributed systems, and big data spaces.
For a helpful introduction I recommend [the Jepsen page on consistency models](https://jepsen.io/consistency).
The tl;dr there is that [strict serializable](https://jepsen.io/consistency/models/strict-serializable) is what you wish were the case: all interactions are applied in an order that tracks the order they happened in the real world.
The other, weaker models introduce semantic anomalies in the interest of avoiding performance anomalies (up to and including database unavailability).
That doesn't mean the other models are bad, but they are certainly spookier and require more expertise on your part.

Materialize supports both [strict serializable]((https://jepsen.io/consistency/models/strict-serializable)) and [serializable](https://jepsen.io/consistency/models/serializable) modes.
Serializability still requires interactions be applied in some order, but the order doesn't need to match the real world;
for example, you could be served stale results in order to see them faster than if you waited for the results to catch up to their fresh inputs.
We start you off with strict serializability so that you aren't surprised by the apparent mis-orderings of (non-strict) serializability, and then teach you about the latter if you believe you need to squeeze more performance out of Materialize and can absorb the potential confusion.

However, definitions like strict serializability and serializability only apply to systems that accept commands and provide responses.
There are other dimensions to consistency as we move into the world of streamed inputs, maintained views, and streamed outputs.

### Consistency in Materialize

Although Materialize fits the mold of an interactive SQL database, and provides the guarantees of one, it has additional streaming touchpoints.
Input data can be provided by external sources like Kafka and Postgres, which do not "transact" against Materialize.
Materialized views should be kept always up to date, as if they are refreshed instantaneously on each data update.
Output data can be provided to external sinks like Kafka, as streams of events rather than sequences of transactions.
We need to speak clearly about how Materialize's consistency guarantees integrate with these features.

Although things sound like they might be about to get more complicated, I think they get easier by getting more *specific* about how we maintain consistency in Materialize.

Materialize uses a concurrency control mechanism called [Virtual Time](https://materialize.com/blog/virtual-time-consistency-scalability/).
Every command and data update get assigned a virtual timestamp, and then Materialize applies these operations in the order of these timestamps. 
Although there is some subtlety to how we *assign* the timestamps to operations, once that is done the system behaves in what we think is an largely unsurprising and thoroughly consistent manner.
Not only will Materialize behave as if all operations happen in *some* order, as required by serializability, *we can just show you what that order is*.

### Consistency with Input Sources

Materialize draws streamed input data from external sources, like Kafka and PostgreSQL.
Ideally, Materialize would assign timestamps to updates that exactly track the moments of change in the upstream data.
In practice, these sources are often insufficiently specific about their changes, and Materialize instead "reclocks" their sequence of states into its virtual time.
When it does so, it assigns timestamps that aim to be consistent with the source itself.

Most streamed sources have a notion of order, in some cases a total order like PostgreSQL's replication log, and in some cases a weaker order like Kafka's partitioned topics.
Materialize's timestamp assignment should (and does) respect this order, so that you see a plausible database state.
Materialize records for each virtual timestamp the coordinates in the input order that describe the subset of data available at that timestamp. 
A new data update is assigned the first timestamp whose coordinates contain the update.
As long as the recorded coordinates move forward along the order as times increase, the revealed states of the data also move forward following the order.

Many streamed sources reveal transactional boundaries, such as PostgreSQL's replication log.
Kafka itself supports "transactional writes" but does not reveal the transaction boundaries to readers; you would need to use Debezium configured with a transaction topic to provide transaction information with it.
For PostgreSQL, Materialize assigns identical timestamps to all updates associated with the same transaction.
This ensures that other operations either see all or none of the updates in any transaction.

Finally, having written something to an upstream system (and received confirmation) you might like to be certain it is now available and reflected in Materialize.
This can be achieved by transacting against the upstream system for each timestamp we produce, but is not currently done by Materialize.
We think we should do it, however, and you should expect systems that can provide this level of fidelity to external data sources.

Timestamp assignment is the moment Materialize introduces order to its often inconsistent sources of data. 
It is also the moment we are able to be precise about the consistency properties we are able to maintain, and which we will need to invent.

### Correctness (Internal Consistency)

Materialize has streaming internals, and uses them to continually keep various materialized views up to date.
Even with careful timestamps on input updates, with all the updates in motion through the streaming internals there is the real possibility that Materialize might reveal inconsistent results.
More specifically, because Materialize commits to showing you specific results for each timestamp, we might show *incorrect* results.

Although correctness of query evaluation is table stakes for most databases, many stream processors continually produce outputs that are the correct answer to no possible input data. 
This comes under the name of [eventual consistency](), which allows systems to be transiently incorrect as long as their inputs continue to change.
This is "pretty much always" for stream processors, and yet several popular systems can't be relied on to produce correct outputs while connected to live data.
For an excellent overview, [Jamie Brandon's post on "internal consistency"](https://www.scattered-thoughts.net/writing/internal-consistency-in-streaming-systems/) evaluates this property for ksqlDB, Flink's Table API, and Materialize (which was the one of the three that passed).

Materialize produces **correct** outputs for timestamped inputs.
Anything else is a bug.

We can see this in a playground environement using a query like Jamie used in his post.
Our [guided tutorial](https://materialize.com/docs/get-started/quickstart/) sets up a source of auction transactions, with buyers and sellers.
Although many things change continually, we would hope that the sum of all credits through sales match the sum of all debits through sales.
They should always be exactly identical, and if even for a moment they are not that would be a bug in Materialize.

```sql
-- Maintain the credits due to each account.
CREATE MATERIALIZED VIEW credits AS
SELECT seller, SUM(amount) AS total
FROM winning_bids
GROUP BY seller;

-- Maintain the credits owed by each account.
CREATE MATERIALIZED VIEW debits AS
SELECT buyer, SUM(amount) AS total
FROM winning_bids
GROUP BY buyer;

-- Maintain the net balance for each account.
CREATE VIEW balance AS
SELECT 
    coalesce(seller, buyer) as id, 
    coalesce(credits.total, 0) - coalesce(debits.total, 0) AS total
FROM credits FULL OUTER JOIN debits ON(credits.seller = debits.buyer);

-- This will always equal zero.
SELECT SUM (total) FROM balance;
```

Importantly, nothing about the above example relies on the views being created in the same session, by the same person, or even on the same physical hardware.
Materialize will ensure that `credits`, `debits`, and `balance` always track exactly the correct answer for the timestamped input, and as indicated above will always have a net balance of zero.

To assess internal consistency for systems, it can help to write views that track *invariants* of your data. 
If there is something you know should always hold, for example that the net balances are zero, then you can observe the results and see if you ever observe a result that violates the invariant.

As another example, using the views above, every `buyer` present in `winning_bids` will have an entry in `balance`.
This should always be the case.
We can write a view that checks for violations of this constraint, for which any entry would constitute a correctness violation.
```sql
SELECT * FROM winning_bids 
WHERE buyer NOT IN (SELECT id FROM balance);
```

You can similarly be certain that when you see a result that it corresponds to the correct answer on a specific input. 
For example, if you want to notify users whose balance is below 100, the following view is certain to only report users for which it *actually happened*.
```sql
SELECT * FROM balance WHERE total < -100
```
If you would like to go a step further, you can even report the specific moment at which it was the case, by including the `mz_now()` timestamp function as part of the query.
```sql
SELECT mz_now(), * FROM balance WHERE total < -100
```
All results Materialize produces are the specific answers to the query on the input data as it existed at the query time.
Materialize is internally consistent.

### Completeness of Output Streams

## Consistency and Operational Confidence