# Capturing Change Data Capture (CDC) Data

Change Data Capture (CDC) describes the process of recording and communicating how a collection of data changes.
There are several ways to do this, ranging from the rather simple to the seemingly quite clever.
However, in many cases the cleverness comes at a cost, one that you might not have realized.

The cost of cleverness is often invisible to the CDC provider, and is borne instead by the recipient.
It is not necessarily a bad call to move cost from the CDC provider to the recipient, but it's worth knowing the cost.
In several cases this cost is disproportionate, increasing each recipient's resource requirements up from potentially constant space and work to the (potentially much) more expensive "maintain a mirror of all the data".

Depending on your CDC representation, Materialize, for example, spends a surprising amount of resources simply "reassembling your data", as its first step.
Kafka data modeled with key-based compaction, or "upsert" behavior, needs a second copy of the data maintained with random access, before you can work effectively with it. 
Alternatively, Materialize's PostgreSQL source is incredibly cheap, because PostgreSQL can present its CDC data in a form that doesn't impose a disproportionate downstream cost.

To drive the point home, the CDC recipient in our examples will be charged with tasks only as complicated as
1. Maintaining the number of records in total.
2. Maintaining the number of records for each value of one column.
3. Maintaining the number of records for each value of two columns.

Pretty simple tasks, but they will become progressively harder the more clever our CDC provider is.


Of course, this all depends on how you record CDC data, so let's start there!

## CDC representations

We'll focus our attention on change data capture for a collection of relational data: a multiset of elements ("rows") each of which have distinct attributes ("columns").
The conclusions generalize, perhaps moreso, to more rich data models.

The most straightforward representation of a change (in my mind, at least) is a list of rows that are now in, and a list of rows that are now out.
To go from the prior collection to the new collection, we remove rows that are out and incorporate rows that are in.
This happens to be the representiation that [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) uses internally, upon which [Materialize](https://github.com/MaterializeInc/materialize/) is implemented.
This representation is not especially clever, in that any change at all to a row results in a republication of the entire row, often twice (to both remove the old and insert the new rows).

A more clever representation can be seen with [Debezium](https://debezium.io), which transmits each change with a pairs of records: `before` and `after`.
This single transmission couples both states of the changed row, and from a list of these pairs you could produce the two lists about of records in and out.
What's clever about this representation is that by coupling the changes, there is the opportunity to more efficiently draw attention to the changes.
One could, for example, represent the `{ before, after }` pair by reporting `before` and only the columns that have changed in `after`, let's call it `changed`.
This can both use less space and more directly call attention to the changes.

Getting more clever, collections often have [primary keys](https://en.wikipedia.org/wiki/Primary_key).
These are columns that mean to uniquely identify a row, where any one value occurs at most once in that column in the collection, at any time.
This is exciting, especially for clever people, because it is a concise way to reference the contents of `before` without having to present them:
the prior value of the record (`before`) has already been presented to the recipient, and is identified by some `key`, so why not transmit `{ key, after }` instead?
The recipient can look up `before`, and retract it.
If there is no `before` that means that this is an insertion of a new record; if `after` is `NULL` (a special value) that means that you should just delete `before` and not replace it with anything.
You can see this representation in [Kafka's key-based retention](https://developer.confluent.io/courses/architecture/compaction/#:~:text=Topic%20Compaction%3A%20Key%2DBased%20Retention,value%20for%20a%20given%20key.).

Pushing the very limits of cleverness, let's combine these two techniques.
If `before` has been transmitted already, we could transmit as little as `{ key, changed }`, indicating only the primary key and the changed column values.
This could be tiny, or at least finally proportional to the size of the change, rather than depending somehow on shuttling entire (potentially large!) records around.

There are probably additional clever things beyond these, or perhaps orthogonal to them, but we'll just talk about these in this post.

## Downstream burden

Our discussion so far has been about the CDC provider: the one producing the change data capture stream.
Presumably though, you capture data with the intent of using it somehow.
How you hope to use it is what leads us to our more nuanced evaluation of cleverness.

There are some pretty straightforward uses, and we'll knock them out because I think they do highlight the cleverness of the techniques, without grim downsides.
1.  You might want to mirror your data to another [OLTP database](https://en.wikipedia.org/wiki/Online_transaction_processing). 

    This database almost certainly supports point look-ups (referencing data by `key`) and can receive even the most clever of representations and fish out the `before` records and update them.
    Depending on the database implementation, you may even have to go fish them up in order to update them, so there's potentially relatively little marginal cost to doing so.
    
2.  You might want to land your data in a [analytic data warehouse](https://en.wikipedia.org/wiki/Data_warehouse).

    This data warehouse probably doesn't support efficient point look-ups, but instead can efficiently *merge* your updates periodically.
    Batch warehouses economize on the costs of data updates by only redoing the work periodically, work that would be expensive to perform continually because of the cost of looking up `before` values without indexes.

If this is your plan for CDC, I think all the cleverness above is probably just raw unbridled cleverness, and you should be delighted.

However.

There are even cooler things you can do with CDC streams, faster and leaner and more capable things you can do, and they start to reveal that the cleverness is really a trade-off.
Fundamentally, they need to be more complicated than simply mirroring your data, and more responsive than periodic batch updates support.

## Maintaining `SELECT COUNT(*)`

Let's say you want to keep track of how many records are in your CDC stream.
It's not a very complicated task; most tasks are strictly more complicated than this: maintaining histograms, maintaining averages, maintaining even more complicated SQL.

But let's just start with keeping track of how many records are in your CDC stream.

If you are the recipient of a stream of `{ key, after }` tuples, or `{ key, changed }` tuples, or any of the CDC representations that optimize out the `before` field, what does your implementation look like?
For any `key` you only need to know whether it already exists, in which case the count does not change, or whether it does not exist, in which case you should increment the count by one. If `after` is `NULL` indicating a deletion, you can probably rely on `before` existing and just decrement the count, but potentially you'd like to know `before` in that case too for hygiene..

So, you have to maintain all of the `key` values you've seen. 
That's kind of annoying, and potentially quite a lot of data. 
At least, it is proportional to the size of the input data, rather than proportional to the size of the thing you are maintaining: a single count.
The work you have to do for each update is also much greater: random access into a potentially large pile of data, versus just incrementing or decrementing the count.

What about the in and out lists, or the `{ before, after }` variants? 
They just update the count, by the difference in the in and out list lengths, or the derived amount from the non-`NULL` `before` and `after` fields.
No state is required by the recipient, other than the count itself.

Clever folks may realize that the problem with the clever approaches is that you couldn't tell insertions from updates.
That's a pretty easy fix, in that you could just add that to the CDC messages.
This fixes up the problem with maintaining the count, and perhaps it fixes up all problems?

## Maintaining `SELECT age, COUNT(*)`

This time we aren't just maintaining a total count, but a count of the number of records with some value.
Any update tells us the new value, and so it's not so hard to figure out which count to increment, but we also need to find out which counts to decrement.
At least if we want to maintain the correct answer for data that might contain deletions, and most CDC data does (it's why you are using CDC instead of an append-only event stream).

Any CDC representation that optimizes out the `before` value of all columns obliges the recipient, at least one who needs to maintain `SELECT column, COUNT(*)`, to mirror the corresponding data, to determine how to correctly update the results. 

How much data needs to be mirrored? 
All of the `{ key, age }` entries for all of the records.
If you get a `{ key, changed }`, even being able to distinguish between inserts and updates, you need the specific `age` associated with `key`, which means you need to maintain the full map from `key` to `age`. 
Even though the result likely has some small number of counts, one for each age, the recipient must maintain all distinct keys of a potentially large collection.

Again, clever folks might realize that the problem is leaving out the `before` values of columns, not that we narrow the columns down to those that have changed. What if we ship `{ key, before_cols, after_cols }`, presenting the only the changed columns, but with their prior values?

## Maintaining `SELECT age, zip, COUNT(*)`

We now need to maintain a count with two columns used as the key.

Let's imagine for the moment that movement between ZIP codes is uncorrelated with birthdays: the changes that flow in will likely change either `age` or `zip`.
Unfortunately, to correctly update counts when say an `age` changes, we'll need to track down the `zip` of the corresponding `key`, both to determine which count to increment and which count to decrement.

At this point it may seem that in order to avoid disproportionate costs for the reciever, you need to transmit full `before` and `after` information for the columns they require.
That information can be compressed, for example `{ before, changed }`, but I'm not aware of a way to avoid it.

## Looping back around

In the cases above, simple and naive CDC representations result in more efficient implementations for the recipients.
That doesn't mean you should alway use them: your OLTP CDC provider is potentially critical infrastructure under great load.
However, by using the more sophisticated CDC representations, you are introducing a downstream cost.
If you didn't realize that, it's worth a think; see if you can tolerate setting `wal_level` to `logical` in PostgreSQL, or use direct replication out of PostgreSQL rather than through Kafka.

What about Debezium, then? 
Seems pretty good in the round-up above, and uses Kafka to decouple your infrastructure.
At the time of writing, to the best of my understanding, Debezium has some deduplication gotchas that mean that you cannot rely on the `before` and `after` being correct, and you kinda need to maintain a second copy of the data if you want to be certain.
Materialize wants to be certain, and so it maintains the second copy, at some cost.
But the representation they've chose seems fine.

All in all, these choices represent trade-offs. 
Especially with Change Data Capture, which means to be fast and cheap, it can be valuable to understand that you may be imposing a cost, either in latency or resources, for your downstream uses of your data.
Being aware of the costs puts you in a position to reduce them.