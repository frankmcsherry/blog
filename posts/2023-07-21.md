# Capturing Change Data Capture (CDC) Data

# DRAFT

Change Data Capture (CDC) describes the process of recording and communicating how a collection of data changes.
There are several ways to do this, ranging from the rather simple to the apparently quite clever.
You've possibly used some of the clever ways, and may even have felt clever to do so.
However, in many cases the cleverness comes at a cost, one that you might not have realized.

The cost of cleverness is often invisible to the CDC implementor, and is borne instead by the recipient.
It is not necessarily a bad call to move cost from the CDC implementor to the recipient, but it's worth knowing the cost.
In several cases this cost is disproportionate, increasing each recipient's resource requirements up from potentially constant space and work to the (potentially much) more expensive "maintain a mirror of all the data".

Of course, this all depends on how you record CDC data, so let's start there!

## CDC representations

We'll focus our attention on change data capture for a collection of relational data: a multiset of elements ("rows") each of which have distinct attributes ("columns").

The most straightforward representation of a change (in my mind, at least) is a list of rows that are now in, and a list of rows that are now out.
To go from the prior collection to the new collection, we remove the rows that are out and incorporate the rows that are in.
This happens to be the representiation that [differential dataflow](https://github.com/TimelyDataflow/differential-dataflow) uses internally, upon which [Materialize](https://github.com/MaterializeInc/materialize/) is implemented.
This representation is not especially clever, in that any change at all to a row results in a republication of the entire row, often twice (to retract and then add the new row).

A more clever representation can be seen with [Debezium], which transmits pairs of records: `before` and `after`.
This single transmission couples both states of the changed row, and from a list of them you could produce the two lists about of records in and out.
What's clever about this representation is that by coupling the changes, there is the opportunity to more efficiently draw attention to the changes.
One could, for example, represent the `{ before, after }` pair by reporting `before` and only the columns that have changed in `after`, let's call it `changed`.
This cas both use less space and more directly call attention to the changes.

Getting more clever, collections often have [primary keys](https://en.wikipedia.org/wiki/Primary_key).
These are columns that mean to uniquely identify a row, where any one value occurs at most once in that column in the collection, at any time.
This is exciting, especially for clever people, because it is a concise way to reference the contents of `before` without having to present them:
the prior value of the record (`before`) has already been presented to the recipient, and is identified by some `key`, so why not transmit `{ key, after }` instead?
The recipient can look up `before`, and retract it.
If there is no `before` that means that this is an insertion of a new record; if `after` is `NULL` (a special value) that means that you should just delete `before` and not replace it with anything.

Pushing the very limits of cleverness, let's combine these two techniques.
If `before` has been transmitted already, we could transmit as little as `{ key, changed }`, indicating only the primary key and the changed column values.
This could be tiny, or at least finally proportional to the size of the change, rather than depending somehow on shuttling entire (potentially large!) records around.

There are probably additional clever things beyond these, or perhaps orthogonal to them, but we'll just talk about these in this post.

## Downstream burden

Our discussion so far has been about the CDC implementor: the one producing the change data capture stream.
Presumably though, you capture data with the intent of using it somehow.
How you hope to use it is what leads us to our more nuanced evaluation of cleverness.

There are some pretty straightforward uses, and we'll knock them out because I think they do highlight the cleverness of the techniques.
1.  You might want to mirror your CDC data to another [OLTP database](https://en.wikipedia.org/wiki/Online_transaction_processing). 

    This database almost certainly supports point look-ups (referencing data by `key`) and can receive even the most clever of representations and fish out the `before` records and update them.
    Depending on the database implementation, you may even have to go fish them up in order to update them, so there's potentially relatively little marginal cost.
    
2.  You might want to land your CDC data in a [analytic data warehouse](https://en.wikipedia.org/wiki/Data_warehouse).

    This data warehouse probably doesn't support efficient point look-ups, but instead can efficiently *merge* your updates periodically.
    Batch warehouses economize on the costs here by only redoing the work periodically, work that would be expensive to perform continually because of the cost of looking up `before` values without indexes.

If this is your plan for CDC, I think all the cleverness is probably just raw unbridled cleverness.

However.

There are even cooler things you can do with CDC streams, faster and leaner and more capable things you can do, and they start to reveal that the cleverness is really a trade-off.

## Maintaining `SELECT COUNT(*)`

Let's say you want to keep track of how many records are in your CDC stream.
It's not a very complicated task; most are strictly more complicated than this: maintaining histograms, maintaining averages, maintaining even more complicated SQL.

But let's just start with keeping track of how many records are in your CDC stream.

If you are the recipient of a stream of `{ key, after }` pairs, or any of the CDC representations that optimize out the `before` field, what does your implementation look like?
For any `key` you need to know whether it already exists, in which case the count does not change, or whether it does not exist, in which case you should increment the count by one. If `after` is `NULL` indicating a deletion, you can probably rely on `before` existing and just decrement the count.

So, you have to maintain all of the `key` values you've seen. 
That's kind of annoying, and potentially quite a lot of data. 
At least, it is proportional to the size of the input data, rather than proportional to the size of the thing you are maintaining: a count.

What about the in and out lists, or the `{ before, after }` variants? 
They just update the count, by the different in the in and out list lengths, or the derived amount from the non-`NULL` `before` and `after` fields.
No state required, other than the count itself.

Clever folks may realize that the problem with the clever approaches is that you couldn't tell insertions from updates.
That's a pretty easy fix, in that you could just add that to the CDC messages.
This fixes up the problem with maintaining the count, and perhaps it fixes up all problems?

## Maintaining `SELECT age, COUNT(*)`

This time we aren't just maintaining a total count, but a count of the number of records with some value.
Any update tells us the new value, and so it's not so hard to figure out which count to increment, but we also need to find out which counts to decrement.
At least if we want to maintain the correct answer for data that might contain deletions.

Any CDC representation that optimizes out the `before` value of all columns obliges the recipient, at least one who needs to maintain `SELECT column, COUNT(*)`, to mirror the corresponding data, to determine how to correctly update the results. 

How much data needs to be mirrored? 
All of it.
If you get a `{ key, changed }`, even being able to distinguish between inserts and updates, you need the specific `age` associated with `key`, which means you need to maintain the full map from `key` to `age`. Even though the result likely has some small number of counts, the recipient must maintain all distinct keys of a potentially large collection.

Again, clever folks might realize that the problem is leaving out the `before` values of columns, not that we narrow the columns down to those that have changed. What if we ship `{ key, before_cols, after_cols }`, presenting the only the changed columns, but with their prior values?

## Maintaining `SELECT age, zip, COUNT(*)`

We now need to maintain a count with two columns used as the key.

Let's imagine for the moment that movement between ZIP codes is uncorrelated with birthdays: the changes that flow in will likely change either `age` or `zip`.
Unfortunately, to correctly update counts when say an `age` changes, we'll need to track down the `zip` of the corresponding `key`, even just to determine which count to increment let alone decrement.
