## Demonstrating Operational Data with SQL

Databases, Big Data, and Stream Processors have long had the property that it can be hard to demonstrate their value.
Databases coordinate the work of multiple teams of independent workers, and don't shine when there is just one user.
Big Data systems introduce scalable patterns that can be purely overhead when the data fit on a single laptop.
Stream Processors aim to get the lowest of end-to-end latencies, but do nothing of any consequence on static data.
These systems demonstrate value when you have variety, volume, and velocity, and most demo data sets have none of these.

Materialize, an operational data warehouse backed by scalable streaming systems, has all three of these challenges!

Fortunately, Materialize is powerful enough to synthesize its own operational data for demonstration purposes.
In this post, we'll build a recipe for a generic live data source using standard SQL primitives and some Materialize magic.
We'll then add various additional flavors: distributions over keys, irregular validity, foreign key relationships.

### My Motivation: Materialize

Materialize has a few product beats it wants to hit when we demo it, derived from our product principles.

* Responsiveness: Materialize should be able to get back to you ASAP, even with lots of data involved.
* Freshness: Materialize should reflect new data almost immediately, even through complex logic.
* Consistency: Materialize's outputs should always reflect a consistent state, even across multiple users and views.

We want to get folks to that "aha!" moment where they realize that Materialize is like no other technology they know of.
Until that moment, Materialize could just be a trenchcoat containing Postgres, Spark, and Flink stacked according to your preferences.

Of course, different contexts connect for different users.
Some folks think about transactions and fraud and want to see how to get in front of that.
Others have users of their own, and know that sluggish, stale, inconsistent results are how they lose their users, and want to feel the lived experience.
Many users won't believe a thing until the data looks like their data, with the same schemas and data distributions, and the same business logic.
These are all legitimate concerns, and tell me about the inherent *heterogeneity* involved in demonstrating something.

I want to be able to demonstrate Materialize more effectively, which is some amount tied up in demonstrating it more flexibly.

### A Recipe for Operational Data

Let's start with a statement of what we'd like to see in terms of live data.
We'll start with something simple, and then build towards something more credible.

At Materilaize, operational data is characterized by moment-to-moment updates to relational data.
The simplest possible thing would be a collection of data of some size, that continually changes at some rate.

1. Volume: How much data should there be in steady state?
2. Velocity: How often should the data change?

So for there's no specific statement about what the data needs to be, and this is great.
We'll work with a collection whose rows contain random data, from which we'll derive more "plausible" data.

```sql
-- Generate a sliding window over random data.
-- Arguments: <volume>, <velocity>
SELECT 
    moment,
    digest(moment::text, 'md5') as random,
FROM 
    generate_series(
        '1970-01-01 00:00:00+00', 
        '2099-01-01 00:00:00+00', 
        <velocity>) moment
WHERE now() BETWEEN moment AND moment + <volume>;
```

This example uses `generate_series` to produce moments at which events will occur.
The `<velocity>` argument chooses the step size of the `generate_series` call, and locks in the cadence of updates.
The `<volume>` argument controls for how long each record lingers, and sets the steady state size.
The result is a sliding window over random data, where you get to control the volume and velocity.

Now, while you can *type* the above, it won't actually run properly.
The query describes 130 years of data, probably at something like a one second update frequency (because you wanted live data, right?).
Worse, if you try and materialize it as a view, that `now()` causes all sorts of problems.
To actually get this to work, we'll have to use some clever tricks.

Going forward, we'll use `<volume>` to be `'3 hours'` and take our `<velocity>` to be `'1 second'`.
These are arbitrary, and just here to help make the queries be things we can copy/paste and see work.

#### Clever trick 1: using `mz_now()`

Our first clever trick is to move from `now()` to `mz_now()`.
These are very similar functions, where the former is "real time" and the latter is "Materialize time".
The `now()` function gets you the contents of the system clock, and `mz_now()` gets you the transaction time of your command.
The main difference between the two is that we can materialize some queries containing `mz_now()`, unlike any query containing `now()`.

```sql
-- Generate a sliding window over random data.
-- Arguments: <volume>, <velocity>
SELECT 
    moment,
    digest(moment::text, 'md5') as random,
FROM 
    generate_series(
        '1970-01-01 00:00:00+00', 
        '2099-01-01 00:00:00+00', 
        <velocity>) moment
--    /------\---- LOOK HERE!
WHERE mz_now() BETWEEN moment AND moment + '3 hours';
```
This very simple change means that Materialize now has the ability to keep the query up to date.
Materialize has a feature called "temporal filters" that allows `mz_now()` in `WHERE` clauses, because we are able to invert the clause and see the moment (transaction time) at which changes will occur.

Unfortunately, the implementation strategy for keeping this view up to date involves first producing all the data (we don't have any magical insight into `generate_series` that allows us to invert its implementation).
But fortunately, we have other clever tricks.

#### Clever trick 2: Hierachical Generation

The problem above is that we generate all the data at once, and then filter it.
We could instead generate the years of interest, from them the days of interest, from them the hours of interest, then minutes of interest, then seconds of interest, and finally milliseconds of interest.
In a sense we are generating *intervals* rather than *moments*, and then producing moments from the intervals.

Let's start by generating all the years we might be interested in.
We start with all the years we might reasonably need, and a `WHERE` clause that checks for intersection of the interval (`+ '1 year'`) and the extension by volume (`+ '3 hours'`).
```sql
-- Each year-long interval of interest
CREATE VIEW years AS
SELECT * 
FROM generate_series(
    '1970-01-01 00:00:00+00', 
    '2099-01-01 00:00:00+00', 
    '1 year') year
WHERE mz_now() BETWEEN year AND year + '1 year' + '3 hours';
```
At this point, we'll repeatedly refine the intervals by subdividing into the next granularity.
We'll do this for years into days, and you can use your imagination for the others.
```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
)
WHERE mz_now() BETWEEN day AND day + '3 hours';
```
We'll repeat this on to a view `seconds`, and stop there.
We could continue to milliseconds, but the changes are become harder to see because of the continual change.

Unfortunately, there is a final gotcha.
Materialize is smart enough to see that all of our views are data independent, and it will eagerly evaluate them up through the year 2099.
We were hoping to get away from that, to have MZ hold back the *evaluation* until time passed, but these blasted declarative systems are just too clever.

#### Clever trick 3: An empty table

We can fix everything by introducing an empty table.

The empty table is only present to ruin Materialize's ability to be certain it already knows the right answer.
We'll introduce it to each of our views in the same place, and its only function is to menace Materialize with the possibility that it *could* contain data.
But it won't.

```sql
-- Each day-long interval of interest
CREATE VIEW days AS
SELECT * FROM (
    SELECT generate_series(
        year, 
        year + '1 year' - '1 day'::interval, 
        '1 day') as day
    FROM years
    -- THIS NEXT LINE IS NEW!!
    UNION ALL SELECT * FROM empty
)
WHERE mz_now() BETWEEN day AND day + '3 hours';
```

With these tricks in hand, we now have the ability to spin it up and see what it looks like.

```sql
CREATE DEFAULT INDEX ON days;
```

We'll want to create the same default indexes on our other views: `hours`, `minutes`, and `seconds`.

#### Finishing touches

As a final bit of housekeeping, we'll want to go from intervals back to moments, with some additional inequalities.
```sql
-- The final view we'll want to use.
CREATE VIEW moments AS
SELECT second AS moment FROM seconds
WHERE mz_now() >= second
  AND mz_now() < second + '3 hours';
```
The only change here is the `mz_now()` inequality, which now avoids `BETWEEN` because it has inclusive upper bounds.
The result is now a view that always has exactly 3 * 60 * 60 = 10800 elements in it.
We can verify this by subscribing to the changelog of the count query:
```sql
-- Determine the count and monitor its changes.
COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
```
This reports an initial value of 10800, and then repeatedly reports (second by second) that there are no additional changes.
```
materialize=> COPY (
    SUBSCRIBE (SELECT COUNT(*) FROM moments) 
    WITH (progress = true)
)
TO stdout;
1716210913609	t	\N	\N
1716210913609	f	1	10800
1716210914250	t	\N	\N
1716210914264	t	\N	\N
1716210914685	t	\N	\N
1716210915000	t	\N	\N
1716210915684	t	\N	\N
1716210916000	t	\N	\N
1716210916248	t	\N	\N
1716210916288	t	\N	\N
1716210916330	t	\N	\N
1716210916683	t	\N	\N
^CCancel request sent
ERROR:  canceling statement due to user request
materialize=> 
```
All rows with a second column of `t` are "progress" statements rather than data updates.
The second row, the only one with a `f`, confirms a single record (`1`) with a value of `10800`.

For anyone who wants to nerd out, you can also remove the `COUNT(*)` from the subscribe above, and you'll get the same information more concisely, and more efficiently. 
While you ponder that, we'll move on to turning these moments into plausible data!

### From Moments to Plausible Data

We have now a view `moments` containing some volume of records, which change with some velocity.
These records are just the literal moments at which they occur, which is not yet the backbone of a great demonstration.

Materialize has an [`AUCTION` load generator](https://materialize.com/docs/sql/create-source/load-generator/#auction), which populates `auctions` and `bids` tables.
Their contents look roughly like so:
```
materialize=> select * from auctions;
 id | seller |        item        |          end_time          
----+--------+--------------------+----------------------------
  2 |   1592 | Custom Art         | 2024-05-20 13:43:16.398+00
  3 |   1411 | City Bar Crawl     | 2024-05-20 13:43:19.402+00
  1 |   1824 | Best Pizza in Town | 2024-05-20 13:43:06.387+00
  4 |   2822 | Best Pizza in Town | 2024-05-20 13:43:24.407+00
  ...
(4 rows)
```
```
materialize=> select * from bids;
 id | buyer | auction_id | amount |          bid_time          
----+-------+------------+--------+----------------------------
 31 |    88 |          3 |     67 | 2024-05-20 13:43:10.402+00
 10 |  3844 |          1 |     59 | 2024-05-20 13:42:56.387+00
 11 |  1861 |          1 |     40 | 2024-05-20 13:42:57.387+00
 12 |  3338 |          1 |     97 | 2024-05-20 13:42:58.387+00
 ...
```

Auctions have a unique id, a seller id, an item description, and an end time.
Bids have a unique id (no relation), a buyer id, an auction id, the amount of the bid, and the time of the bid.

The auctions and bids are append-only, and never retract records, unlike the generator we've built.
We could remove the expiration (the `mz_now() < something` fragment), but our generator would then produce data from 1970 through today, or we would have to introduce a "start time".
For now we won't do that, and will use our three hour window instead.

The `seller`, `item`, `buyer`, and `amount` fields are all random, within some bounds.
As a first cut, we'll think about just using random values for each of the columns.
Where might we get randomness, you ask?
Well, if *pseudo*-randomness is good enough (it will be), we can use cryptographic hashes of the moments.
```sql
-- Extract pseudorandom bytes from each moment.
CREATE VIEW random AS
SELECT moment, digest(moment::text, 'md5') as random
FROM moments;
```
We could start pulling bytes out of `random` and use them to populate columns, and we'd have a first cut at random data.
Columns like `item` are populated by joining with a constant collection (part of the generator), but `id` and `seller` could just be random.
The `end_time` is currently deterministic, at 10 seconds after the auction creation.

```sql
-- Totally accurate auction generator.
CREATE VIEW auctions_core AS
SELECT 
    moment,
    random,
    get_byte(random, 0) + 
    get_byte(random, 1) * 256 + 
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) AS seller,
    get_byte(random, 4) as item,
    moment + '10 seconds' as end_time
FROM random;
```

The columns of `bids` are also pretty random, but columns like `auction_id` and `bid_time` do want to have some relation to the rows of `auctions`.
We'll get to that after we do a bit of work with `auctions`.

#### Custom Expiration

As you can see, auctions are pretty brief at ten seconds, and not many are in play at the same time.
To change this in the load generator involves recompiling and redeploying Materialize, and isn't something we'll want to do.
Instead, we can just change the SQL, and put a random number of seconds in there.
```sql
    ...
    -- Have each auction expire after up to 256 seconds.
    moment + (get_byte(random, 5)::text || ' seconds')::interval as end_time
    ...
```
We could certainly crank this up higher if we like, and the iteration time is only the time it takes you to edit the SQL.

If we want the auction to vanish from `auctions` at this time, we could accomplish this with another temporal filter:
```sql
WHERE mz_now() < end_time
```
As soon as we reach `end_time` the auction will vanish from `auctions`.
This is a very helpful pattern for load generators that want to control when data arrive and when it departs, in finer detail than "a three hour window".

For the moment we'll leave out the temporal filter, so that we can experience late-arriving bids.
Also, we don't want to artificially reduce our volume.
Perhaps thematically we can think of this as auctions whose winners have been locked in, but whose accounts have not yet been settled.

#### Realistic Data

Our random numbers for `item` aren't nearly as nice as what the existing load generator produces.
However, we can get the same results by putting those values in a view and using our integer `item` to join against the view.
```sql
-- A static view giving names to items.
CREATE VIEW items (id, item) AS VALUES
    (0, 'Signed Memorabilia'),
    (1, 'City Bar Crawl'),
    (2, 'Best Pizza in Town'),
    (3, 'Gift Basket'),
    (4, 'Custom Art');
```

Now when we want to produce an actual auction record, we can join against items like so
```sql
-- View that mirrors the `auctions` table from our load generator.
CREATE VIEW auctions AS
SELECT id, seller, items.item, end_time
FROM auctions_core, items
WHERE auction.item = items.id;
```

We've now got a view `auctions` 

#### Foreign Keys

Each bid in `bids` references an auction, and we are unlikely to find an extant auction if we just use random numbers for `auction_id`.
We'd like to base our `bids` on the available auctions, and have them occur at times that make sense for the auction.

We can accomplish this by deriving the bids for an auction from `auctions` itself, using some available randomness to propose a number of bids, and then further randomness to determine the details of each bid.
```sql
CREATE VIEW bids AS
-- Establish per-bid records and randomness.
WITH prework AS (
    SELECT 
        id as auction_id,
        moment as auction_start,
        end_time as auction_end,
        digest(random::text || generate_series(1, get_byte(random, 5))::text, 'md5') as random
    FROM auctions_core
)
SELECT
    get_byte(random, 0) +
    get_byte(random, 1) * 256 +
    get_byte(random, 2) * 65536 as id,
    get_byte(random, 3) AS buyer,
    auction_id,
    get_byte(random, 4)::numeric AS amount,
    auction_start + (get_byte(random, 5)::text || ' seconds')::interval as bid_time
FROM prework;
```

We now have a pile of bids for each auction, with the compelling property that when the auction goes away so too do its bids.
This gives us "referential integrity", the property of foreign keys (`bids.auction_id`) that their referent (`auction.id`) is always valid.

### Demonstrating Materialize

Let's take out load generator out for a spin, and see if we can't hit the beats of responsiveness, freshness, and consistency.
These are borrowed from our quickstart, but we are able to lean in to more pointed experiences that we can create, by making the data do what we need to both put ourselves into a tight spot and show how Materialize gets you out.

#### Responsiveness

Materialize is able to respond immediately, even to complex queries over large volumes of data.
With our three hour window and new auction each second, we are looking at 10800 auctions.
Each auction has a random number of bids, 100+ on average, giving us one million bids at any moment.
```
materialize=> select count(*) from auctions;
 count 
-------
 10800
(1 row)

Time: 151.228 ms
materialize=> select count(*) from bids;
  count  
---------
 1375481
(1 row)

Time: 950.929 ms
materialize=> 
```
One second to count one million things is not great, but this is running on our smallest instance (`25cc`; roughly 1/4 of a core).
Also, we aren't yet using Materialize's ability to *maintain* results.

We have an index `bids_id` on `bids` by column `id`. 
When we want to find a specific bid by id, this can be very fast .
```
materialize=> select * from bids where id = 3;
 id | buyer | auction_id | amount |        bid_time        
----+-------+------------+--------+------------------------
  3 |   226 |    8494847 |    104 | 2024-05-21 12:19:37+00
(1 row)

Time: 19.776 ms
materialize=> 
```
Inspecting the query history (a feature in Materialize's console) we can see it only took 5ms for the DB (the additional latency is between NYC and AWS's us-east-1).
This really is just a look-up into a maintained index, admittedly only on `bids` rather than something sophisticated.

```sql
-- Determine auction winners: the greatest bid before expiration.
CREATE VIEW winning_bids AS
  SELECT DISTINCT ON (auctions.id) bids.*,
    auctions.item,
    auctions.seller
  FROM auctions, bids
  WHERE auctions.id = bids.auction_id
    AND bids.bid_time < auctions.end_time
    AND mz_now() >= auctions.end_time
  ORDER BY auctions.id,
    bids.amount DESC,
    bids.bid_time,
    bids.buyer;
```

Directly querying this view results in a not-especially-responsive experience
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
        256 |   169 |    254
      62990 |    72 |    252
      45074 |    83 |    253
      60691 |   214 |    253
      11808 |    43 |    219
(5 rows)

Time: 10956.198 ms (00:10.956)
materialize=> 
```

However, we can create indexes on `winning_bids`, and once they are up and running everything gets better.
```sql
CREATE INDEX wins_by_buyer ON winning_bids (buyer);
CREATE INDEX wins_by_seller ON winning_bids (seller);
```
```
materialize=> select auction_id, buyer, amount from winning_bids limit 5;
 auction_id | buyer | amount 
------------+-------+--------
    5719329 |     0 |    255
    7866928 |     0 |    224
    5433708 |     0 |    253
   15094638 |     0 |    253
    7498887 |     0 |    255
(5 rows)

Time: 69.544 ms
materialize=> 
```
Rather than grind over the million-plus bids to find winners, the ~10000 results are maintained and its easy to read the first five.

In addition, our indexes set us up for responsive ad-hoc queries.
Here's an example where we look for "auction flippers": folks who are both buyers and sellers.
```sql
-- Look for users who re-
CREATE VIEW potential_flippers AS
  SELECT w2.seller,
         w2.item AS item,
         w2.amount AS seller_amount,
         w1.amount AS buyer_amount
  FROM winning_bids w1,
       winning_bids w2
  WHERE w1.buyer = w2.seller
    AND w2.amount > w1.amount
    AND w1.item = w2.item;
```
As it turns out, we have so many auctions relative to buyers and sellers that everyone looks like a flipper.
```
materialize=> select distinct(seller) from fraud_activity limit 5;
 seller 
--------
      0
      1
      2
      3
      4
(5 rows)

Time: 65.498 ms
materialize=> 
```

---

**FRANK**: This isn't a great demonstration yet, because it's only a join between 10k things. 
We should either get the data larger, or dial down the amount of time that it takes.

---

Materialize's indexes make working with your data and query results interactive.
They compute and then maintain your results, so that there is no work to do when you want to inspect them.
Additionally, they store the results in indexed form, so that you can zip right to the records you (or your JOIN) need to see.

#### Freshness

How do you show freshness with a load generator?
We've already seen that Materialize is bright enough to determine the *future* of the load generator; we had to work around that.
What do you need to see to be sure that Materialize is *responding* rather than just following a pre-written script for the future?

We need **interaction**!

Let's create a table we can modify, through our own whims and fancies.
Our modifications to this, unknown to the load generator, will be how we demonstrate the speed at which Materialize updates results as data change.
```sql
-- Accounts that we might flag for fraud.
CREATE TABLE fraud_accounts (id bigint);
```

Let's look at a query that calls out the top five accounts that win auctions.
We'll subscribe to it, meaning we get to watch the updates as they happen.
```sql
-- Top five non-fraud accounts, by auction wins.
SUBSCRIBE TO (
  SELECT buyer, count(*)
  FROM winning_bids
  WHERE buyer NOT IN (SELECT id FROM fraud_accounts)
  GROUP BY buyer
  ORDER BY 2 DESC LIMIT 5
);
```
This produces first a snapshot and then a continual stream of updates.
I've added line breaks between moments of updates, for visual clarity.
```
1716298979329	1	47	52
1716298979329	1	88	53
1716298979329	1	105	55
1716298979329	1	133	54
1716298979329	1	165	53

1716298980000	-1	88	53
1716298980000	1	88	54

1716298981000	1	58	52
1716298981000	-1	165	53

1716298984000	1	88	53
1716298984000	-1	88	54

1716298997000	1	133	53
1716298997000	-1	133	54
...
```
Many of these are just changes to the counts, but one of them swaps out buyer `165` for buyer `58`.
The big one is buyer `105` though. 
Let's tag them as suspicious by adding them to `fraud_accounts`.
```sql
INSERT INTO fraud_accounts VALUES (105);
```
Over in the subscription window, we immediately see the following
```
...
1716299106000	1	79	52
1716299106000	-1	133	53
1716299163518	1	88	52
1716299163518	-1	105	55
1716299180000	-1	57	52
1716299180000	1	57	53
...
```
Out of nowhere, certainly not on some clean second boundary, buyer 105 vanishes and is replaced by buyer 88. 
It's tricking to reveal this freshness to you, in textual form, but it's something you can see happen.

---

**FRANK**: Is there a version of this query that inserts `105, now()`, and somehow surfaces the time as part of the output?
Obviously removing records based on an insertion doesn't do this (the record is gone), but perhaps there is a different query, e.g. "monitored accounts" perhaps?

---

The results, even for complex queries, are updated immediately.

#### Consistency

All the freshness and responsiveness in the world doesn't mean much if the results are incoherent.
Materialize only ever presents actual results that actually happened, with no transient errors.
When you see results, you can confidently act on them knowing that they are real, and don't need further second to bake.

Let's take a look at consistency through the lens of account balances as auctions close and winning buyers must pay sellers.
```sql
-- Account ids, with credits and debits from auctions sold and won.
CREATE VIEW funds_movement AS
  SELECT id,
         SUM(credits) AS credits,
         SUM(debits) AS debits
  FROM (
    SELECT seller AS id, amount AS credits, 0 AS debits
    FROM winning_bids
    UNION ALL
    SELECT buyer AS id, 0 AS credits, amount AS debits
    FROM winning_bids
  )
  GROUP BY id;
```

These balances derive from the same source: `winning_bids`, and although they'll vary from account to account, they should all add up.
Specifically, if we get the total credits and the total debits, they should 100% of the time be exactly equal.
```sql
-- Discrepancy between credits and debits.
SELECT SUM(credits) - SUM(debits) 
FROM funds_movement;
```
This query reports zero, 100% of the time.
We can `SUBSCRIBE` to the query to be notified of any change.
```
materialize=> COPY (SUBSCRIBE (
    SELECT SUM(credits) - SUM(debits) 
    FROM funds_movement
)) TO STDOUT;

1716312983129	1	0
```
This tells us that at time `1716312983129`, there was `1` record, and it was `0`.
You can sit there a while, and there will be no changes.
You could also add the `WITH (PROGRESS = TRUE)` option, and it will provide regular heartbeats confirming that it is still zero.
The credits and debits always add up, and aren't for a moment inconsistent.

We can set up similar views for other assertions.
For example, every account that has sold or won an auction should have a balance.
A SQL query can look for violations of this, and we can monitor it to see that it is always empty.
If it is ever non-empty, perhaps there are bugs in the query logic, its contents are immediately actionable: there is a specific time where the inputs evaluated to an invariant-violating output, and we can take it from there.

The consistency extends across multiple independent sessions.
The moment you get confirmation that the insert into `fraud_accounts`, you can be certain that no one will see that account in the top five non-fraudulent auction winners.
This guarantee is called "strict serializability", that the system behaves as if every event occurred at a specific time between its start and end, and is the strongest guarantee that databases provide.

### Appendix: All the SQL