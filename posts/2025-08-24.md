## Checking in on Datatoad

I've been working on / with [datatoad](https://github.com/frankmcsherry/datatoad) for a bit now, as a way to learn more about alternate ways to work with data.
It's an interpreted Datalog engine, with many limitations, but also many opportunities for me at least to learn about doing things with computation and information.
I thought this might be a good time to check in on that, and think about where to go next with it.

One of my main goals was to explore the moments that are new to me about *interpreted* computation.
We're building an interepreter, one that is not specialized for the programs that it runs ahead of time.
Interpretation doesn't have to be expensive, and I think Datalog can be a good example of why that is.
Although Datalog can come across as complicated or weird, the computational kernels at its core don't necessarily require specialization.

### A data model

Datatoad has a pretty simple data model.
Like most Datalogs, facts are spread across named "relations" each of which has a fixed arity (number of columns).
The columns have literal values that are all of type `[u8]`: a sequence of bytes of some length that they get to choose.

A sequence of bytes seems flexible enough for an interpreter, but it also seems like it steps away from a lot of structure that you could take advantage of with a compiler.
If you knew that each literal was a `u32`, you could use simple arithmetic to find the location of the ith literal in a list, e.g. multiplying an index by four because a `u32` is four bytes wide.
If you want to sort a list of `u32` data you can do that in place very efficiently, but if you had to sort a list of `[u8]`, which isn't even a type that Rust will let you instantiate in most places, you've got a pile of questions to sort out first.
The generality immediately introduces you to a bunch of problems that you might not have realized that the compiler solves for you.

At the same time, (vanilla) Datalog has a pretty limited vocubulary of what you can do with these literals.
You can test them for equality.
That's pretty much the only thing you get to do.
We won't needa a particularly rich type system to help us do this.
Mostly, we need to see if we can claw back the performance we lose when cutting the compiler out of the loop.

### Run-time specialization

One of the main ideas in `datatoad`, and probably all other interpreters for data-intensive computation, is to pick a data layout that can be introspected and upgraded at runtime.

For example, `datatoad` stores its `[u8]` data in a flat list, with offsets that indicate the boundaries of each list item.
Informally, it looks a bit like
```rust
struct ByteLists {
    /// All byte lists, appended.
    values: Vec<u8>,
    /// Offsets in `values` where list boundaries occur.
    bounds: Vec<usize>,
}
```
This works for general lists of bytes, but it also reveals through `bounds` those cases where all of our lists have length four, as would be the case if we have exclusively `u32` data.

The `Vec<usize>` type isn't great for efficient run-time introspection.
You have to look at all the elements to see if everything is a multiple of four, and there aren't a sneaky `3` and `5` right next to each other.
Instead, you could write something that optimistically imagines a stride, and records that for as long as it can:
```rust
enum Bounds {
    /// A stride and a length.
    Strided(usize, usize),
    /// Arbitrary offsets.
    General(Vec<usize>),
}
```
You could see how pushing a new offset into a `Bounds` could either bump the count for the `Strided` variant, if approprate, or convert it to the `General` variant if not.
Datatoad does something like this, and it's relatively cheap to see at runtime if your data are strided, and if so use that stride to cut over to specialized logic.

For example, the logic that sorts and deduplicates a set of facts does exactly this.
It starts from a `Vec<u8>`, and asks whether the facts all have a common number of columns (one hopes so), and whether the terms they reference all have a common number of bytes (negotiable).
If these check out, it dives into a specialized implementation based on the product of their widths.
```rust
/// Sorts and potentially deduplicates
pub fn sort<const DEDUP: bool>(facts: &mut Facts) {
    // Attempt to sniff out a known pattern of fact and term sizes.
    // Clearly needs to be generalized, or something.

    let terms = facts.bounds.strided();
    let bytes = facts.values.bounds.strided();

    let width = terms.and_then(|w0| bytes.map(|w1| w0 * w1));
    match width {
        Some(8)  => sort_help::<DEDUP,  8>(facts, terms.unwrap()),
        Some(12) => sort_help::<DEDUP, 12>(facts, terms.unwrap()),
        Some(16) => sort_help::<DEDUP, 16>(facts, terms.unwrap()),
        _ => {
            use crate::facts::Sorted;
            facts.sort::<DEDUP>();
        }
    }
}
```

As a sidebar, because we are just checking for equality of terms, we can use something like [radix sort](https://en.wikipedia.org/wiki/Radix_sort) to go ~2x faster than Rust's optimized in-place unstable sort.
It's a handy perk of just using sequences of bytes, and not trafficking in user-defined orders on things.
When all terms have the same length, bottom-up radix sort is on the table, and .. it's just really quite fast.
Otherwise, top-down radix sort works too, but my implementation is slower than my bottom-up implementation.

I hope this gives a bit of a taste for the sort of specialization you have access to in an interpreter, where some light runtime information allows you to head to an optimized computational kernel.

### Columns

One of the limitations of the `sort` logic above is that it doesn't do a great job when all but one of the columns have a fixed width.
The test fails in that case, and we go to the slow path.
I'm still on the prowl for a sort that handles this case well; I'm imagining it's a variant of radix sort.

However, there are instances of specialization that are robust to relations with a mix of column types.
The main conceptual shift here is to laying out your data in columns, rather than rows, to colocate the data with the common structural properties.

Let's say we have a relation with three columns, each of which is a `[u8]`.
We could pack the whole relation into one long `Vec<u8>`, with boundaries for every term, and further boundaries for every fact (the latter likely with a stride of three).
This would be a "row-oriented" representation, as the whole of each fact are immediately adjacent.
It's a great representation in a lot of cases, sorting being one great example, but it gets in the way of interpretation.

Another representation would model the relation as three `Vec<u8>` lists, one for each column, with independently tracked strides for each column.
The columns are meant to be more "homogenous" than the row-oriented pile of bytes, and each column independently may be strided and by different amounts.
This representation efficiently reveals the strided structure, and will allow us to divert to optimized implementations more often.

The key distinction here is that when interpreting, we want to be able to identify large hunks of data and logic about which insight about specialization can apply.
We don't want to have to reconsider the specialization for each fact; that's expensive and defeats the purpose.
But if we can slice the data, and harder the logic, so that the specialization opportunities remain, we're in luck.

We'll look at a concrete example, merging sets of facts, but need to start with a detour through `datatoad`'s internal representation.

### Columnar Tries

Datatoad represent sets of facts with a column-oriented trie representation.
The tl;dr is that each column is represented as a sequence of lists of values.
Importantly, not just a sequence of values, nor a list of values, but a sequence of lists of values.
This would be a `Vec<Vec<T>>` in Rust, not just a `Vec<T>`.

It roughly shaped as:
```rust
struct Layer<T> {
    values: Vec<T>,
    bounds: Vec<usize>,
}
```
This looks very similar to the shape we had above for `ByteLists`, but .. it's totally different I'm so sorry.

Each layer contains the lists that represent the ways we can extend various prefixes of facts.
Concretely, layer `i`'s `j`th list contains the values that can be appended to the `j`th distinct prefix of the first `i` attributes.
That is too much for me to understand, and I wrote it.
Let's look at an example.

Here is a sorted list of rows, each containing three columns of potentially different types let's say.
```
(A, 1, x),
(A, 1, y),
(A, 1, z),
(A, 2, x),
(A, 3, x),
(A, 3, y),
(B, 2, z),
(C, 1, x),
(C, 2, x).
```

Layer 0 would contain lists of elements from `{ A, B, C }` that can extend the distinct length-0 prefixes.
There's only one length-0 prefix: `()`, so there's only one list.
That list contains `[A, B, C]`, because those are the distinct values that extend `()` in our set..

Layer 1 would contain lists of elements from `{ 1, 2, 3 }` that can extend the distinct length-1 prefixes.
There are three length-1 prefixes: `(A)`, `(B)`, and `(C)`, so there would be three lists.
Those three lists are `[1, 2, 3]`, `[2]`, and `[1, 2]`, because they are the distinct values that extend `(A)`, `(B)`, and `(C)` respectively.

Layer 2 would contain lists of elements from `{ x, y, z }` that can extend the distinct length-2 prefixes.
There are six length-2 prefixes, because those are the total number of list items in layer 1.
Those six lists are `[x, y, z]`, `[x]`, `[x, y]`, `[z]`, `[x]`, and `[x]`, because they are the distinct values that extend the six distinct length-2 prefixes..

Our columnar trie has three layers, and they look like
```
layer 0: [[A, B, C]],
layer 1: [[1, 2, 3], [2], [1, 2]],
layer 2: [[x, y, z], [x], [x, y], [z], [x], [x]].
```

Each layer has as many lists as the previous had total list items, and each list indicates the distinct ways to extend the tuple, or "path through the layers".
Eventually there is a last layer, and we just stop there with whatever tuples we end up with.

### Merging sets of facts (Union)

Datalog maintains sets of facts, and as more are derived we potentially spend quite a lot of time unioning sets of facts together.
In a row-oriented representation this can be pretty easy to write:
```rust
facts.sort();
facts.dedup();
```
Very easy to write, but very not so performant as it turns out.
At least, sorting a pile of variously length byte sequences can be hairy, and I don't have a way to make it go fast.

How do things look with the columnar trie representation?

There are at least two ways to write the code, and I'd like to talk about both of them because I did them in this order, and the second one is the one you actually want (I think).

#### The first approach (row-oriented)

I know we said columns, but .. the easiest code to write when merging two lists of sorted things is a merge.
That is, you walk linearly through each of the things, recording and advancing your cursor in whichever one is smaller.
It's linear time, which is amazing so surely that works great?

It's not too hard to write, but I won't copy/paste it at you because there are oodles of nits and details and such.
The rough idea is that we are going to walk through the last layer of both inputs, which are in 1:1 correspondence with the facts in each trie.
As we do, we'll maintain the tuple or "path" that led us to the element of the last layer, occasionally popping up a level or two as we "exhaust" lists, and advancing to the next element of the higher layer.
We'll be clever in a few ways, but mostly by noticing that when we don't match at a higher layer we can just copy ranges of the lower layers, as any prefix exclusive to one input can just be copied over (there are no facts from the other input that need to be stitched in).

It's a fine way to do things, and goes faster than my best implementation on row data (sort, then dedup).

It's not the fastest way to do things though, so let's look at that instead.

#### The second approach (col-oriented)

If we think of our two inputs as columnar tries, each a sequence of layers, what do we need to do for each layer?

The first layer is probably the easiest, as there is one list and the values just need to be merged.
Each of the values that appear in the first layer of either input will be present in the output, there will only be one list, and so we just need to merge the values of the first layers to form the first layer of the output.

The second layer is more complicated.

To form the second layer, we'll need to produce the lists that correspond to the values of the first layer.
When forming a list for a value present in both first layers, we'll need to merge the second layer lists.
When forming a list for a value present only in one of the first layers, we'll just need to copy the list in (or merge with an empty list, but .. no).

It's very important in forming the second layer, and generally each subsequent layer, to understand the interleaving of values from the previous layer.
That will drive whether we merge corresponding lists, or just copy over lists from either input.
We'll represent this information with a sequence of ranges describing the potential exclusivity:

1. Lists `this[index0]` and `that[index1]` correspond to matching prefixes.
2. Lists `this[lower .. upper]` correspond to prefixes absent from `that`.
3. Lists `that[lower .. upper]` correspond to prefixes absent from `this`.

We'll form a sequence of instances of these three variants to record all we need to know about how the comparisons of the previous layer played out.
Importantly, this information doesn't leak anything about the type `T` of data contained in the layer, which is what unlocks our ability to specialize.

More concretely, we'll maintain a list of `Report`s, which are each
```rust
/// Records exclusivity of ranges of values.
enum Report {
    /// `this(lower, upper]` exclusively.
    This(usize, usize),
    /// both `this[index0]` and `that[index1]`.
    Both(usize, usize),
    /// `that(lower, upper]` exclusively.
    That(usize, usize),
}
```

We'll merge two corresponding layers taking a list of reports as input, to pilot the merging of the lists the two layers contain.
Each `This` or `That` variant results in copying lists from the corresponding layer, and emitting `This` or `That` variants with new bounds (the lower bound of the first list, and upper bound of the last list).
Each `Both` variant results in a deeper dive where we merge a list and emit a potential mix of `This`, `That`, and `Both` reports for the next layer.

The per-layer `union` logic is where we do most of our work, but we make sure to write it to be generic over the type of list item, as we'll want access to these different implementations.
I don't have the whole implementation to paste at you, but the signature looks like so:
```rust
/// Merges two sequences of lists using alignment information in `reports`.
///
/// The `NEXT` parameter indicates whether the next round of reports should be prepared,
/// which is for example not necessary in the last layer of a union, as there is no next
/// layer to prepare reports for, and the cost can be higher there than for prior layers.
pub fn union<'a, const NEXT: bool, C: Container<Ref<'a>: Ord>>(
    lists0: <Lists<C> as Container>::Borrowed<'a>,
    lists1: <Lists<C> as Container>::Borrowed<'a>,
    reports: &mut std::collections::VecDeque<Report>,
) -> Lists<C> {
    ...
}
```
There's probably some weird stuff you don't get immediately (`Lists`, `Container`, `Borrowed`), but the main point is that the method acts on pairs of layers, and is generic with respect to a container type `C`, which is where we will distinguish between e.g. `[u8]` and `[u8; 4]`.
The implementation is roughly as described above, with no real secrets except that the comparisors and merging will be specialized based on `C`.

The trie-level `union` logic is relatively concise, and just repeatedly invokes the right form of the per-layer `union` logic:
```rust
pub fn union(&self, other: &Self) -> Self {

    assert_eq!(self.layers.len(), other.layers.len());
    let mut layers = Vec::with_capacity(self.layers.len());

    let mut reports = std::collections::VecDeque::default();
    reports.push_back(Report::Both(0, 0));
    for (layer0, layer1) in self.layers.iter().zip(other.layers.iter()) {
        let lists0 = layer0.list.borrow();
        let lists1 = layer1.list.borrow();
        let list = match (upgrade_hint(lists0), upgrade_hint(lists1)) {
            (Some(4), Some(4)) => {
                if layers.len() + 1 < self.layers.len() { downgrade(union::<true,_>(upgrade::<4>(lists0).unwrap(), upgrade::<4>(lists1).unwrap(), &mut reports)) }
                else { downgrade(union::<false,_>(upgrade::<4>(lists0).unwrap(), upgrade::<4>(lists1).unwrap(), &mut reports)) }
            }
            _ => {
                if layers.len() + 1 < self.layers.len() { union::<true,_>(lists0, lists1, &mut reports) }
                else { union::<false,_>(lists0, lists1, &mut reports) }
            }
        };
        layers.push(Layer { list });
    }

    Self { layers }
}
```
It's concise, but maybe not clear.

Each layer has two branching moments, first if we can confirm that the byte sequences all have length four (add your favorite widths here, but not too many), and second if this is the last layer (for which we want to spare ourselves the production of reports).
The gobbledegook in there is ensuring that we specialize the data appropriately in each of these cases, and ultimately call specialized logic.

And that's `union` with columnar tries.
Let's see how it runs!

#### Comparing implementations

First, I don't love either of these implementations.
The "right answer" should involve a blend of the two, more "column chunking" than whole column at a time, I suspect.
I also have some odd logic for merging lists that has a bunch of sequential dependencies, and I'm sure prevents the underlying hardware from zipping through data faster.

Also, all of the benchmarks I have at the moment don't spend all so much time in `union`.
Some amount, for sure, but it's not like `datatoad` is mainly unioning data.
It seems to be mostly radix sorting data, after which everything is pretty cheap.

Let's grab the GALEN example I've been using as reference, and load both versions up with [`samply`](https://crates.io/crates/samply), with a 1KHz sample rate.
We'll report the number of samples in the `union` method, as well as the number overall to compare.

|            | in union | in total |
|------------|---------:|---------:|
| row-based  |    5,084 |   20,159 |
| col-based  |    3,279 |   18,561 |

I should stress that in this case all data are `[u8; 4]`, so the row-based approach is also able to upgrade the data to fixed-width slices, and never touches variable length binary data.
The numbers would be a blow-out if all but one column were fixed width, especially if it was the first column, which ends up relatively tiny.

So there's actually a solid performance improvement here even if you ignore the "specialization robustness".
What we've done is removed a lot of intense control flow, bouncing around record by record maybe popping items out of the current tuple maybe not, and replaced it with relatively more direct control flow for each column.
There's a lot of potential to make things even leaner, looking in to how to perform the merges in ways that are more SIMD and processor friendly, for example.

### Differencing sets of facts (Antijoin)

Oh did you think it was only `union`?

Another thing that you end up doing a lot of in Datalog is determining the facts in one set but not the other.
Fundamentally, a bottom-up Datalog evaluator keeps running as long as it produces *new* facts, but to figure out which facts are new we need to rule out facts we already had.
This "set difference" is called an "antijoin" in the database sphere.

Like taking the union, performing an antijoin can also be done both row-by-row, or column-by-column.
I'll let you ponder out the row-by-row version, but .. it's a merge just like for the `union` logic, only you record a fact when the merge finds it in one specific input and not the other.
It has the same linear-time appeal, but also the relatively intense control flow.

Let's discuss the column-by-column approach!
One way to think about the approach is that we'll identify facts that are present in both inputs, and just copy over the first trie minus those facts present in both.
If you recall the `Report` enum above, we'll do the same thing except we are only interested in the `Both` variants: these indicate matching fact prefixes from the two input tries.
They tell us not only that things match so far, but also where to find the extensions to the matching prefixes.
If we develop these all the way to the last layers, then any matches are now in the intersection of the two sets of facts.

The detail is less gory than for the layer `union` logic, so I'm going to share it.
Informally, `reports` is like a list of `Report`s, except that since it's only the `Both` variants we'll just record their two integers as a pair.
For perverse reasons, we're reading out of the same `VecDeque` that we are writing back into.
```rust
/// Intersects aligned lists from two inputs, recording common list items in `reports`.
///
/// The `reports` input should contain pairs of list indices that should be intersected,
/// and it will be populated with pairs of item indices (not list indices) that reference
/// equal items in indicated lists.
#[inline(never)]
pub fn intersection<'a, C: Container<Ref<'a>: Ord>>(
    list0: <Lists<C> as Container>::Borrowed<'a>,
    list1: <Lists<C> as Container>::Borrowed<'a>,
    reports: &mut std::collections::VecDeque<(usize, usize)>,
) {
    let report_count = reports.len();
    for _ in 0 .. report_count {
        let (index0, index1) = reports.pop_front().unwrap();

        // Fetch the bounds from the layers.
        let (mut lower0, upper0) = list0.bounds.bounds(index0);
        let (mut lower1, upper1) = list1.bounds.bounds(index1);

        // Scour the intersecting range for matches.
        while lower0 < upper0 && lower1 < upper1 {
            let val0 = list0.values.get(lower0);
            let val1 = list1.values.get(lower1);
            match val0.cmp(&val1) {
                std::cmp::Ordering::Less => {
                    lower0 += 1;
                    crate::join::gallop(list0.values, &mut lower0, upper0, |x| x < val1);
                },
                std::cmp::Ordering::Equal => {
                    reports.push_back((lower0, lower1));
                    lower0 += 1;
                    lower1 += 1;
                },
                std::cmp::Ordering::Greater => {
                    lower1 += 1;
                    crate::join::gallop(list1.values, &mut lower1, upper1, |x| x < val0);
                },
            }
        }
    }
}
```
The `gallop` methods fast-forward indexes until we find a value that might match again, taking logarithmic time in the size of the gap to cross.
But otherwise, this takes as input equated lists, and for those pairs of lists finds and records indexes of extensions that also match.

Having intersected the two inputs, we now want to restrict the first input to non-intersection facts.
The final `reports` tell us which of the facts, in order of the last-layer extensions, to retain and not retain.
We'll switch these over to a `Vec<bool>` (actually a `VecDeque<bool>`) and *ascend* the layers.

Why *ascend* the layers?
What makes that the right direction?

Before we can form a layer, we need to know whether its values had any extensions that were retained.
If so, we'll need the value, and if not we'll want to exclude the value.
As we ascend the layers we'll consume and produce a list of bits indicating whether the corresponding list had any values or not, i.e. whether it was retained and therefore its prefix must be as well.
Each layer retains exactly the values for lists that are retained, and produces an output list and indicates such with its bit exactly when at least one of its list items is retained.

As we saw before, the list of bits forms the boundary between these layers, leaking nothing about the types contained within.
This abstraction barrier is what unlocks the specialization opportunity.
Without it, we would be unable to describe to `datatoad` what the code should do in response to specialized data, without spelling out all combinations for all layers.

The restriction code looks like so:
```rust
/// Restrict list items based on `items`, producing a new layer and updating `items`.
#[inline(never)]
pub fn restrict<'a, C: Container>(
    lists: <Lists<C> as Container>::Borrowed<'a>,
    items: &mut std::collections::VecDeque<bool>,
) -> Lists<C> {

    // In principle we can copy runs of items described in `items`, and figure out
    // the bounds and updated items second, from `lists.bounds` and `items`.
    // Likely best to lean in to this to get as much bulk copying as possible.
    let mut output = <Lists::<C> as Container>::with_capacity_for([lists].into_iter());

    assert_eq!(items.len(), lists.values.len());
    let mut item_pos = 0;
    for list_index in 0 .. lists.len() {
        let (lower, upper) = lists.bounds.bounds(list_index);
        for item_index in lower .. upper {
            if items.pop_front().unwrap() {
                output.values.push(lists.values.get(item_index));
            }
        }
        if output.values.len() > output.bounds.borrow().last().unwrap_or(0) as usize {
            output.bounds.push(output.values.len() as u64);
            items.push_back(true);
        }
        else {
            items.push_back(false);
        }
    }

    assert_eq!(items.len(), lists.len());
    output
}
```

The `antijoin` code mirrors the `union` code above, where it specializes based on the type of the layer.
It's a bit wordier because there are two passes calling two methods (`intersection` and `restrict`), but nothing too clever other than an early escape should all bits for some layer be `true` (all higher layers will be retained excatly as is).

Same experiment as before, with `samply` at 1KHz.

|            | in except | in total |
|------------|----------:|---------:|
| row-based  |     3,451 |   19,417 |
| col-based  |     2,528 |   18,505 |

That's within 50ms of the previous run for the column-based approach, which is nice consistency (it's the same code).
As before, the row-based version is specialized to `[u8; 4]` throughout, so this is really just shifting the logic to column-orientation.

One benefit the column approach does have is that `except` is actually a multi-way antijoin, where one input is restricted by several other collections of established facts.
The story there is that our facts are represented in a log-structured merge .. trie representation, and the pre-existing facts are actually a list of columnar tries of geometrically increasing size.
The row-based implementation repeatedly applies a binary antijoin, because writing the multiway merge code was hard (you try).

The column-based approach performs the intersections for each other trie, merges their results into one list of Booleans, and then calls `restrict` and forms an output only once.
So, there's real work the column-based approach can avoid.
I'm not sure if that's a fortunate coincidence or a recurring pattern.

### Intersecting sets of facts (Semijoin)

The same thing works for semijoins.
They are like antijoins, except you keep the intersection, rather than the input minus the intersection.
I don't have any semijoins showing up in queries at the moment (well, one small instance).
They are relatively rarer in conventional binary joins, which you usually perform to bring in additional information from other relations.

However, semijoin do show up more prominently in worst-case optimal joins, which is why I went down this whole rabbit-hole to begin with (I needed efficient semijoins).
I would imagine that's coming up next, as well as first-class support for antijoins in the supported Datalog.
